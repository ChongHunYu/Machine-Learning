{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83677184},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"1caef2","input":"","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"efa51a","input":"import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nX = np.random.randn(64).reshape(8,8)\npf = PolynomialFeatures(5)\nX = pf.fit_transform(X)\nX.shape","output":{"0":{"data":{"text/plain":"(8, 1287)"},"exec_count":1,"output_type":"execute_result"}},"pos":1,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"f7bf07","input":"from sklearn.datasets import make_blobs\n#import numpy as np\n#import pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\n\nX,y = make_blobs(centers=2,cluster_std = 4,random_state=11)\nss = StandardScaler()\nX = ss.fit_transform(X)\ny = 2*y-1\nXr = X[y==1]\nXb = X[y==-1]\nplt.scatter(Xr[:,0],Xr[:,1])\nplt.scatter(Xb[:,0],Xb[:,1])\nplt.show()","output":{"0":{"data":{"image/png":"bdf75780f0fca2d93ea4da84d63919b3077a537c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":411,"width":715},"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"594f90","input":"from sklearn.svm import SVC\n\n## Code from here: https://stackoverflow.com/questions/51495819/how-to-plot-svm-decision-boundary-in-sklearn-python\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n#model = svm.SVC(kernel='linear')\n\ndef plot_model(clf,X):\n    fig, ax = plt.subplots()\n    # title for the plots\n    title = \"\"\n    #title = ('Decision surface of linear SVC ')\n    # Set-up grid for plotting.\n    X0, X1 = X[:, 0], X[:, 1]\n    xx, yy = make_meshgrid(X0, X1)\n\n    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n    #ax.set_ylabel('y label here')\n    #ax.set_xlabel('x label here')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n    #ax.legend()\n    plt.show()\n\n    ","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"ca87a0","input":"model = SVC(kernel=\"poly\", degree=7, coef0 = 1)  #coef0 = r\n\nclf = model.fit(X, y)\nplot_model(clf,X)    \n","output":{"0":{"data":{"image/png":"423b8ade95c794fed176302f23eb76881b3c375a","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"55daf0","input":"## Yes, this model is terrible\n\nmodel = SVC(kernel=\"poly\", degree=10)  #coef0 = r\n\nclf = model.fit(X, y)\nplot_model(clf,X)   ","output":{"0":{"data":{"image/png":"5a0b9d6cab00363c2b1404541b7dfad5c47a2660","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"c3ddf3","input":"model = SVC(kernel=\"rbf\")  \nclf = model.fit(X, y)\nplot_model(clf,X)    \n","output":{"0":{"data":{"image/png":"b0a45f30650c93442cc998bd6c47db6b19fc1e99","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"28fb36","input":"model = SVC(kernel=\"rbf\",gamma=12)  \nclf = model.fit(X, y)\nplot_model(clf,X)    ","output":{"0":{"data":{"image/png":"33bb8c7d3eb3d3581dd260b5dbd2c84675f8c3b7","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"c257c0","input":"model = SVC(kernel=\"linear\")  \nclf = model.fit(X, y)\nplot_model(clf,X)  ","output":{"0":{"data":{"image/png":"a28ce4612cdbf5b8ae6f6a53904147b87617f475","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":8,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"84a27f","input":"model = SVC(kernel=\"sigmoid\")  \nclf = model.fit(X, y)\nplot_model(clf,X)  ","output":{"0":{"data":{"image/png":"6b64f6b52039d44d341b74278ff4cd43ccc3d639","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":398,"width":687}},"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"markdown","id":"30aab4","input":"### The dual problem\n\nFor any constrained optimization problem (such as training an SVM) there is an associated optimization problem called the [dual](https://en.wikipedia.org/wiki/Duality_(optimization)).\n\nIn the case of a convex optimization problem (such as training an SVM) the dual problem and the original (\"primal\") problem have the same solutions.\n\nWe will skip the technicalities, but we need to see the form of the dual SVM problem to understand the kernel trick.\n\nThe notation for the dual is much more suited to the convention that $\\bar{w}$ **does not** have the bias weight $b$, and $\\bar{x}$ **does not** start with a bias 1. We will use that notation here.  \n\nThe dual SVM problem is to minimize the following with respect to $\\alpha_1,\\alpha_2,\\ldots,\\alpha_N$ subject to $\\alpha_n \\geq 0$ for $n=1,2,\\ldots,N$.\n\n$$\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\bar{x}_i^T\\bar{x}_j - \\sum_{i=1}^N \\alpha_i.$$\n\nOnce the solution is known, $\\bar{w}$ can be determined by the equations\n\n$$\\bar{w} = \\sum_{i=1}^N \\alpha_i y_i \\bar{x}_i$$\n\n$$b = \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N (y_i - \\bar{w}^T\\bar{x}_i)$$\n\nwhere $n_s$ is the number of support vectors.\n\n(This is the hard margin version -- there is also a soft margin formula.)\n\n### Observation\n\nIn order to apply the weights $\\bar{w},b$ to an instance $\\bar{x}$, we calculate\n\n$$h_{\\bar{w},b}(\\bar{x}) = \\bar{w}^T\\bar{x} + b$$\n\nBy making substitutions using the above equations, the right hand side can be expressed *entirely in terms of the $\\alpha_i$ and the training set $(\\bar{x}_1,y_1)\\ldots,(\\bar{x}_N,y_N)$*.\n\nIn fact\n\n$h_{\\bar{w},b}(\\bar{x}) = \\sum_{i=1, \\alpha_i > 0}^N \\alpha_iy_i\\bar{x}_i^T\\bar{x} + b$\n\nand $b$ can further be expressed as \n\n$$b = \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N  \\left (1 - y_i \\sum_{j=1, \\alpha_j > 0}^N \\alpha_j y_j \\bar{x}_i^T\\bar{x}_j \\right )$$\n\nPutting it together,\n\n$$h_{\\bar{w},b}(\\bar{x}) = \\sum_{i=1, \\alpha_i > 0}^N \\alpha_iy_i\\bar{x}_i^T\\bar{x} + \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N  \\left (1 - y_i \\sum_{j=1, \\alpha_j > 0}^N \\alpha_j y_j \\bar{x}_i^T\\bar{x}_j \\right )$$\n\n\n---\n\nNow notice that **if** we performed a variable transformation $\\bar{z} = \\phi(\\bar{x})$ (e.g. polynomial features) then the above expression would become\n\n$$h_{\\bar{w},b}(\\bar{x}) = \\sum_{i=1, \\alpha_i > 0}^N \\alpha_iy_i\\phi(\\bar{x}_i)^T\\phi(\\bar{x}) + \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N  \\left (1 - y_i \\sum_{j=1, \\alpha_j > 0}^N \\alpha_j y_j \\phi(\\bar{x}_i)^T\\phi(\\bar{x}_j) \\right )$$\n\n### The kernel trick\n\nThe kernel trick is the observation that all we really need to be able to do is *dot products* in the higher dimensional space in order to train and evaluate the SVM in the $\\mathcal{Z}$-space.  \n\nWe do not actually literally have to do the transformation $\\bar{x} \\mapsto \\phi(\\bar{x})$, provided we can compute $K(\\bar{x}_i,\\bar{x}_j) =  \\phi(\\bar{x}_i)^T\\phi(\\bar{x}_j)$.\n\n\n### Example, the polynomial kernel\n\nConsider a two dimensional feature space with two instances $\\bar{x} = [x_1,x_2]$ and $\\bar{z} = [z_1,z_2]$.  \n\n(Here $\\bar{z}$ is not a transformed version of $\\bar{x}$, it is just another variable name.)\n\nNow consider the degree 2 polynomial feature transformation defined by\n\n$$\\phi(\\bar{x}) = [1,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2} x_1x_2,x_1^2,x_2^2].$$\n\nWith $\\bar{z}$ this is \n\n$$\\phi(\\bar{z}) = [1,\\sqrt{2}z_1,\\sqrt{2}z_2,\\sqrt{2} z_1z_2,z_1^2,z_2^2].$$\n\n\nThen\n\n$$\\phi(\\bar{x})^T\\phi(\\bar{z}) = 1 + 2x_1z_1+2x_2z_2+2x_1x_2z_1z_2+ x_1^2z_1^2+x_2^2z_2^2.$$\n\nBut notice that\n\n$$(1+\\bar{x}^T\\bar{z})^2 = 1 + 2x_1z_1+2x_2z_2+2x_1x_2z_1z_2+ x_1^2z_1^2+x_2^2z_2^2.$$\n\nIn other words,\n\n$$\\phi(\\bar{x})^T\\phi(\\bar{z}) = (1+\\bar{x}^T\\bar{z})^2.$$\n\nThus, we can compute the dot product without doing the variable transformation!\n\nThis is true not just for degree $2$ polynomials, but polynomials of any degree (with appropriate constants).\n\nWe call this **the polynomial kernel of degree $k$**\n\n$$K(\\bar{x},\\bar{z}) = (1+\\bar{x}^T\\bar{z})^k.$$\n\nThe moral is **we can work with polynomial features without paying the cost in time and space.**\n\n\nA function $K(\\bar{x},\\bar{z}) = \\phi(\\bar{x})^T\\phi(\\bar{z})$ for some variable transformation $\\phi(\\cdot)$ is called a **kernel**.\n\nThe interesting cases are when $K(\\bar{x},\\bar{z})$ is much easier to compute than $\\phi(\\bar{x})^T\\phi(\\bar{z})$ (such as with the polynomial kernel).\n\nWith a kernel function we can \"virtually\" work in the higher dimensional space.\n\nWe just use\n\n$$h_{\\bar{w},b}(\\bar{x}) = \\sum_{i=1, \\alpha_i > 0}^N \\alpha_iy_iK(\\bar{x}_i,\\bar{x}) + \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N  \\left (1 - y_i \\sum_{j=1, \\alpha_j > 0}^N \\alpha_j y_j K(\\bar{x}_i,\\bar{x}_j) \\right )$$\n\nrather than\n\n$$h_{\\bar{w},b}(\\bar{x}) = \\sum_{i=1, \\alpha_i > 0}^N \\alpha_iy_i\\phi(\\bar{x}_i)^T\\phi(\\bar{x}) + \\frac{1}{n_s} \\sum_{i=1, \\alpha_i > 0}^N  \\left (1 - y_i \\sum_{j=1, \\alpha_j > 0}^N \\alpha_j y_j \\phi(\\bar{x}_i)^T\\phi(\\bar{x}_j) \\right ).$$\n\nThe higher dimensions are never actually explicitly created (assuming the kernel doesn't create them).\n\n### Common kernel functions\n\nThe following are some common kernel functions.  \n\nNotice that some of them use additional hyperparameters.\n\n1. *Linear* $K(\\bar{x},\\bar{z}) = \\bar{x}^T\\bar{z}$\n2. *Polynomial* $K(\\bar{x},\\bar{z}) = (1+\\bar{x}^T\\bar{z})^k$ (or the fancier $K(\\bar{x},\\bar{z}) = (r+\\gamma\\bar{x}^T\\bar{z})^k$).\n3. *Gaussian radial basis function* $K(\\bar{x},\\bar{z}) = \\exp(-\\gamma |\\!| \\bar{x}-\\bar{z}|\\!|^2)$\n4. *Sigmoid*  $K(\\bar{x},\\bar{z}) = \\tanh(\\gamma\\bar{x}^T\\bar{z}+r)$\n\nThere are other kernels such as [string kernels](https://en.wikipedia.org/wiki/String_kernel).\n\nIn serveral cases (Gaussian RBF and Sigmoid) it is not at all obvious what the mapping $\\phi(\\cdot)$ would be.\n\nInterestingly we don't even need to know $\\phi$ -- we just need to know that some such $\\phi$ exists.\n\n[**Mercer's Theorem**](https://en.wikipedia.org/wiki/Mercer%27s_theorem) gives conditions under which $K(\\bar{x},\\bar{z})$ will be a kernel for some $\\phi(\\bar{x})$.\n\nThese include a few very general conditions like continuity, and symmetry.\n\nOne sufficient condition is to check that the matrix $A$ with entries $a_{i,j} = K(\\bar{x}_i,\\bar{x}_j)$ is positive semidefinite  ($\\forall \\bar{v}$, $\\bar{v}^TA\\bar{v} \\geq 0$).\n\n### Using the kernels\n\nWe will not implement SVMs capable of the kernel trick (though it's a good exercise to think about how to translate the dual problem into QP).\n\nWe can use them through the sklearn library. \n\n\n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"4f8c13","input":"### Speed and space\n\nOne major downside of working with a dataset with a huge number of features is speed.\n\nAnother is space.\n\nEither of these restrictions may be prohibitive when working with a large dataset. \n\nThe **kernel trick** allows us to enjoy the benefits of feature transformations without paying the cost.\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"53b0d7","input":"### The end\n\nIf you want to know a little more, these are good slides:\n\nhttps://svivek.com/teaching/lectures/slides/svm/kernels.pdf\n\nMuch of this material was adapted from *Hands-On Machine Learning with Scikit-Learn and TensorFlow* by Géron.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"b3cb98","input":"### The Kernel Trick\n\nMany times in the course so far we have transformed our variables to work in a higher dimensional space (e.g. adding polynomial features).\n\nOne downside to simply adding polynomial features is that it can create a huge number of variables.\n\nAn example is below.  \n\nWe begin with a dataset with 8 features, and add degree 5 polynomial features.\n\nThis results in a new dataset with over 1000 features.\n\n","pos":0,"type":"cell"}
{"id":0,"time":1589206863268,"type":"user"}
{"last_load":1589206866127,"type":"file"}