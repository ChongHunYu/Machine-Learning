{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83513344},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"ce8075","input":"","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f5f8c6","input":"","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":41,"id":"d7a39c","input":"## The practice data (nothing to do here)\n\nimport mystuff as ms\nimport numpy as np\n\nnp.random.seed(666)\n\nX,y,w = ms.myblobs()\n\nms.lin_boundary(w,X,y)\n\nhelp(ms)","output":{"0":{"data":{"image/png":"e78b0790415ad88302da66f6877fcd10b494c8d0","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":41,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"Help on module mystuff:\n\nNAME\n    mystuff\n\nFUNCTIONS\n    compare_boundary(w, ww, X, y, xmin=-8, xmax=8, ymin=-8, ymax=8)\n        lin_boundary(w,X,y,xmin=-8,xmax=8,ymin=-8,ymax=8)\n    \n    lin_boundary(w, X, y, xmin=-8, xmax=8, ymin=-8, ymax=8)\n        lin_boundary(w,X,y,xmin=-8,xmax=8,ymin=-8,ymax=8)\n    \n    myblobs(N=30, sig_yes=1, sig_no=1)\n\nFILE\n    /home/user/ML_Spr_20/Homeworks/HW2_PLA/mystuff.py\n\n\n"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":42,"id":"755064","input":"\"\"\"\n******************************************************************************************************\nInput: X, an N x (d+1) dimensional matrix with a bias column\nInput: y, an N dimensional vector of +1 and -1.\n\nw = a vector of d+1 zeros (or random numbers)\nWhile a misclassified instance x,y exists:\n    w = w+x*y\n\nReturn w\n******************************************************************************************************\n\nWhat you have to do is pick just one misclassified x and the corresponding y.  For instance you might do this:\n\nx = X[mc][0]   #  This one is as good as any.\nymc = y[mc][0]  # this is the corresponding y value.\n\nNow you can use x and ymc the way x and y are used in the pseudocode.\n******************************************************************************************************\n\nThe problem is that mc needs to be updated inside the loop.\nEach time through the loop you should get a fresh misclassified point, if one exists.\nIf none exist, then break out of the loop....\n\n******************************************************************************************************\nThe problem is that mc needs to be updated inside the loop.  Each time through the loop you \nshould get a fresh misclassified point, if one exists.  If none exist, then break out of the loop....\n*******************************************************************************************************\n\"\"\"\n\ndef PLA(X,y,w,max_iter=1000):\n    i = 0\n    mc = (np.sign(X.dot(w)) != y)\n    while(mc.any()):\n        w = w+((X[mc][0])*(y[mc][0]))\n        mc = (np.sign(X.dot(w)) != y)\n        i+=1\n        if(mc.any()==False or i>=max_iter):\n            break\n    max_iter = i\n    return w, max_iter\n\ndef E_in(X,y,w):\n    mc = (np.sign(X.dot(w)) != y)\n    return np.sum(mc)/len(mc)\n\n\nw = np.zeros(X.shape[1])\n\nw,i=PLA(X,y,w)\nms.lin_boundary(w,X,y)\n\nprint(\"Finished in {} iterations\".format(i))\nprint(\"E_in = {}\".format(E_in(X,y,w)))","output":{"0":{"data":{"image/png":"54623cb1bd47ccd36f61b6e1b96ade145a34e014","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":42,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"Finished in 7 iterations\nE_in = 0.0\n"}},"pos":3,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"8316a9","input":"import mystuff as ms\nimport numpy as np\nnp.random.seed(1000)\nX,y,w = ms.myblobs(sig_yes=2)\n\nw = np.zeros(X.shape[1])\nw,i=PLA(X,y,w)\nms.lin_boundary(w,X,y)\n\nprint(\"Finished in {} iterations\".format(i))\nprint(\"E_in = {}\".format(E_in(X,y,w)))","output":{"0":{"data":{"image/png":"68344f9a7245c5321fab3b9ab2efd635be41450a","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":43,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"Finished in 1000 iterations\nE_in = 0.15\n"}},"pos":5,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":44,"id":"b8f9c7","input":"\"\"\"\nw(0) = a vector of d+1 zeros\nw_hat = w(0)\nfor t=0,...,max_iter-1:\n    Run PLA for one update to obtain w(t+1)\n    Evaluate E_in(w(t+1))\n    If w(t+1) is better than w_hat in terms of E_in, set w_hat to w(t+1)\nReturn w_hat\n\"\"\"\ndef Pocket(X,y,w,max_iter=1000):\n    w_hat = w\n    t = 0\n    for t in range(max_iter-1):\n        mc = (np.sign(X.dot(w)) != y)\n        w,i=PLA(X,y,w,1)\n        if(E_in(X,y,w)<E_in(X,y,w_hat)):\n            w_hat=w\n            mc=(np.sign(X.dot(w))!=y)\n        t+=1\n    return w_hat,t\n\n\nw = np.zeros(X.shape[1])\nw,i=Pocket(X,y,w)\nms.lin_boundary(w,X,y)\n\nprint(\"Finished in {} iterations\".format(i))\nprint(\"E_in = {}\".format(E_in(X,y,w)))\n","output":{"0":{"data":{"image/png":"4d27c8aeee7604b15edfda9f73c73a75cdd81a3d","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":44,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nE_in = 0.016666666666666666\n"}},"pos":7,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":45,"id":"71ce32","input":"def train_test_split(X,y,train_pct=0.8):\n    cutoff = int(train_pct*X.shape[0])\n    train = np.random.choice(np.arange(X.shape[0]),cutoff,replace=False)\n    test = np.array(list(set(np.arange(X.shape[0])).difference(set(train))))\n    return X[train],y[train],X[test],y[test]\n\nnp.random.seed(1999)\nX,y,w = ms.myblobs(N=200,sig_yes=2)\nms.lin_boundary(w,X,y)\n","output":{"0":{"data":{"image/png":"d73c688799be9cd744a1b82a058e71c06d3cc653","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":45,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":46,"id":"89f708","input":"X_train,y_train,X_test,y_test = train_test_split(X,y)\n\nw = np.zeros(X.shape[1])\nw,i=Pocket(X_train,y_train,w)\nprint(\"Finished in {} iterations\".format(i))\n\nprint(\"Best hypothesis on the training set:\")\nprint(\"E_in (train) = {}\".format(E_in(X_train,y_train,w)))\nms.lin_boundary(w,X_train,y_train)\n\nprint(\"Best hypothesis on the testing set:\")\nprint(\"E_in (test) = {}\".format(E_in(X_test,y_test,w)))\nms.lin_boundary(w,X_test,y_test)\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nBest hypothesis on the training set:\nE_in (train) = 0.03125\n"},"1":{"data":{"image/png":"6b6b06bfcd02f4af3a043933327cbe5e07ee0bba","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":46,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"},"2":{"name":"stdout","output_type":"stream","text":"Best hypothesis on the testing set:\nE_in (test) = 0.05\n"},"3":{"data":{"image/png":"468e8c60d67e5d4df6a66b4391b1ec3b5ee2241a","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":46,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":47,"id":"4b620c","input":"## numpy doesn't play well with strings, so we replace M and B with 1 and 0\n\nimport pandas as pd\n\ndf = pd.read_csv(\"wdbc.data.txt\",header=None,index_col=0)\n\ndf = df.replace('M',1).replace('B',-1)\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>842302</th>\n      <td>1</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>842517</th>\n      <td>1</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>84300903</th>\n      <td>1</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>84348301</th>\n      <td>1</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>84358402</th>\n      <td>1</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>","text/plain":"          1      2      3       4       5        6        7       8        9   \\\n0                                                                               \n842302     1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710   \n842517     1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017   \n84300903   1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790   \n84348301   1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520   \n84358402   1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430   \n\n              10  ...     22     23      24      25      26      27      28  \\\n0                 ...                                                         \n842302    0.2419  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n842517    0.1812  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n84300903  0.2069  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n84348301  0.2597  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n84358402  0.1809  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n\n              29      30       31  \n0                                  \n842302    0.2654  0.4601  0.11890  \n842517    0.1860  0.2750  0.08902  \n84300903  0.2430  0.3613  0.08758  \n84348301  0.2575  0.6638  0.17300  \n84358402  0.1625  0.2364  0.07678  \n\n[5 rows x 31 columns]"},"exec_count":47,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":48,"id":"e3f70b","input":"X = df.values  ## Converts from pandas to a numpy array\n\ny = np.copy(X[:,0])  # get target values in first column\nX[:,0] = 1 # convert first column to bias column\n\nX_train,y_train,X_test,y_test = train_test_split(X,y)\n\nw = np.zeros(X.shape[1])\nw,i=Pocket(X_train,y_train,w,max_iter=1000)\nprint(\"Finished in {} iterations\".format(i))\n\nprint(\"Best hypothesis on the training set:\")\nprint(\"E_in (train) = {}\".format(E_in(X_train,y_train,w)))\n\nprint(\"Best hypothesis on the testing set:\")\nprint(\"E_in (test) = {}\".format(E_in(X_test,y_test,w)))\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nBest hypothesis on the training set:\nE_in (train) = 0.14505494505494507\nBest hypothesis on the testing set:\nE_in (test) = 0.14912280701754385\n"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":49,"id":"9ea3a8","input":"def minmax(X):\n    mins = np.min(X,axis=0)\n    maxs = np.max(X,axis=0)\n    bias = np.allclose(X[:,0],np.ones(X.shape[0]))  ## Is there a bias column?\n    if bias:  ## Without this max=min, causes divide-by-zero\n        maxs[0]=1\n        mins[0]=0\n    return mins,maxs\n\ndef minmax_apply(X,mins,maxs):\n    return (X-mins)/(maxs-mins)\n","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":50,"id":"339094","input":"mins,maxs = minmax(X_train)\nX_train_norm = minmax_apply(X_train,mins,maxs)\nX_test_norm = minmax_apply(X_test,mins,maxs)","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":51,"id":"9014be","input":"\nw = np.zeros(X.shape[1])\nw,i=Pocket(X_train_norm,y_train,w,max_iter=1000)\nprint(\"Finished in {} iterations\".format(i))\n\nprint(\"Best hypothesis on the training set:\")\nprint(\"E_in (train) = {}\".format(E_in(X_train_norm,y_train,w)))\n\nprint(\"Best hypothesis on the testing set:\")\nprint(\"E_in (test) = {}\".format(E_in(X_test_norm,y_test,w)))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nBest hypothesis on the training set:\nE_in (train) = 0.017582417582417582\nBest hypothesis on the testing set:\nE_in (test) = 0.02631578947368421\n"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":52,"id":"580181","input":"\"\"\"\nI suggest making the average of the bias column 0, if there is a bias column.\n#add a bias column of all 0's\n\nbias = np.zeros((X1.shape[0],1))\nX=np.hstack((bias,X1))\n\n\"\"\"\ndef meannorm(X):\n    ### You complete me\n    mn = np.min(X,axis=0)\n    mx = np.max(X,axis=0)\n    bias = np.allclose(X[:,0],np.ones(X.shape[0]))  ## Is there a bias column?\n    if bias:  ## Without this max=min, causes divide-by-zero\n        mx[0]=1\n        mn[0]=0\n    mn,mx=minmax(X)\n    meanorm=all([((X-np.mean(X)))/((max(X)-min(X)))])\n    return mn,mx,meanorm\n\ndef meannorm_apply(X,mins,maxs,mu):\n    return (X-mu)/(maxs-mins)\n\nmins,maxs,mu = meannorm(X_train)\nX_train_norm = meannorm_apply(X_train,mins,maxs,mu)\nX_test_norm = meannorm_apply(X_test,mins,maxs,mu)\n\n\nw = np.zeros(X.shape[1])\nw,i=Pocket(X_train_norm,y_train,w,max_iter=1000)\nprint(\"Finished in {} iterations\".format(i))\n\nprint(\"Best hypothesis on the training set:\")\nprint(\"E_in (train) = {}\".format(E_in(X_train_norm,y_train,w)))\n\nprint(\"Best hypothesis on the testing set:\")\nprint(\"E_in (test) = {}\".format(E_in(X_test_norm,y_test,w)))\n","output":{"0":{"ename":"ValueError","evalue":"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-605da423752d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeannorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mX_train_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeannorm_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mX_test_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeannorm_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-605da423752d>\u001b[0m in \u001b[0;36mmeannorm\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmeanorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeanorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"]}},"pos":21,"type":"cell"}
{"cell_type":"markdown","id":"392749","input":"###  That was terrible!\n\nThe accuracy of the pocket algorithm on the breast cancer data is only about 15%. \n\nWe can massively improve performance by scaling the data.\n\nThis means putting each column roughly in the same range of values.\n\nWe use minmax scaling. \n\nBe careful when you scale not to cheat by looking prematurely at the testing data.\n\nThe scaling parameters have to be based solely on the training data.\n\nIn this case we need the max and min values from each column.\n\nWe get these parameters from the training data, and then apply the minmax transformation as a subsequent step.\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"59593e","input":"### HW2 PLA and Pocket\n\nIn this homework we will implement both the Perceptron Learning Algorithm (PLA) and an improvement called the Pocket algorithm.\n\nWe will run these classifiers on synthetic and real data, and see how they perform on the training and testing sets.\n\nFor the purposes of testing the PLA, we first make some \"blobs\" which we will try to classify.\n\nThe initial decision boundary is just a made up value that happens to work pretty well.\n\nThis data is linearly separable, so PLA can classify it with no errors.\n\nRemember the pseudocode for the perceptron (you should have this in your notes, and in the book):\n\n```\nInput: X, an N x (d+1) dimensional matrix with a bias column\nInput: y, an N dimensional vector of +1 and -1.\n\nw = a vector of d+1 zeros (or random numbers)\nWhile a misclassified instance x,y exists:\n    w = w+x*y\n\nReturn w\n```\n\nWe know that if $\\mathcal{D}$ is not linearly separable then this will never terminate.\n\nFor that reason we also include a parameter called `max_iter` which is the maximum number of iterations we will try. The default value is 1000. \n\nYour PLA should return the learned $\\bar{w}$ and the number of iterations it actually took to converge.\n\nIn your notes you should find numpy code for detecting misclassifications.\n\n#### Testing\n\nIf you read through this document (before starting) you will see that the correct output is shown, although the code that made the correct output has been deleted.\n\nAs you work on your own code you will destroy the correct outputs. \n\nSo that you can continue to use them, I have saved an HTML copy of this document in this folder.  You can use the HTML doc to see what the right answers should be.\n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"64ba6a","input":"### Measuring Performance\n\nAs we discussed in class (see the Feasibility of Learning notes), the performance of the PLA or pocket algorithms on all of $X$ is note a good measure of how that hypothesis will perform out of sample.\n\nWe always want to know how good our classifier really is.\n\nFor that reason we will try using training and testing data.\n\nWe will run the pocket algorithm on the training data to find a good $\\bar{w}$.\n\nThen we will see how that $\\bar{w}$ does on the testing data.  \n\nThis will give us some idea of how $\\bar{w}$ would perform out of sample.\n\n\n### Train test split\n\nBelow you are given a function that implements a train-test split.\n\nJust briefly take note of how it works -- 80% of data is for training by default.\n\nNotice that the rows that go into the training or testing set are randomly selected.\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"8b0d0a","input":"### Your turn\n\nTry implementing mean normalization as described here: https://en.wikipedia.org/wiki/Feature_scaling\n\nI suggest making the average of the bias column 0, if there is a bias column.\n\nIf you don't remember what `axis` means then you might want to review the numpy slides.\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"961bbb","input":"### The Pocket Algorithm\n\nThe pocket algorithm is an improvement on the PLA that works much better on data that isn't linearly separable.\n\nBecause most data in nature is not linearly separable this is important if you really want to use the perceptron for something.\n\nThe pocket algorithm is described on page 80 of learning from data.\n\nFor your convenience I will copy the pseudocode here:\n\n```\nw(0) = a vector of d+1 zeros\nw_hat = w(0)\nfor t=0,...,max_iter-1:\n    Run PLA for one update to obtain w(t+1)\n    Evaluate E_in(w(t+1))\n    If w(t+1) is better than w_hat in terms of E_in, set w_hat to w(t+1)\nReturn w_hat\n```\n\nPlease do not literally put `w(t+1)` or something like that in your program -- that is just mathematical notation and not code. Read the chapter for more clarification.\n\nReading section 3.1.1 of the book before beginning is strongly recommended.\n","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"b0ae65","input":"### Nonlinearly separable data\n\nBelow we try the PLA on data that is not linearly separable.\n\nYou can see that the PLA doesn't do so well on this.  \n\nHave a look at Figure 3.2 on pg 83 of Learning From Data for more insight. \n\nThere's nothing to do here, except take note and pity the poor PLA.\n","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"bb5ecd","input":"### Real Data\n\nIn the above examples we ran the pocket algorithm on 2d data that we just made up.\n\nBut your code should work on data of any dimension. \n\nBelow we'll use the pocket algorithm on the Wisconsin Breast Cancer dataset.\n","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"eb1b53","input":"### Performance\n\nBelow we train the pocket algorithm on the training set and test it on the testing set.\n\nNotice that the error on the training set as only about half of the more realistic value we get on the test set.\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"ec2b64","input":"###  That's better\n\nWe got the out of sample error down below 3%.  That's pretty good.  \n\n","pos":19,"type":"cell"}
{"id":0,"time":1587226555298,"type":"user"}
{"last_load":1587226543837,"type":"file"}