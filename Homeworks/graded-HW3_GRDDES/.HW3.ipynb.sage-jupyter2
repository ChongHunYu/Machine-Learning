{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83836928},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"2146c8","input":"","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4878a0","input":"","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6f8fb0","input":"","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8a093d","input":"","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e1b759","input":"","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":100,"id":"2ef032","input":"def mse_gradient(w,X,y):\n    ## You complete me\n    ## This formula is given above\n    N = X.shape[0]\n    gradientOfMeanSquaredError= 2/N*(X.T.dot(X).dot(w)-X.T.dot(y))\n    return gradientOfMeanSquaredError\n","pos":11,"type":"cell"}
{"cell_type":"code","exec_count":101,"id":"538507","input":"w_init = np.random.randn(2)/10\n\nwgd,path = grad_descent(w_init,X_train,y_train,mse_gradient)\n\n\ny_test_hat = X_test.dot(wgd)\n\nprint(\"w (normal eqn) = {}\".format(w))\nprint(\"w (gradient descent) = {}\".format(wgd))\nprint(\"R2 = {}\".format(R2(y_test,y_test_hat)))","output":{"0":{"name":"stdout","output_type":"stream","text":"w (normal eqn) = [ 0.10939422 41.06580482]\nw (gradient descent) = [ 0.10939422 41.06580482]\nR2 = 0.9038150556357982\n"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":102,"id":"bcae18","input":"## First 7 w's visited in grad descent\npath[:7]","output":{"0":{"data":{"text/plain":"array([[ 0.08630898,  0.12890794],\n       [-0.14041867,  8.22211369],\n       [-0.27606406, 14.71398929],\n       [-0.34789314, 19.9216368 ],\n       [-0.37592668, 24.09931227],\n       [-0.37474442, 27.45089147],\n       [-0.35485799, 30.139861  ]])"},"exec_count":102,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":103,"id":"630acc","input":"## Last 5 w's visited\n\npath[-5:]","output":{"0":{"data":{"text/plain":"array([[ 0.10939422, 41.06580482],\n       [ 0.10939422, 41.06580482],\n       [ 0.10939422, 41.06580482],\n       [ 0.10939422, 41.06580482],\n       [ 0.10939422, 41.06580482]])"},"exec_count":103,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":104,"id":"0075e2","input":"## This will make a picture of the path your w followed during GD\n\nxx = np.linspace(-50,80,100)\nyy = np.linspace(-50,80,100)\n\nXX,YY = np.meshgrid(xx,yy)\nW = np.c_[XX.ravel(),YY.ravel()]\nerrs = ((X.dot(W.T)).T - y)**2\nE = np.mean(errs,axis=1)\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (regression)\")\nplt.xlabel(r\"$w_0$\")\nplt.ylabel(r\"$w_1$\")\nplt.scatter(path[:,0],path[:,1],c='r',alpha=0.5)\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"e02b153002655e5e20df44baed1a436984982d68","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":104,"metadata":{"image/png":{"height":441,"width":667},"needs_background":"light"},"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":105,"id":"cacefa","input":"### This shows how the magnitude of the w your GD considered changed over time\n### When could your program have stopped?\n\nL = [mse_gradient(ww,X,y) for ww in path]\ngradients = np.linalg.norm(L,axis=1)\nplt.plot(np.arange(len(gradients)),gradients)\nplt.title(\"Magnitude of gradient over time steps\")\nplt.ylabel(\"Gradient magnitude\")\nplt.xlabel(\"Time step\")\nplt.show()","output":{"0":{"data":{"image/png":"00204047c9e68de431d6ab834a9f833f95eddfad","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":105,"metadata":{"image/png":{"height":440,"width":717},"needs_background":"light"},"output_type":"execute_result"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":106,"id":"e7dcda","input":"## This matrix version of MSE is from the bottom of pg 84 in the text\n## It is 20 times faster than an implementation with python loops!\n\ndef MSE(w,X,y):\n    return 1/X.shape[0]*(w.T.dot(X.T).dot(X).dot(w) - 2*w.T.dot(X.T).dot(y) + y.dot(y))\n\nerr_diffs = np.diff([MSE(ww,X,y) for ww in path])\n\nplt.plot(np.arange(len(err_diffs)),err_diffs)\nplt.title(\"Error differences over time\")\nplt.ylabel(r\"$E_{in}(w_{t+1})-E_{in}(w_{t})$\")\nplt.xlabel(\"Time step\")\nplt.show()\n\n","output":{"0":{"data":{"image/png":"85400d6ef2981194fe9743b9a771cb53ae0d80fc","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":106,"metadata":{"image/png":{"height":440,"width":733},"needs_background":"light"},"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":107,"id":"0d0752","input":"import pandas as pd\ndf=pd.read_csv(\"housing.data\",header=None,delim_whitespace=True)\ndf.head()","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"        0     1     2   3      4      5     6       7   8      9     10  \\\n0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n\n       11    12    13  \n0  396.90  4.98  24.0  \n1  396.90  9.14  21.6  \n2  392.83  4.03  34.7  \n3  394.63  2.94  33.4  \n4  396.90  5.33  36.2  "},"exec_count":107,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":108,"id":"35d569","input":"X_nb = df.values[:,:-1]\ny = df.values[:,-1]\nX = np.concatenate((np.ones(X_nb.shape[0]).reshape(X_nb.shape[0],1),X_nb),axis=1)\nX_train,X_test,y_train,y_test = train_test_split(X,y)\nw = np.linalg.pinv(X_train).dot(y_train)\ny_train_hat = X_train.dot(w)\ny_test_hat = X_test.dot(w)\n\nprint(\"w (normal equations )= {}\".format(w))\nprint(\"R2 (normal equations) = {}\".format(R2(y_test,y_test_hat)))\nw_init = np.random.randn(X.shape[1])\n\nwgd,path = grad_descent(w_init,X_train,y_train,mse_gradient,eta=0.000001,max_iter=100000)\n\ny_test_hat = X_test.dot(wgd)\n\nprint(\"w (gradient descent) = {}\".format(wgd))\nprint(\"R2 (gradient descent)= {}\".format(R2(y_test,y_test_hat)))","output":{"0":{"name":"stdout","output_type":"stream","text":"w (normal equations )= [ 2.68889211e+01 -1.05020682e-01  4.19079039e-02 -4.16278308e-02\n  2.17861434e+00 -1.23446232e+01  4.57907475e+00 -2.64765372e-02\n -1.37543555e+00  2.10872234e-01 -1.25262409e-02 -8.22507430e-01\n  9.05634346e-03 -3.35279216e-01]\nR2 (normal equations) = 0.5837035675846445\n"},"1":{"name":"stdout","output_type":"stream","text":"w (gradient descent) = [ 0.54360954 -0.09014226  0.08978069 -0.07935464  0.37381637 -1.39600309\n  2.37953346  0.05826803 -0.22972299  0.07749059 -0.0062087   0.2970127\n  0.0215568  -0.54984907]\nR2 (gradient descent)= 0.48382794384544336\n"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":109,"id":"7dd56b","input":"err_diffs = np.diff([MSE(ww,X,y) for ww in path])\n\nplt.plot(np.arange(len(err_diffs)),err_diffs)\nplt.title(\"Error differences over time\")\nplt.ylabel(r\"$E_{in}(w_{t+1})-E_{in}(w_{t})$\")\nplt.xlabel(\"Time step\")\nplt.show()\n\nL = [mse_gradient(ww,X,y) for ww in path]\ngradients = np.linalg.norm(L,axis=1)\nplt.plot(np.arange(len(gradients)),gradients)\nplt.title(\"Magnitude of gradient over time steps\")\nplt.ylabel(\"Gradient magnitude\")\nplt.xlabel(\"Time step\")\nplt.show()\n\n","output":{"0":{"data":{"image/png":"8f02936beb9f70344c2e539924ed469c5892722d","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":109,"metadata":{"image/png":{"height":440,"width":753},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"image/png":"41c9348bb6956d19f8b405247bd20b462a6c08c7","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":109,"metadata":{"image/png":{"height":440,"width":743},"needs_background":"light"},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":110,"id":"18c7d1","input":"err_diffs[-10:]","output":{"0":{"data":{"text/plain":"array([-3.11224796e-05, -3.11220616e-05, -3.11216415e-05, -3.11212224e-05,\n       -3.11208034e-05, -3.11203845e-05, -3.11199658e-05, -3.11195465e-05,\n       -3.11191277e-05, -3.11187087e-05])"},"exec_count":110,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":111,"id":"75e387","input":"gradients[-10:]","output":{"0":{"data":{"text/plain":"array([332.99482175, 332.9949491 , 332.99507645, 332.9952038 ,\n       332.99533115, 332.9954585 , 332.99558584, 332.99571319,\n       332.99584054, 332.99596788])"},"exec_count":111,"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":112,"id":"7d63db","input":"def minmax(X):\n    mins = np.min(X,axis=0)\n    maxs = np.max(X,axis=0)\n    bias = np.allclose(X[:,0],np.ones(X.shape[0]))  ## Is there a bias column?\n    if bias:  ## Without this max=min, causes divide-by-zero\n        maxs[0]=1\n        mins[0]=0\n    return mins,maxs\n\ndef minmax_apply(X,mins,maxs):\n    return (X-mins)/(maxs-mins)\n\nmins,maxs = minmax(X_train)\nX_train = minmax_apply(X_train,mins,maxs)\nX_test = minmax_apply(X_test,mins,maxs)\n","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":113,"id":"6a141e","input":"def pw_linreg_grad(x,w,yy):\n    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n       return:  new w\n       This is the gradient for MSE error.\n       The mathematical formula can be found above.\n       w = (np.linalg.pinv(X_train).dot(y_train))\n       w=w-eta*gradient(w,X,y)\n    \"\"\"\n    return 2*(w.T.dot(x)-yy)*x\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        Xs = X[shuff]\n        ys = y[shuff]\n        for i in range(X.shape[0]):\n            history.append(w)\n            #You complete me\n            x = Xs[i]\n            yy = ys[i]\n            w=w-eta*pw_gradient(x,w,yy)\n    return w,np.array(history)","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":114,"id":"27fbc7","input":"### Be sure X_train and X_test are already scaled for this test.\n### Actual R2 scores will vary but the normal eqns and sgd should give similar results.\n\nw = np.linalg.pinv(X_train).dot(y_train)\nyhat_test_ne = X_test.dot(w)\nprint(\"Solution from normal equations: \\n\",w)\nprint(\"R2_test(w) (normal equations) = {}\".format(R2(yhat_test_ne,y_test)))\n\nwsgd = np.random.randn(X.shape[1])\nwsgd,path = sgd(wsgd,X_train,y_train,pw_linreg_grad,eta=0.01,num_epochs=300)\nprint(\"Solution from SGD\")\nyhat_test = X_test.dot(wsgd)\nprint(\"w = {},\\n R2_test(w) (sgd) = {}\".format(wsgd,R2(yhat_test,y_test)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Solution from normal equations: \n [ 23.6146529   -9.34367746   4.19079039  -1.13560723   2.17861434\n  -5.99948687  23.89819114  -2.57087176 -15.06418274   4.85006137\n  -6.56375022  -7.73156984   3.57164073 -11.75488933]\nR2_test(w) (normal equations) = 0.17835101779501605\n"},"1":{"name":"stdout","output_type":"stream","text":"Solution from SGD\nw = [ 23.56248272  -9.38451918   4.07895924  -1.13766859   2.16953781\n  -5.92372681  23.81784098  -2.45071077 -14.99766162   4.8844509\n  -6.61898303  -7.59473035   3.51613441 -11.59465804],\n R2_test(w) (sgd) = 0.16474811345691542\n"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":115,"id":"cfce18","input":"err_diffs = np.diff([MSE(ww,X,y) for ww in path])\n\nplt.plot(np.arange(len(err_diffs)),err_diffs)\nplt.title(\"Error differences over time\")\nplt.ylabel(r\"$E_{in}(w_{t+1})-E_{in}(w_{t})$\")\nplt.xlabel(\"Time step\")\nplt.show()\n\nL = [mse_gradient(ww,X,y) for ww in path]\ngradients = np.linalg.norm(L,axis=1)\nplt.plot(np.arange(len(gradients)),gradients)\nplt.title(\"Magnitude of gradient over time steps\")\nplt.ylabel(\"Gradient magnitude\")\nplt.xlabel(\"Time step\")\nplt.show()","output":{"0":{"data":{"image/png":"6ec9ba82d7cbdfa2a110e5f4a12aa3a292fcd162","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":115,"metadata":{"image/png":{"height":440,"width":730},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"image/png":"5f444db147e334ce52a4bf7a74b811b46c79d785","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":115,"metadata":{"image/png":{"height":440,"width":720},"needs_background":"light"},"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":94,"id":"314686","input":"import numpy as np\nimport matplotlib.pyplot as plt\n\n### Here is the function you must complete\n### Below you can see some tests which you can use to determine if you've succeeded.\n### In your implementation simply run gradient descent for max_iter iterations.\n### In other words you don't need to implement fancy stopping conditions.\n\ndef grad_descent(w,X,y,gradient,eta=0.1,max_iter=1000):\n    \"\"\" parameters: w, gradient\n        optional: eta (default 0.1) max_iter (default 1000)\n    \"\"\"\n    history=[]  ## Each time a new w is considered, add it to this list\n    history.append(w)\n    for i in range(max_iter):\n        ### You complete me\n        w=w-eta*gradient(w,X,y)\n        history.append(w)\n    return w,np.array(history)\n","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":95,"id":"44d3a5","input":"### First we'll try the function on a made up regression problem.\n\nfrom sklearn.datasets import make_regression\n\nX_no_bias,y = make_regression(n_features=1,noise=10,random_state=100)\n\nplt.scatter(X_no_bias,y,alpha=0.6)\nplt.show()","output":{"0":{"data":{"image/png":"5fde7fb71bdd8c087220bf05859190d924fee55c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":95,"metadata":{"image/png":{"height":411,"width":718},"needs_background":"light"},"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":96,"id":"70f2b1","input":"## Add a bias column\nX = np.c_[np.ones(X_no_bias.shape[0]),X_no_bias]\nX[:5], X.shape,y.shape","output":{"0":{"data":{"text/plain":"(array([[ 1.        , -0.37690335],\n        [ 1.        , -1.1994512 ],\n        [ 1.        , -1.74976547],\n        [ 1.        ,  0.98132079],\n        [ 1.        , -1.70595201]]),\n (100, 2),\n (100,))"},"exec_count":96,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":97,"id":"bdbc6f","input":"\n## Get a train test split of the data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\n","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":98,"id":"bf571e","input":"\nw = np.linalg.pinv(X_train).dot(y_train) ### You complete me\n\ny_train_hat = X_train.dot(w)\nplt.scatter(X_train[:,1],y_train)\nplt.plot(X_train[:,1],y_train_hat,c='r')\nplt.title(\"The regression line learned by the normal eqns\")\nplt.show()","output":{"0":{"data":{"image/png":"43fb6d0d01031c15a7359fb307152aeb92eb99bb","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":98,"metadata":{"image/png":{"height":426,"width":718},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":99,"id":"0ec8fe","input":"def R2(y,yhat):\n    \"\"\"parameters: y, yhat\"\"\"\n    ## You complete me\n    ybar = np.mean(y)\n    \"\"\"  SStot = np.sum((y-yhat)**2)\n    SSres = np.sum((y-ybar)**2)\"\"\"\n    SSres = np.sum((y-yhat)**2)\n    SStot = np.sum((y-ybar)**2)\n    R2= 1-(SSres/SStot)\n    return R2\n    #ybar = np.mean(y)\n    #r2 = sum((y-yhat)**2)/sum((y-ybar)**2)\n    #r2 = 1-r2\n    #return r2\n\ny_test_hat = X_test.dot(w)\n\nprint(\"w = {}\".format(w))\nprint(\"R2 = {}\".format(R2(y_test,y_test_hat)))","output":{"0":{"name":"stdout","output_type":"stream","text":"w = [ 0.10939422 41.06580482]\nR2 = 0.9038150556357983\n"}},"pos":9,"type":"cell"}
{"cell_type":"markdown","id":"2fe7e0","input":"### Homework 3\n#### Gradient Descent\n\nIn this homework we will implement a basic version of gradient descent. \n\nBelow, look for unfinished functions that contain a line like\n\n```\n## You complete me\n```\n\nWhere those occur, fill in the missing code. \n\nWhen your code is correct the output should be similar to the output provided (see the HTML version of this document for backup output images).\n\nAs in all our exercises, please be sure your function works for input that has any dimensionality (changing $d$ and $N$ should not break the code).  \n\nReminder:  It's cheating to google the solution and use the code you find.  Don't pollute your brain by googling.  Just try to solve the problem. \n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"62de6c","input":"### A real dataset\n\nLet's see how your gradient descent performs on a real dataset, such as the Boston Housing data.\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"82a46b","input":"### Uh Oh\n\nWe ran into some problems here.\n\nLinear regression based on the normal equations is doing just fine, but gradient descent is struggling.\n\nWe had to put $\\eta$ all the way down to 0.000001 just to get convergence.\n\nBut then after 100000 iterations, we still haven't converged to a good solution.\n\nWhat's the problem?\n\nFix it!  (Hint: What should you always do to the data before doing gradient descent? You can take the code for this operation from HW 2, or use the sklearn library version.)\n\nWhen you fix it the output should look roughly like this:\n\n```\nw (normal equations )= [ 26.53112618 -10.30154035   4.11512828   2.44959865   2.11057155\n  -9.36246851  17.88281394   1.23610308 -13.15196355   7.6501691\n  -7.15185994  -9.86921614   3.64943825 -19.76860333]\nR2 (normal equations) = 0.7165453029604523\nw (gradient descent) = [ 19.9368617   -6.02022064   2.87944774   1.82739763   2.25551214\n  -6.37598968  22.22541246   1.55715796  -8.46547661   4.94478739\n  -4.63963461  -8.74471267   4.76136717 -18.60811081]\nR2 (gradient descent)= 0.7123144123485858\n```\n\nYou will need to put $\\eta$ and `max_iter` back down to sane values to get this output.","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"9e4745","input":"### Linear regression with the normal equations\n\nAs we've said in class, linear regression with the normal equations is a one-liner.\n\nPlease write that line in the cell below.\n\nYou want\n\n$$\\bar{w} = X_{train}^\\dagger y_{train}$$\n\nwhere $X_{train}^\\dagger$ is the pseudo-inverse of $X_{train}$.","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"a63082","input":"### Stochastic Gradient Descent\n\nIn this section you will implement stochastic gradient descent and apply it so some real data.\n\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"a6af88","input":"### Gradient Descent Optimization\n\nNow let's try to find the solution to this same problem using gradient descent.\n\nYou'll need the gradient of the mean squared error function which is on page 85 of the textbook.\n\nFor your convenience:\n\n$$ \\nabla E_{in}(\\bar{w}) = \\frac{2}{N}(X^TX\\bar{w}-X^Ty).$$\n\nRemember that the computer is not psychic and will not understand \"N\" which is the number of rows in $X$.\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"d61f8b","input":"### Assess performance\n\nBelow we use $R^2$ to assess the performance of the model.\n\nPlease help us out by implementing the `R2` function.\n\nYou can remind yourself of the definition here:\n\nhttps://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions\n\n\n","pos":7,"type":"cell"}
{"id":0,"time":1589216801488,"type":"user"}
{"last_load":1589214560863,"type":"file"}