{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83480576},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"81f0d7","input":"","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":384,"id":"98b21d","input":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nrandom_state = 42\nX,y = make_regression(n_samples=10, n_features=1, n_informative=1, n_targets=1, bias=3, effective_rank=None, tail_strength=0.5, noise=4, shuffle=True, coef=False, random_state=random_state)\nX[:5]","output":{"0":{"data":{"text/plain":"array([[-0.1382643 ],\n       [-0.46947439],\n       [ 0.76743473],\n       [-0.23413696],\n       [-0.23415337]])"},"exec_count":384,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":385,"id":"985cf2","input":"plt.scatter(X[:,-1],y)\nplt.show()","output":{"0":{"data":{"image/png":"71e77ddea814c9997214c0471c9f3270f8247cc0","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":385,"metadata":{"image/png":{"height":411,"width":712},"needs_background":"light"},"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":386,"id":"476c43","input":"## We will work with high dimensional polynomial features\n\nfrom sklearn.preprocessing import PolynomialFeatures\ndeg = 10\npf = PolynomialFeatures(deg)\nX = pf.fit_transform(X)\n","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":387,"id":"98f368","input":"#w = ### You complete me.  Implement regular linear regression using pinv.\nw  = np.linalg.pinv(X).dot(y)\nxx = np.linspace(min(X[:,1]),max(X[:,1]),num=200)\nXX = pf.fit_transform(xx.reshape(-1,1))\ncurve = XX.dot(w)\nplt.plot(xx,curve)\nplt.scatter(X[:,1],y)\nplt.ylim(-11,35)\nplt.show()\nplt.plot(xx,curve)\nplt.scatter(X[:,1],y)\nplt.show()","output":{"0":{"data":{"image/png":"5ef963579dfee3dff8338f093c0c18402034e6e7","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":387,"metadata":{"image/png":{"height":411,"width":712},"needs_background":"light"},"output_type":"execute_result"},"1":{"data":{"image/png":"971fefff88fc4b67d799b3c9e3465153ad0ca03c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":387,"metadata":{"image/png":{"height":421,"width":713},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":388,"id":"fc4994","input":"alpha = 10**(-2)\nA = np.eye(X.shape[1])\nA[0][0]=0\n\nw = np.linalg.pinv((X.T.dot(X) + alpha*A)).dot(X.T).dot(y)\n\n\nxx = np.linspace(min(X[:,1]),max(X[:,1]),num=200)\nXX = pf.fit_transform(xx.reshape(-1,1))\ncurve = XX.dot(w)\nplt.plot(xx,curve)\nplt.scatter(X[:,1],y)\nplt.ylim(-11,35)\nplt.show()\n","output":{"0":{"data":{"image/png":"af2cf010cee071d3a105f97c110e532f6baec35b","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":388,"metadata":{"image/png":{"height":411,"width":712},"needs_background":"light"},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":389,"id":"13746e","input":"import pandas as pd\ndf = pd.read_csv(\"dataset_2193_autoPrice.csv\")\nprint(df.shape)\ndf.head(30)","output":{"0":{"name":"stdout","output_type":"stream","text":"(159, 16)\n"},"1":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symboling</th>\n      <th>normalized-losses</th>\n      <th>wheel-base</th>\n      <th>length</th>\n      <th>width</th>\n      <th>height</th>\n      <th>curb-weight</th>\n      <th>engine-size</th>\n      <th>bore</th>\n      <th>stroke</th>\n      <th>compression-ratio</th>\n      <th>horsepower</th>\n      <th>peak-rpm</th>\n      <th>city-mpg</th>\n      <th>highway-mpg</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>164</td>\n      <td>99.8</td>\n      <td>176.6</td>\n      <td>66.2</td>\n      <td>54.3</td>\n      <td>2337</td>\n      <td>109</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>10.00</td>\n      <td>102</td>\n      <td>5500</td>\n      <td>24</td>\n      <td>30</td>\n      <td>13950</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>164</td>\n      <td>99.4</td>\n      <td>176.6</td>\n      <td>66.4</td>\n      <td>54.3</td>\n      <td>2824</td>\n      <td>136</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>8.00</td>\n      <td>115</td>\n      <td>5500</td>\n      <td>18</td>\n      <td>22</td>\n      <td>17450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>158</td>\n      <td>105.8</td>\n      <td>192.7</td>\n      <td>71.4</td>\n      <td>55.7</td>\n      <td>2844</td>\n      <td>136</td>\n      <td>3.19</td>\n      <td>3.40</td>\n      <td>8.50</td>\n      <td>110</td>\n      <td>5500</td>\n      <td>19</td>\n      <td>25</td>\n      <td>17710</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>158</td>\n      <td>105.8</td>\n      <td>192.7</td>\n      <td>71.4</td>\n      <td>55.9</td>\n      <td>3086</td>\n      <td>131</td>\n      <td>3.13</td>\n      <td>3.40</td>\n      <td>8.30</td>\n      <td>140</td>\n      <td>5500</td>\n      <td>17</td>\n      <td>20</td>\n      <td>23875</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>192</td>\n      <td>101.2</td>\n      <td>176.8</td>\n      <td>64.8</td>\n      <td>54.3</td>\n      <td>2395</td>\n      <td>108</td>\n      <td>3.50</td>\n      <td>2.80</td>\n      <td>8.80</td>\n      <td>101</td>\n      <td>5800</td>\n      <td>23</td>\n      <td>29</td>\n      <td>16430</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>192</td>\n      <td>101.2</td>\n      <td>176.8</td>\n      <td>64.8</td>\n      <td>54.3</td>\n      <td>2395</td>\n      <td>108</td>\n      <td>3.50</td>\n      <td>2.80</td>\n      <td>8.80</td>\n      <td>101</td>\n      <td>5800</td>\n      <td>23</td>\n      <td>29</td>\n      <td>16925</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>188</td>\n      <td>101.2</td>\n      <td>176.8</td>\n      <td>64.8</td>\n      <td>54.3</td>\n      <td>2710</td>\n      <td>164</td>\n      <td>3.31</td>\n      <td>3.19</td>\n      <td>9.00</td>\n      <td>121</td>\n      <td>4250</td>\n      <td>21</td>\n      <td>28</td>\n      <td>20970</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>188</td>\n      <td>101.2</td>\n      <td>176.8</td>\n      <td>64.8</td>\n      <td>54.3</td>\n      <td>2765</td>\n      <td>164</td>\n      <td>3.31</td>\n      <td>3.19</td>\n      <td>9.00</td>\n      <td>121</td>\n      <td>4250</td>\n      <td>21</td>\n      <td>28</td>\n      <td>21105</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>121</td>\n      <td>88.4</td>\n      <td>141.1</td>\n      <td>60.3</td>\n      <td>53.2</td>\n      <td>1488</td>\n      <td>61</td>\n      <td>2.91</td>\n      <td>3.03</td>\n      <td>9.50</td>\n      <td>48</td>\n      <td>5100</td>\n      <td>47</td>\n      <td>53</td>\n      <td>5151</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>98</td>\n      <td>94.5</td>\n      <td>155.9</td>\n      <td>63.6</td>\n      <td>52.0</td>\n      <td>1874</td>\n      <td>90</td>\n      <td>3.03</td>\n      <td>3.11</td>\n      <td>9.60</td>\n      <td>70</td>\n      <td>5400</td>\n      <td>38</td>\n      <td>43</td>\n      <td>6295</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>81</td>\n      <td>94.5</td>\n      <td>158.8</td>\n      <td>63.6</td>\n      <td>52.0</td>\n      <td>1909</td>\n      <td>90</td>\n      <td>3.03</td>\n      <td>3.11</td>\n      <td>9.60</td>\n      <td>70</td>\n      <td>5400</td>\n      <td>38</td>\n      <td>43</td>\n      <td>6575</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>118</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.8</td>\n      <td>1876</td>\n      <td>90</td>\n      <td>2.97</td>\n      <td>3.23</td>\n      <td>9.41</td>\n      <td>68</td>\n      <td>5500</td>\n      <td>37</td>\n      <td>41</td>\n      <td>5572</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>118</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.8</td>\n      <td>1876</td>\n      <td>90</td>\n      <td>2.97</td>\n      <td>3.23</td>\n      <td>9.40</td>\n      <td>68</td>\n      <td>5500</td>\n      <td>31</td>\n      <td>38</td>\n      <td>6377</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>118</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.8</td>\n      <td>2128</td>\n      <td>98</td>\n      <td>3.03</td>\n      <td>3.39</td>\n      <td>7.60</td>\n      <td>102</td>\n      <td>5500</td>\n      <td>24</td>\n      <td>30</td>\n      <td>7957</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>148</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.6</td>\n      <td>1967</td>\n      <td>90</td>\n      <td>2.97</td>\n      <td>3.23</td>\n      <td>9.40</td>\n      <td>68</td>\n      <td>5500</td>\n      <td>31</td>\n      <td>38</td>\n      <td>6229</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1</td>\n      <td>148</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.6</td>\n      <td>1989</td>\n      <td>90</td>\n      <td>2.97</td>\n      <td>3.23</td>\n      <td>9.40</td>\n      <td>68</td>\n      <td>5500</td>\n      <td>31</td>\n      <td>38</td>\n      <td>6692</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>148</td>\n      <td>93.7</td>\n      <td>157.3</td>\n      <td>63.8</td>\n      <td>50.6</td>\n      <td>1989</td>\n      <td>90</td>\n      <td>2.97</td>\n      <td>3.23</td>\n      <td>9.40</td>\n      <td>68</td>\n      <td>5500</td>\n      <td>31</td>\n      <td>38</td>\n      <td>7609</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-1</td>\n      <td>110</td>\n      <td>103.3</td>\n      <td>174.6</td>\n      <td>64.6</td>\n      <td>59.8</td>\n      <td>2535</td>\n      <td>122</td>\n      <td>3.34</td>\n      <td>3.46</td>\n      <td>8.50</td>\n      <td>88</td>\n      <td>5000</td>\n      <td>24</td>\n      <td>30</td>\n      <td>8921</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3</td>\n      <td>145</td>\n      <td>95.9</td>\n      <td>173.2</td>\n      <td>66.3</td>\n      <td>50.2</td>\n      <td>2811</td>\n      <td>156</td>\n      <td>3.60</td>\n      <td>3.90</td>\n      <td>7.00</td>\n      <td>145</td>\n      <td>5000</td>\n      <td>19</td>\n      <td>24</td>\n      <td>12964</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2</td>\n      <td>137</td>\n      <td>86.6</td>\n      <td>144.6</td>\n      <td>63.9</td>\n      <td>50.8</td>\n      <td>1713</td>\n      <td>92</td>\n      <td>2.91</td>\n      <td>3.41</td>\n      <td>9.60</td>\n      <td>58</td>\n      <td>4800</td>\n      <td>49</td>\n      <td>54</td>\n      <td>6479</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>137</td>\n      <td>86.6</td>\n      <td>144.6</td>\n      <td>63.9</td>\n      <td>50.8</td>\n      <td>1819</td>\n      <td>92</td>\n      <td>2.91</td>\n      <td>3.41</td>\n      <td>9.20</td>\n      <td>76</td>\n      <td>6000</td>\n      <td>31</td>\n      <td>38</td>\n      <td>6855</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>101</td>\n      <td>93.7</td>\n      <td>150.0</td>\n      <td>64.0</td>\n      <td>52.6</td>\n      <td>1837</td>\n      <td>79</td>\n      <td>2.91</td>\n      <td>3.07</td>\n      <td>10.10</td>\n      <td>60</td>\n      <td>5500</td>\n      <td>38</td>\n      <td>42</td>\n      <td>5399</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>101</td>\n      <td>93.7</td>\n      <td>150.0</td>\n      <td>64.0</td>\n      <td>52.6</td>\n      <td>1940</td>\n      <td>92</td>\n      <td>2.91</td>\n      <td>3.41</td>\n      <td>9.20</td>\n      <td>76</td>\n      <td>6000</td>\n      <td>30</td>\n      <td>34</td>\n      <td>6529</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1</td>\n      <td>101</td>\n      <td>93.7</td>\n      <td>150.0</td>\n      <td>64.0</td>\n      <td>52.6</td>\n      <td>1956</td>\n      <td>92</td>\n      <td>2.91</td>\n      <td>3.41</td>\n      <td>9.20</td>\n      <td>76</td>\n      <td>6000</td>\n      <td>30</td>\n      <td>34</td>\n      <td>7129</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0</td>\n      <td>110</td>\n      <td>96.5</td>\n      <td>163.4</td>\n      <td>64.0</td>\n      <td>54.5</td>\n      <td>2010</td>\n      <td>92</td>\n      <td>2.91</td>\n      <td>3.41</td>\n      <td>9.20</td>\n      <td>76</td>\n      <td>6000</td>\n      <td>30</td>\n      <td>34</td>\n      <td>7295</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0</td>\n      <td>78</td>\n      <td>96.5</td>\n      <td>157.1</td>\n      <td>63.9</td>\n      <td>58.3</td>\n      <td>2024</td>\n      <td>92</td>\n      <td>2.92</td>\n      <td>3.41</td>\n      <td>9.20</td>\n      <td>76</td>\n      <td>6000</td>\n      <td>30</td>\n      <td>34</td>\n      <td>7295</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0</td>\n      <td>106</td>\n      <td>96.5</td>\n      <td>167.5</td>\n      <td>65.2</td>\n      <td>53.3</td>\n      <td>2236</td>\n      <td>110</td>\n      <td>3.15</td>\n      <td>3.58</td>\n      <td>9.00</td>\n      <td>86</td>\n      <td>5800</td>\n      <td>27</td>\n      <td>33</td>\n      <td>7895</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0</td>\n      <td>106</td>\n      <td>96.5</td>\n      <td>167.5</td>\n      <td>65.2</td>\n      <td>53.3</td>\n      <td>2289</td>\n      <td>110</td>\n      <td>3.15</td>\n      <td>3.58</td>\n      <td>9.00</td>\n      <td>86</td>\n      <td>5800</td>\n      <td>27</td>\n      <td>33</td>\n      <td>9095</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n      <td>85</td>\n      <td>96.5</td>\n      <td>175.4</td>\n      <td>65.2</td>\n      <td>54.1</td>\n      <td>2304</td>\n      <td>110</td>\n      <td>3.15</td>\n      <td>3.58</td>\n      <td>9.00</td>\n      <td>86</td>\n      <td>5800</td>\n      <td>27</td>\n      <td>33</td>\n      <td>8845</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0</td>\n      <td>85</td>\n      <td>96.5</td>\n      <td>175.4</td>\n      <td>62.5</td>\n      <td>54.1</td>\n      <td>2372</td>\n      <td>110</td>\n      <td>3.15</td>\n      <td>3.58</td>\n      <td>9.00</td>\n      <td>86</td>\n      <td>5800</td>\n      <td>27</td>\n      <td>33</td>\n      <td>10295</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"    symboling  normalized-losses  wheel-base  length  width  height  \\\n0           2                164        99.8   176.6   66.2    54.3   \n1           2                164        99.4   176.6   66.4    54.3   \n2           1                158       105.8   192.7   71.4    55.7   \n3           1                158       105.8   192.7   71.4    55.9   \n4           2                192       101.2   176.8   64.8    54.3   \n5           0                192       101.2   176.8   64.8    54.3   \n6           0                188       101.2   176.8   64.8    54.3   \n7           0                188       101.2   176.8   64.8    54.3   \n8           2                121        88.4   141.1   60.3    53.2   \n9           1                 98        94.5   155.9   63.6    52.0   \n10          0                 81        94.5   158.8   63.6    52.0   \n11          1                118        93.7   157.3   63.8    50.8   \n12          1                118        93.7   157.3   63.8    50.8   \n13          1                118        93.7   157.3   63.8    50.8   \n14          1                148        93.7   157.3   63.8    50.6   \n15          1                148        93.7   157.3   63.8    50.6   \n16          1                148        93.7   157.3   63.8    50.6   \n17         -1                110       103.3   174.6   64.6    59.8   \n18          3                145        95.9   173.2   66.3    50.2   \n19          2                137        86.6   144.6   63.9    50.8   \n20          2                137        86.6   144.6   63.9    50.8   \n21          1                101        93.7   150.0   64.0    52.6   \n22          1                101        93.7   150.0   64.0    52.6   \n23          1                101        93.7   150.0   64.0    52.6   \n24          0                110        96.5   163.4   64.0    54.5   \n25          0                 78        96.5   157.1   63.9    58.3   \n26          0                106        96.5   167.5   65.2    53.3   \n27          0                106        96.5   167.5   65.2    53.3   \n28          0                 85        96.5   175.4   65.2    54.1   \n29          0                 85        96.5   175.4   62.5    54.1   \n\n    curb-weight  engine-size  bore  stroke  compression-ratio  horsepower  \\\n0          2337          109  3.19    3.40              10.00         102   \n1          2824          136  3.19    3.40               8.00         115   \n2          2844          136  3.19    3.40               8.50         110   \n3          3086          131  3.13    3.40               8.30         140   \n4          2395          108  3.50    2.80               8.80         101   \n5          2395          108  3.50    2.80               8.80         101   \n6          2710          164  3.31    3.19               9.00         121   \n7          2765          164  3.31    3.19               9.00         121   \n8          1488           61  2.91    3.03               9.50          48   \n9          1874           90  3.03    3.11               9.60          70   \n10         1909           90  3.03    3.11               9.60          70   \n11         1876           90  2.97    3.23               9.41          68   \n12         1876           90  2.97    3.23               9.40          68   \n13         2128           98  3.03    3.39               7.60         102   \n14         1967           90  2.97    3.23               9.40          68   \n15         1989           90  2.97    3.23               9.40          68   \n16         1989           90  2.97    3.23               9.40          68   \n17         2535          122  3.34    3.46               8.50          88   \n18         2811          156  3.60    3.90               7.00         145   \n19         1713           92  2.91    3.41               9.60          58   \n20         1819           92  2.91    3.41               9.20          76   \n21         1837           79  2.91    3.07              10.10          60   \n22         1940           92  2.91    3.41               9.20          76   \n23         1956           92  2.91    3.41               9.20          76   \n24         2010           92  2.91    3.41               9.20          76   \n25         2024           92  2.92    3.41               9.20          76   \n26         2236          110  3.15    3.58               9.00          86   \n27         2289          110  3.15    3.58               9.00          86   \n28         2304          110  3.15    3.58               9.00          86   \n29         2372          110  3.15    3.58               9.00          86   \n\n    peak-rpm  city-mpg  highway-mpg  class  \n0       5500        24           30  13950  \n1       5500        18           22  17450  \n2       5500        19           25  17710  \n3       5500        17           20  23875  \n4       5800        23           29  16430  \n5       5800        23           29  16925  \n6       4250        21           28  20970  \n7       4250        21           28  21105  \n8       5100        47           53   5151  \n9       5400        38           43   6295  \n10      5400        38           43   6575  \n11      5500        37           41   5572  \n12      5500        31           38   6377  \n13      5500        24           30   7957  \n14      5500        31           38   6229  \n15      5500        31           38   6692  \n16      5500        31           38   7609  \n17      5000        24           30   8921  \n18      5000        19           24  12964  \n19      4800        49           54   6479  \n20      6000        31           38   6855  \n21      5500        38           42   5399  \n22      6000        30           34   6529  \n23      6000        30           34   7129  \n24      6000        30           34   7295  \n25      6000        30           34   7295  \n26      5800        27           33   7895  \n27      5800        27           33   9095  \n28      5800        27           33   8845  \n29      5800        27           33  10295  "},"exec_count":389,"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":390,"id":"8d222e","input":"X = df.values[:,:-1]\n#print(X)\ny = df.values[:,-1]\n\n#print(y)\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n\n\n\nss.fit(X_train)\nX_train = ss.transform(X_train)\nX_test = ss.transform(X_test)\n\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]\n\n#print(X_train)\n#print(X_test)\n\n\n\n\n","pos":11,"type":"cell"}
{"cell_type":"code","exec_count":391,"id":"de2d8c","input":"from sklearn.metrics import r2_score as r2\nfrom sklearn.metrics import mean_absolute_error\n\n### You complete me\n\n\"\"\"\nProblem 2: Tune up\n+Find the value of alpha α that gives the best R^2 performance on the test set.\n+Report on the value of alpha α as well as the corresponding R^2 score.\n\n+Hint: Not regularizing the bias improves outcomes significantly for this dataset.\n+Suggestion: Try alpha = 2^k  for k ranging -8 to 8.\n\"\"\"\n\n\n\n\"\"\"\nA = np.eye(X.shape[1])\nA[0][0]=0\n\"\"\"\n\n#Using X train\n#print(X_train.shape[1])\nA = np.eye(X_train.shape[1])\n\n#print(A)\nA[0][0]=0 #putting a zero at (0,0) in the matrix\n#print(A)\n\n#w  = np.linalg.pinv((X.T.dot(X) + alpha*A)). dot(X.T).dot(y)\n\nfor alpha in (2**k for k in range(-8,9)):             #9 as in  = 9-1 = 8  // -8 - 8 \n    w  = np.linalg.pinv((X_train.T.dot(X_train) + alpha*A)). dot(X_train.T).dot(y_train)\n    y_hat = X_test.dot(w)\n    print(\"The value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α (\", alpha,\") | R2 (\", r2(y_test,y_hat),\") | MAE (\",mean_absolute_error(y_test,y_hat),\")\")\n\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"The value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.00390625 ) | R2 ( 0.7112251290406393 ) | MAE ( 2032.1664466639281 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.0078125 ) | R2 ( 0.7112667159743123 ) | MAE ( 2032.030253528781 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.015625 ) | R2 ( 0.7113497487010523 ) | MAE ( 2031.75842331381 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.03125 ) | R2 ( 0.7115152526213016 ) | MAE ( 2031.216970557567 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.0625 ) | R2 ( 0.7118440381611134 ) | MAE ( 2030.1427658144428 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.125 ) | R2 ( 0.7124929045339145 ) | MAE ( 2028.0281577457827 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.25 ) | R2 ( 0.7137572034661774 ) | MAE ( 2023.9266922081028 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 0.5 ) | R2 ( 0.716161897857546 ) | MAE ( 2016.1828031539048 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 1 ) | R2 ( 0.7205392230418579 ) | MAE ( 2002.208423847281 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 2 ) | R2 ( 0.7279236547769271 ) | MAE ( 1979.8338719194592 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 4 ) | R2 ( 0.7389124924704352 ) | MAE ( 1953.2263298893909 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 8 ) | R2 ( 0.7524044233939903 ) | MAE ( 1924.128854629366 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 16 ) | R2 ( 0.7653135468180049 ) | MAE ( 1888.8739569479208 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 32 ) | R2 ( 0.7754946654182069 ) | MAE ( 1843.089646774945 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 64 ) | R2 ( 0.7842407051280998 ) | MAE ( 1780.58520041362 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 128 ) | R2 ( 0.7914971623027429 ) | MAE ( 1719.7175310774903 )\nThe value of *Alpha*,*R^2 Score*, and *Mean Absolute Error* : α ( 256 ) | R2 ( 0.7856367165525879 ) | MAE ( 1750.1113470501873 )\n"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":392,"id":"c69b80","input":"from sklearn.metrics import r2_score as r2\nfrom sklearn.metrics import mean_absolute_error\n\n\n#gradient g\n#you might want to make the first coordinate of the gradient 0 instead of N\n\"\"\"\nX_train.shape (119,16)\nX.shape (159, 15)\nprint (X.T.dot(X).shape) == (15,15)\n\"\"\"\ndef g_l1(w,X,y,alpha):\n    ## You complete me\n    gradient_G = 2/X.shape[0]*(X.T.dot(X).dot(w)-X.T.dot(y)) + alpha*np.sign(w)\n    return gradient_G\n\ndef grad_descent(w,X,y,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6)):\n    \"\"\" parameters: w, gradient\n    optional: eta (default 0.1) max_iter (default 1000)\n    \"\"\" \n    history=[]\n    history.append(w)\n    for i in range(max_iter):\n        eta = eta*(1-schedule) ## Decreasing eta over time\n        w = w-eta*gradient(w,X,y,alpha)\n        history.append(w)\n    return w,np.array(history)\n\n# run g first\ng = g_l1\n\nfor alpha in [2**k for k in [0,1,2,3,4,5,6,7,8,9,10,11,12]]:\n    w = np.zeros(X_train.shape[1])\n    w,path = grad_descent(w,X_train,y_train,g,alpha,max_iter=3000)\n    y_hat = X_test.dot(w)\n    print(\"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α (\", alpha,\")  | R2 (\", r2(y_test,y_hat),\")  | MAE (\",mean_absolute_error(y_test,y_hat),\")\")\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 1 )  | R2 ( 0.7115431830859413 )  | MAE ( 2030.3150599420442 )\n"},"1":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 2 )  | R2 ( 0.7118999093214395 )  | MAE ( 2028.327310621571 )\nReporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 4 )  | R2 ( 0.7126044805476552 )  | MAE ( 2024.3518128800126 )\n"},"2":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 8 )  | R2 ( 0.7136664711668712 )  | MAE ( 2020.3322387216067 )\n"},"3":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 16 )  | R2 ( 0.7157487396466317 )  | MAE ( 2012.412278267763 )\nReporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 32 )  | R2 ( 0.7201989703629005 )  | MAE ( 1997.9918666804347 )\n"},"4":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 64 )  | R2 ( 0.7275868560286988 )  | MAE ( 1981.212298975433 )\nReporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 128 )  | R2 ( 0.7420700072102878 )  | MAE ( 1952.7045041472902 )\n"},"5":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 256 )  | R2 ( 0.7610979651436096 )  | MAE ( 1912.195287682909 )\nReporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 512 )  | R2 ( 0.7621570128889443 )  | MAE ( 1952.2513599573401 )\n"},"6":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 1024 )  | R2 ( 0.8039276703959707 )  | MAE ( 1644.5206583570375 )\nReporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 2048 )  | R2 ( 0.6997807951266672 )  | MAE ( 2013.3572301067688 )\n"},"7":{"name":"stdout","output_type":"stream","text":"Reporting on the value of *Alpha* as well as the corresponding *R^2 Score* and *Mean Absolute Error* :    α ( 4096 )  | R2 ( 0.2969960058258734 )  | MAE ( 2877.642871086836 )\n"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":393,"id":"a278db","input":"\n### Please use the values already here rather than rerunning.\n\nalpha=2**10\n#print(w)\nw = np.zeros(X_train.shape[1])\nw,path = grad_descent(w,X_train,y_train,g,alpha,eta=0.1,max_iter=3000,schedule=1/100)\n\n\n\nprint(\"alpha :\",alpha,\"\\n\\nw :\",w)\n\n#copying w to list_w\nlist_w = w\ndf.columns[~np.isclose(list_w,0)]","output":{"0":{"name":"stdout","output_type":"stream","text":"alpha : 1024 \n\nw : [ 1.09304622e+04  4.32599231e-13  2.47778247e+02  1.46160686e-11\n  1.17792467e-11  1.65776906e+03  5.56261488e-12  1.83944273e+03\n  1.73869530e+03  1.42415849e-12  3.89913396e-12  8.88784656e-13\n  7.55945568e-12 -5.11097925e-12 -1.27363709e-11 -7.54546589e-12]\n"},"1":{"data":{"text/plain":"Index(['symboling', 'wheel-base', 'height', 'engine-size', 'bore'], dtype='object')"},"exec_count":393,"output_type":"execute_result"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":394,"id":"8beab9","input":"#import numpy as np\n\nnp.random.seed(42)\n\nw = np.random.randn(X_train.shape[1])\n\n#alpha is 800??? I was curious what the actual value of alpha is.\n\nw,path = grad_descent(w,X_train,y_train,g_l1,alpha=500,eta=0.1,max_iter=3000,schedule=10**(-3))\n\n#y_test_hats\ny_test_hats = [X_test.dot(wp) for wp in path]\n\n#r2scores\nr2scores = [r2(yth,y_test) for yth in y_test_hats]\n\n#y_train_hats\ny_train_hats = [X_train.dot(wp) for wp in path]\n\n#r2scores_train\nr2scores_train = [r2(yth,y_train) for yth in y_train_hats]\n\n\nnum = len(y_test_hats)\n\nplt.plot(range(num),r2scores,label=\"training set\")\nplt.plot(range(num),r2scores_train,label=\"testing set\")\n\nplt.ylim(0.5,.8)\nplt.xlim(0,20)\n\nplt.title(\"Test error (R2) as a function of iteration number\")\nplt.xlabel(\"iteration\")\nplt.ylabel(\"R2 score\")\n\nplt.legend()\nplt.show()\n","output":{"0":{"data":{"image/png":"5bcba55be6ab985ab97bd29a80611328178d8c71","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":394,"metadata":{"image/png":{"height":440,"width":738},"needs_background":"light"},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":395,"id":"255bb2","input":"best_weight_index = np.argmax(r2scores)\n\n\nwbest = path[best_weight_index]\n\n\nbest_score = r2scores[best_weight_index]\n\n\nprint(\"best score, last score\")\n\n\nbest_score, r2scores[-1]\n","output":{"0":{"name":"stdout","output_type":"stream","text":"best score, last score\n"},"1":{"data":{"text/plain":"(0.7985062473094278, 0.7573558892529373)"},"exec_count":395,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":396,"id":"9baa84","input":"best_weight_index","output":{"0":{"data":{"text/plain":"20"},"exec_count":396,"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":397,"id":"08b4d7","input":"## There is some jitter here, which is typical of LASSO\nplt.plot(range(num)[max(best_weight_index-100,0):best_weight_index+100],r2scores[max(best_weight_index-100,0):best_weight_index+100])\nplt.ylim(0,1)\nplt.xlim(0,24)\nplt.title(\"Test error (R2) as a function of iteration number\")\nplt.xlabel(\"iteration\")\nplt.ylabel(\"R2 score\")\nplt.show()","output":{"0":{"data":{"image/png":"2d945a9c4797410a9b741bea26765e137eb65ebd","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":397,"metadata":{"image/png":{"height":440,"width":720},"needs_background":"light"},"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":399,"id":"c629a6","input":"metric = lambda yhat,y: 1-accuracy_score(yhat,y)  ### high values are bad\nmetric([1,2],[3,4])\n#y_hat_test","output":{"0":{"data":{"text/plain":"1.0"},"exec_count":399,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":408,"id":"2c6de6","input":"\nimport statistics\nfrom sklearn.metrics import mean_squared_error\nmetric = mean_squared_error   ## high values are bad\n#metric = r2   ## high values are bad\ngradient = g_l1\n\n#metric = lambda yhat,y: 1-accuracy_score(yhat,y)   <-- kept throwing error\n\n\ndef grad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha=800,eta=0.1,max_iter=3000,schedule=10**(-3)):\n    for i in range(max_iter):\n        #You are welcome to call the existing gradient descent function as a subroutine in this new function.\n        w,path = grad_descent(w,X_train,y_train,gradient=gradient,alpha=alpha,eta=eta,max_iter=max_iter-1,schedule=schedule)\n        #after receving the first run grad_descent, we have recieved the last w found from line 24 \n        y_test_hats = [X_test.dot(w) for w in path]\n        #print(\"y_test_hats: \",y_test_hats)\n        scores = [metric(yth,y_test) for yth in y_test_hats]\n        #print(\"scores: \",scores)\n        best_weight_index = np.argmax(scores)\n        #print(\"best_weight_index: \",best_weight_index)\n        wbest = path[best_weight_index]  # w,path == w,history \n        #print(\"wbest: \",wbest)\n        if len(scores) >= 7:\n            if statistics.mean(scores[-6:-1])<statistics.mean(scores[-7:-2]):\n                break\n    return path[best_weight_index], best_weight_index\n\n\nw = np.random.randn(X_train.shape[1])\nwbest, iteration = grad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6))\nyhat = X_test.dot(wbest)\nprint(\"Best: \", r2(yhat, y_test),\"Iteration: \",iteration)\n\n\n\n\n\nnum = len(y_test_hats)\n\nplt.plot(range(num),r2scores,label=\"training set\")\nplt.plot(range(num),r2scores_train,label=\"testing set\")\n\nplt.ylim(0,1)\n\nplt.title(\"Test error (R2) as a function of iteration number\")\nplt.xlabel(\"iteration\")\nplt.ylabel(\"R2 score\")\n\nplt.legend()\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Best:  0.8044989303121156 Iteration:  343\n"},"1":{"data":{"image/png":"ce66352f6fc5549b72316529cc020849f10c575e","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":408,"metadata":{"image/png":{"height":440,"width":720},"needs_background":"light"},"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":412,"id":"b0e3e7","input":"\"\"\"\ny_test_hats = [X_test.dot(wp) for wp in path]\n\n\nr2scores = [r2(yth,y_test) for yth in y_test_hats]\n\n\n\ny_train_hats = [X_train.dot(wp) for wp in path]\nr2scores_train = [r2(yth,y_train) for yth in y_train_hats]\n\nbest_weight_index = np.argmax(r2scores)\nwbest = path[best_weight_index]\nbest_score = r2scores[best_weight_index]\n\n1) stop when the trailing 7 iteration average of test performance starts decreasing and return the best performing weight so far\n2) stop when the performance is 1% worse than the best performance observed so far and return the best weight yet encountered.\n\n\"\"\"\n##Regression case\n\n\nimport statistics\nfrom sklearn.metrics import accuracy_score\nmetric = r2   ## high values are bad\ngradient = g_l1\n\n#metric = lambda yhat,y: 1-accuracy_score(yhat,y)   <-- kept throwing error\n\n\ndef grad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha=800,eta=0.1,max_iter=3000,schedule=10**(-3)):\n    for i in range(max_iter):\n        #You are welcome to call the existing gradient descent function as a subroutine in this new function.\n        w,path = grad_descent(w,X_train,y_train,gradient=gradient,alpha=alpha,eta=eta,max_iter=max_iter-1,schedule=schedule)\n        #after receving the first run grad_descent, we have recieved the last w found from line 24 \n        y_test_hats = [X_test.dot(w) for w in path]\n        #print(\"y_test_hats: \",y_test_hats)\n        scores = [metric(yth,y_test) for yth in y_test_hats]\n        #print(\"scores: \",scores)\n        best_weight_index = np.argmax(scores)\n        #print(\"best_weight_index: \",best_weight_index)\n        wbest = path[best_weight_index]  # w,path == w,history \n        #print(\"wbest: \",wbest)\n        if len(scores) >= 7:\n            if statistics.mean(scores[-6:-1])<statistics.mean(scores[-7:-2]):\n                break\n    return path[best_weight_index], best_weight_index\n\n\nw = np.random.randn(X_train.shape[1])\nwbest, iteration = grad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6))\nyhat = X_test.dot(wbest)\nprint(\"Best: \", r2(yhat, y_test),\"Iteration: \",iteration)\n\n\n\n\n\nnum = len(y_test_hats)\n\nplt.plot(range(num),r2scores,label=\"training set\")\nplt.plot(range(num),r2scores_train,label=\"testing set\")\n\nplt.ylim(0.73,1)\n\nplt.title(\"Test error (R2) as a function of iteration number\")\nplt.xlabel(\"iteration\")\nplt.ylabel(\"R2 score\")\n\nplt.legend()\nplt.show()","output":{"0":{"name":"stdout","output_type":"stream","text":"Best:  0.8092307097374252 Iteration:  257\n"},"1":{"data":{"image/png":"89b1d5cd33468dd97fdd34c85856b3d85fe91a6b","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":412,"metadata":{"image/png":{"height":440,"width":726},"needs_background":"light"},"output_type":"execute_result"}},"pos":27,"scrolled":true,"type":"cell"}
{"cell_type":"markdown","id":"1dc719","input":"### Real data\n\nLet's try regularized linear regression with polynomial features on some real datasets.\n\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"25ed4d","input":"## Regularization\n\nIn this assignment you will practice using regularization.\n\nHere is our video playlist on the topic of regularization:\n\nhttps://www.youtube.com/playlist?list=PL0LaMcMQNvEj4y0ieMIWFbHxelZgLdemh\n\nThere is also a good discussion of this topic in *Hands-On Machine Learning* and in our textbook *Learning from Data*.\n\nAdditionally you have some worksheets in the following Cocalc directories that might be helpful:\n\n```\nSlides/Bias-Variance\nSlides/Zspace\nSlides/Regularization\nActivities/RegConcreteExample\n```\n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"3741d1","input":"### Early stopping\n\nWe said that the basic form of regularization is\n\n$$E_{aug}(\\bar{w}) = E_{in}(\\bar{w}) + \\Omega(\\bar{w})$$\n\nwhere $\\Omega(\\bar{w})$ is some kind of complexity penalty.\n\nSo far we've explored using L2 and L1 norms for the complexity penalty.\n\n\nConsider that as we perform gradient descent we explore a series of weights $\\bar{w}_1,\\bar{w}_2,\\ldots,\\bar{w}_{T}$, where the descent process goes on for $T$ iterations.\n\nOne way of defining \"complexity\" for a $\\bar{w}_t$ in this list might just be the size of its index $t$.\n\nIn other words, as the descent process goes on the $\\bar{w}$ become more and more \"complex\" and might begin to overfit the training data.\n\nIn fact this can clearly be seen to happen in real life as the plots below demonstrate.\n\nThe best **testing** score encountered often happens early.\n\nIt is possible to modify gradient descent so that the process stops when the performance on the test set begins to go down.\n\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"3dffc2","input":"### Problem 4: LASSO for feature selection\n\nLASSO is famous for eliminating certain features.\n\nLook below and observe that only a few dimensions of the weight vector have values that are significantly above zero.\n\nWhat are the most important features?\n\n(Hint: be sure to consider the bias weight)","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"41149c","input":"### Problem 3: LASSO regression\n\nBelow we will use gradient descent to implement LASSO regression.\n\nRecall that the error function for LASSO is\n\n$E_{in}(\\bar{w}) = MSE(\\bar{w}) + \\alpha\\sum_{i=1}^d |w_i|$\n\nwhere MSE= Mean Squared Error.\n\nTo perform gradient descent we need $\\nabla E_{in}$.\n\nThis is just \n\n\n$\\nabla E_{in}(\\bar{w}) = \\nabla MSE(\\bar{w}) + \\nabla \\alpha\\sum_{i=1}^d |w_i|$\n\n\nFor your convenience:\n\n$\\nabla MSE(\\bar{w}) = \\frac{2}{N}(X^TX\\bar{w} - X^T\\bar{y})$.\n\n\nWhat is $\\nabla \\alpha\\sum_{i=1}^d |w_i|$?\n\nThis depends on $\\frac{d}{dx} |x|$.\n\nThat derivative is -1 if $x<0$, and 1 if $x > 0$.  If $x=0$ it is undefined.\n\nBut we will make it true by fiat that $\\frac{d}{dx} |0| = 0$, even though it isn't really true.\n\nThis is an example of a [subderivative](https://en.wikipedia.org/wiki/Subderivative) and it works well in practice.\n\nWith this \"cheat\", notice that $\\nabla \\alpha\\sum_{i=1}^d |w_i| = \\alpha sign(\\bar{w})$.\n\nLike with ridge regression, you might want to make the first coordinate of the gradient 0 if you do not want to penalize the bias weight.  (For this dataset penalizing the bias actually helps a little).\n\nFinish implementing LASSO below by implementing the gradient `g`.\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"4c61ad","input":"# Froming looking above, I observed the 5 features are above 0. From running line 16, we are able to see 'symboling', 'wheel-base', 'height', 'engine-size' and 'bore' are the most important features. But I do not think symboling is an important feature? Because the values for \"symboling\" are from 0 to 2 and it is just a mark or character used as a conventional representation of an object, function, or process. #\n","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"4d361b","input":"### Implementing early stopping\n\nImplement a version of gradient descent with this signature:\n\n```\ndef grad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6)):\n```\n\nThis function should work like regular gradient descent, but it should return the weight $\\bar{w}$ that achieves the best performance on `X_test` and `y_test` rather than the last $\\bar{w}$ found.\n\nIt is okay to stop `grad_descent_es` when test performance begins to degrade.\n\nThis function takes the test data as extra parameters as well as a new parameter called `metric`.  This is the function that will be used to evaluate performance.\n\nFor example in the case of classification you might do\n\n```\nfrom sklearn.metrics import accuracy_score\nmetric = lambda yhat,y: 1-accuracy_score(yhat,y)  ### high values are bad\ngrad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6))\n```\n(This uses a python [lambda](https://realpython.com/python-lambda/) expression, not the hyperparameter lambda)\n\nOn the other hand for regression you could write\n\n```\nfrom sklearn.metrics import mean_squared_error\nmetric = mean_squared_error   ## high values are bad\ngrad_descent_es(w,X_train,y_train,X_test,y_test,metric,gradient,alpha,eta=0.1,max_iter=1000,schedule=10**(-6))\n```\n\nWithin the `grad_descent_es` you can use `metric` like this:\n\n```\nperformance = metric(y_hat_test,y_test)\n```\n\nwhere `y_hat_test` is the set of predictions the model makes on `X_test`. \n\n\nYou are welcome to call the existing gradient descent function as a subroutine in this new function.  \n\n","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"74c6dd","input":"## $\\nabla MSE(\\bar{w}) = \\frac{2}{N}(X^TX\\bar{w} - X^T\\bar{y})$. ##\n## With this \"cheat\", notice that $\\nabla \\alpha\\sum_{i=1}^d |w_i| = \\alpha sign(\\bar{w})$. ##\n\n## Like with ridge regression, you might want to make the first coordinate of the gradient 0 if you do not want to penalize the bias weight.  (For this dataset penalizing the bias actually helps a little). ##\n\n## Finish implementing LASSO below by implementing the gradient `g`. ##","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"9e5ee2","input":"### Regularized linear regression\n\nBelow you need to fill in the missing code to implement regularized linear regression (Ridge regression).  The formula you need is above.\n\n### $\\bar{w}_{reg} = ((X^TX+\\alpha A)^{-1})X^T\\bar{y}$ ###\n\n\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"b7a708","input":"### Unregularized linear regression\n\nBelow we fit the points using a high degree polynomial, using no regularization.\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b95403","input":"### Problem 2: Tune up\n\nFind the value of $\\alpha$ that gives the best $R^2$ performance on the test set.\n\nReport on the value of $\\alpha$ as well as the corresponding $R^2$ score.\n\nHint:  Not regularizing the bias improves outcomes significantly for this dataset.\n\nSuggestion:  Try $\\alpha=2^k$ for $k$ ranging from $-8$ to $8$.\n","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"c033af","input":"### Problem 1\n\nBelow we will implement regularized linear regression with polynomial features.\n\nRecall that the formula is \n\n$\\bar{w}_{reg} = (X^TX+\\alpha A)X^T\\bar{y}$\n\nAs usual note that many of these products are matrix products for which you need to use the numpy dot operator.  The constant $\\alpha$ is a hyperparameter that you will adjust. (In the class meeting we called this parameter $\\lambda$ but `lambda` is a reserved Python word).\n\nThe matrix $A$ can either be the $(d+1)\\times(d+1)$ identity matrix $I$, or the same matrix but with the upper left 1 converted to a 0.  (This depends on whether or not you want to apply regularization to the bias (or intercept) weight).\n\nIn numpy $I$ is \n\n```\nnp.eye(X.shape[1])\n```\n\nassuming that $X$ has a bias column.\n\nTo create $A$ you could use\n\n\n```\nA=np.eye(X.shape[1])\nA[0][0]=0\n```\n\nRead through the code below and look for places that say \"You complete me\".\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"fb1636","input":"### Best score vs last score\n\nYou can see from the code below that the weight found by gradient descent actually peaked at around the 20th iteration (from the point of view of the test set).\n\nAfter that the weights began to overfit the training set and test performance began to decrease.\n","pos":22,"type":"cell"}
{"id":0,"time":1589906078529,"type":"user"}
{"last_load":1589906081921,"type":"file"}