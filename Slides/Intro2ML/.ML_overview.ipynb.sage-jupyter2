{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83181568},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"231390","input":"","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"2b60aa","input":"import numpy as np\nimport mystuff as ms\n\n\nX=np.array([]).reshape(0,3)\ny=np.array([])\nw = np.array([1,-2,3])\nms.lin_boundary(w,X,y)","output":{"0":{"data":{"image/png":"008104a72b612aa7fc0a7f31fb3a821159b7f8e9","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":1,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"e969fd","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[ 1.        , -1.58159474, -2.90885798],\n       [ 1.        ,  0.97402525,  0.4954294 ]])"},"exec_count":10,"output_type":"execute_result"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"eb4ca6","input":"badx = X[mc][0]\nbady = y[mc][0]\nw = w+bady*badx\nms.lin_boundary(w,X,y)","output":{"0":{"data":{"image/png":"fc8e1e7da0bb8871fd063e73c7732e78b205ac8c","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":11,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"d8b0d9","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[ 1.        ,  2.61879316,  2.1669837 ],\n       [ 1.        , -0.7647271 , -1.65228359]])"},"exec_count":12,"output_type":"execute_result"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"fe562a","input":"badx = X[mc][0]\nbady = y[mc][0]\nw = w+bady*badx\nms.lin_boundary(w,X,y)","output":{"0":{"data":{"image/png":"b2f6bbe756fb44bf7695eb909dbbff332f4998a1","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":13,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":40,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"305e01","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[ 1.        , -1.58159474, -2.90885798],\n       [ 1.        ,  0.97402525,  0.4954294 ],\n       [ 1.        ,  2.82539962,  0.24628874],\n       [ 1.        , -0.7647271 , -1.65228359]])"},"exec_count":14,"output_type":"execute_result"}},"pos":41,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"2a6da9","input":"N = 10\nmu_yes = [-2,0]\nsig_yes = 2\nX_yes=np.ones(3*N).reshape(N,3)\nX_yes[:,0] = np.ones(N)\nX_yes[:,1]=np.random.randn(N)*sig_yes+mu_yes[0]\nX_yes[:,2]=np.random.randn(N)*sig_yes+mu_yes[1]\n\ny_yes=np.ones(N)\n\nmu_no = [3,0]\nsig_no = 0.7\nX_no=np.ones(3*N).reshape(N,3)\nX_no[:,0] = np.ones(N)\nX_no[:,1]=np.random.randn(N)*sig_no+mu_no[0]\nX_no[:,2]=np.random.randn(N)*sig_no+mu_no[1]\n\ny_no=np.ones(N)*(-1)\n\nX = np.vstack((X_yes,X_no))\ny = np.hstack((y_yes,y_no))\nw = np.array([1,-2,3])\nms.lin_boundary(w,X,y)\n","output":{"0":{"data":{"image/png":"f8d7199ac173b749b0c3991ec5d0c8c7740a98df","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"39221a","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[ 1.        , -1.0772671 , -2.13934413],\n       [ 1.        ,  0.67772068, -0.77271115],\n       [ 1.        ,  0.69720397, -3.65212875]])"},"exec_count":3,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"54f824","input":"np.random.seed(76)\nN = 2\nmu_yes = [-2,0]\nsig_yes = 2\nX_yes=np.ones(3*N).reshape(N,3)\nX_yes[:,0] = np.ones(N)\nX_yes[:,1]=np.random.randn(N)*sig_yes+mu_yes[0]\nX_yes[:,2]=np.random.randn(N)*sig_yes+mu_yes[1]\n\ny_yes=np.ones(N)\n\nmu_no = [3,0]\nsig_no = 0.7\nX_no=np.ones(3*N).reshape(N,3)\nX_no[:,0] = np.ones(N)\nX_no[:,1]=np.random.randn(N)*sig_no+mu_no[0]\nX_no[:,2]=np.random.randn(N)*sig_no+mu_no[1]\n\ny_no=np.ones(N)*(-1)\n\nX = np.vstack((X_yes,X_no))\ny = np.hstack((y_yes,y_no))\nw = np.array([1,-2,3])\nms.lin_boundary(w,X,y)\n","output":{"0":{"data":{"image/png":"6a4ea85d83bad42c5026f22e8c5019716f2abfa4","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"1d9091","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[ 1.        , -0.11939738, -2.11907271]])"},"exec_count":5,"output_type":"execute_result"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"fdf388","input":"badx = X[mc][0]\nbady = y[mc][0]\nw = w+bady*badx\nms.lin_boundary(w,X,y)","output":{"0":{"data":{"image/png":"3dd3f0f64eb3ef1f25bcece860823b0bcc192798","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"343f52","input":"#misclassified\nmc = np.sign(X.dot(w))!= y\nX[mc]","output":{"0":{"data":{"text/plain":"array([], shape=(0, 3), dtype=float64)"},"exec_count":8,"output_type":"execute_result"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"d845c8","input":"np.random.seed(19191)\nN = 4\nmu_yes = [-2,0]\nsig_yes = 2\nX_yes=np.ones(3*N).reshape(N,3)\nX_yes[:,0] = np.ones(N)\nX_yes[:,1]=np.random.randn(N)*sig_yes+mu_yes[0]\nX_yes[:,2]=np.random.randn(N)*sig_yes+mu_yes[1]\n\ny_yes=np.ones(N)\n\nmu_no = [3,0]\nsig_no = 1.5\nX_no=np.ones(3*N).reshape(N,3)\nX_no[:,0] = np.ones(N)\nX_no[:,1]=np.random.randn(N)*sig_no+mu_no[0]\nX_no[:,2]=np.random.randn(N)*sig_no+mu_no[1]\n\ny_no=np.ones(N)*(-1)\n\nX = np.vstack((X_yes,X_no))\ny = np.hstack((y_yes,y_no))\nw = np.array([1,-2,3])\nms.lin_boundary(w,X,y)\n","output":{"0":{"data":{"image/png":"6ab73fdb52e89d62918ea652a82b7d2f186b0a2d","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":9,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":36,"scrolled":true,"type":"cell"}
{"cell_type":"markdown","id":"068bdb","input":"### Wait... what's $\\mathcal{H}$?\n\nWhen using neural networks, $\\mathcal{H}$ might be all functions that can be represented as a neural net of a certain size.\n\nIn this class we will discuss many learning algorithms and describe their hypothesis spaces -- here we're just trying to give an abstract overview of **any** learning algorithm.","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"0853dd","input":"### Perceptrons\n\nIt would be convenient if we could represent $ax_1 + bx_2 + c$ as the dot product\n\n$$[a,b,c][1,x_1,x_2]^T=\n\\begin{align}\n    [a,b,c]  \\begin{bmatrix}\n           1 \\\\\n           x_{1} \\\\\n           x_{2}\n         \\end{bmatrix}\n\\end{align} = a+bx_1 + cx_2\n$$\n\nThe advantage to doing this is so great that we will change $\\mathcal{X}$ a little bit to make it possible.  **We will assume that the first coordinate of all $\\bar{x} \\in \\mathcal{X}$ is 1.**  \n\n$\\mathcal{X} = \\{1\\}\\times\\mathbb{R}^d$\n\nThis is called \"adding a bias column\".","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"0b1788","input":"### Supervised vs Unsupervised learning\n\nFor the work we did above our dataset $\\mathcal{D}$ included labels.  \n\nThat is, $\\mathcal{D}$ consists of pairs $(\\bar{x},y)$ where\n\n$$f(x)=y$$\n\nThis is called **supervised learning**.\n\nOften the correct labels are painstakingly provided by humans (Taskrabbit, Mechanical Turk). \n","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"1b55fa","input":"### Perceptrons\n\nThe hypothesis space of perceptrons is built of of linear functions in variables $x_1,x_2,\\ldots,x_d$. For example if $d=2$, we might have\n\n$\\mathcal{H} = \\{sign(a+ bx_1 + cx_2) : a,b,c \\in \\mathbb{R}\\}$\n\nwhere sign is +1 for non-negative inputs and -1 for negative inputs.\n","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"22f713","input":"### Classification problems\n\nLet $\\mathcal{X}$ denote the space of all images.\n\nFor instance, $\\mathcal{X}$ might be all grayscale images with 256x256 pixels.\n\nWe will assume that each element of $\\mathcal{X}$ is a vector, and denote elements of $\\mathcal{X}$ as $\\bar{x} \\in \\mathcal{X}$.\n\nLet $\\mathcal{Y}$ denote the possible classifications: $\\mathcal{Y} = \\{0,1\\}$\n\nwhere `1=cat` and `0=no cat`. \n\nLet $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ be the **ideal classifier**:  For every $\\bar{x} \\in \\mathcal{X}$, $f(x)$ is the correct classification for $\\bar{x}$.\n\nIt is useful to talk about $f$ but we don't really understand it and probably can never find it.\n\n","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"349cd1","input":"### Multiclass classification\n\nIn the cat example and the work with the perceptron we did **binary classification**.\n\nIn other words we wanted to map each instance into one of two categories.\n\nSometimes the problem might be to predict which of several discrete categories an instance $\\bar{x}$ belongs to.  This is called **multiclass classification**.\n\nFor example you might want to take data about a piece of meat and classify it as \"Grade A\", \"Grade B\", \"Grade C\" or \"Grade F (reject)\".\n\nYou might also think about automatically classifying sprains from radiological images.\n","pos":48,"type":"cell"}
{"cell_type":"markdown","id":"3a4d34","input":"### Oh yeah, the data.  How does that fit in?\n\nExactly what we do with the data depends on what kind of data we have.\n\nIn a problem like cat classification, usually we have a set of **labeled examples**.\n\nThat means we have a bunch of images, say $\\bar{x}_1,\\bar{x}_2,\\ldots,\\bar{x}_n$.\n\nCorresponding to each $\\bar{x}_i$ we also have a **label** $y_i \\in \\mathcal{Y}$ which tells us whether $\\bar{x}_i$ is an example of a cat, or a non-example of a cat.\n\nA dataset will typically contain both examples and non-examples.  They may or may not be proportionally represented.  Maybe most pictures in our dataset are **not** pictures of cats.\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"405081","input":"### Wait... what's $\\mathcal{H}$?\n\nExactly what $\\mathcal{H}$ is depends on the learning algorithm.\n\nWhen using linear classification, $\\mathcal{H}$ might be determined by linear functions.\n\nFor example if $\\mathcal{X} = \\mathbb{R}^2$ and $\\mathcal{Y} = \\{0,1\\}$, we could have\n\n$\\mathcal{H} = \\{ [\\![ ax_1 + bx_2 + c > 0]\\!] : a,b,c \\in \\mathbb{R}\\}$\n\nwhere $[\\![P]\\!]$ is 0 if $P$ is false and 1 if $P$ is true.\n\nThis is a family of functions from $\\mathcal{X} \\rightarrow \\mathcal{Y}$.\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"47a721","input":"### But what if $f$ is not in $\\mathcal{H}$?\n\nAlmost certainly $f$, the ideal classifier, is not in $\\mathcal{H}$.  \n\nAll we can hope to find is the $g \\in \\mathcal{H}$ that best approximates $f$.\n\nExactly what \"best\" means and how exactly $\\mathcal{H}$ is searched depends on the learning algorithm $\\mathcal{A}$.\n\nIdeally we want the function in $\\mathcal{H}$ closest to $f$ when *all possible* images are considered.\n\nIn practice we have to use only the data we own, and hope that our data is representative.\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"49af2c","input":"### Perceptrons\n\nWe can now express $\\mathcal{H}$ very concisely:\n\n$\\mathcal{H} = \\{ sign(\\bar{w}\\cdot \\bar{x}) : \\bar{w} \\in \\mathbb{R}^{d+1}\\}$\n\nwhere $\\displaystyle \\bar{w}\\cdot \\bar{x} = \\sum_{i=0}^d w_ix_i$ is the dot product, and $x_0=1$.","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"4ba45d","input":"## The PLA update rule\n\nThe heart of the PLA is how the weight vector is updated at each time step.\n\nThe pseudocode for PLA is:\n\n1. Initialization: $t=0$, $\\bar{w}_0 = \\bar{0}$ \n2. While a misclassified instance $(\\bar{x},y)$ exists in $\\mathcal{D}$:\n3. $\\,\\,\\,\\,\\bar{w}_{t+1} = \\bar{w}_{t}+y\\bar{x}$\n4. $\\,\\,\\,\\,t = t+1$","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"4cad4f","input":"### Basic problem types\n\nThe problem we just described is a **classification** problem:\n\nThere are two categories: cat and no-cat\n\nThe problem is to take an image and classify it.\n\nThat is, we want to assign the image to the right category.\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"5a7a3b","input":"### Unsupervised learning\n\nHowever you might have unlabeled data.  \n\nYou should still be able to extract information from this data (called **unsupervised learning**).\n\nFor example a bunch of datapoints $\\bar{x}_1,\\bar{x}_2,\\ldots,\\bar{x}_n$ gives information about the probability distribution $P(\\mathcal{X}=\\bar{x})$ that \"generates\" the data.\n\nThere might also be natural clusters in the data that you can discover.\n\nYou can try to reduce the dimensionality of the data and create various visualizations.","pos":45,"type":"cell"}
{"cell_type":"markdown","id":"65415c","input":"### Adding some data points\n\nBelow we add some data to the plot.  \n\nThere are some positive examples (blue) and negative examples (red).\n\nThe boundary we have classifies them most data correctly (but not all).\n\n\"Learning\" in the sense of classification is finding the \"best\" boundary.\n","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"668849","input":"### A motivating example: Cat recognition\n\nConsider the problem of determining whether an image contains a picture of a cat.\n\nA human being might try to give rules for recognizing a cat.\n\nBut what would they be?\n\nThey would be a complicated mess and probably not work very well.  \n\nWe do not teach babies to recognize cats by giving them rules.\n\nWe give them data:  lots of examples of cats.\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"67b8fe","input":"### Regression\n\nRegression retains the same basic setup as classification.\n\nIt is still a form of supervised learning. \n\nThere is still an ideal $f:\\mathcal{X} \\rightarrow \\mathcal{Y}$ that we want to learn.\n\nThere is still a hypothesis space $\\mathcal{H}$ of functions $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$.\n\nThe plan is still to somehow find the $g \\in \\mathcal{H}$ that best approximates $f$.\n\nOften the same algorithm can be used for classification or regression with only minor changes.  ","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"6b7825","input":"### What's learning?\n\nThere is no official definition of what it means for a machine to learn.\n\nThis is a philosophical question.\n\nBut we can roughly say:  Performance should improve with experience.\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"717979","input":"### Cat recognition cont.\n\nWe want to do the same thing with a machine that we did with the baby.\n\nNamely:  Let the machine come up with its own model of cathood by seeing lots of sample images.\n\nMachine learning gives a framework for how to do this.\n\nOften we have no idea how the machine representation of cathood works, even though the learning procedure was designed by humans.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"7f8900","input":"### The hypothesis space\n\nWe want to search for $f$ but we probably can't search the space of all functions from $\\mathcal{X}$ to $\\mathcal{Y}$, denoted ${}^\\mathcal{X}\\mathcal{Y}$.\n\nIt's too big and unstructured.  \n\nIf there are only 1000 images in $\\mathcal{X}$ then ${}^\\mathcal{X}\\mathcal{Y}$ already has cardinality $|\\mathcal{Y}|^{|\\mathcal{X}|} =  2^{1000}$.\n\nMaking matters worse, there are, in reality, a virtually unlimited set of images. \n\nRather than try to search brute force through ${}^\\mathcal{X}\\mathcal{Y}$, we propose a **hypothesis space** $\\mathcal{H} \\subseteq {}^\\mathcal{X}\\mathcal{Y}$.\n\nThis is usually a \"nice\" set of functions that can be efficiently searched.","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"92f955","input":"# Introduction to Machine Learning\n\nIn these notes we will introduce the basic definitions, notations and abstractions used in the course.\n\nFirst we give an overview of ML and then start exploring supervised learning in detail.\n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"9b41ee","input":"### Batch vs Online learning\n\nOften all data is available at the start of a ML project.  \n\nIn other words $\\mathcal{D}$ is complete and does not change.  \n\nThis is called **batch** learning.  \n\nIn another scenario $\\mathcal{D}$ might be constantly growing as additional data points are added.\n\nThis idea is called **online** learning.  \n\nIn online learning you often talk about \"mistake bounds\":  The maximum number of times the algorithm will make a wrong prediction.  \n\nIn most real ML applications the dataset gets enlarged from time to time and you have to retrain everything. ","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"b5205f","input":"### Boundaries\n\nGeometrically an $h \\in \\mathcal{H}$ divides $\\mathbb{R}^d$ into two parts:\n\npositive predictions:\n\n$$\\{\\bar{x}\\in\\mathbb{R}^d: h(1,\\bar{x})=+1\\}$$\n\nnegative predictions:\n\n$$\\{\\bar{x}\\in\\mathbb{R}^d: h(1,\\bar{x})=-1\\}$$\n\nThe separation between these two regions is called the **decision boundary**.\n","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"b52c57","input":"### Classification vs Regression\n\nIn the example we did above we used $\\mathcal{Y} = \\{-1,1\\}$.\n\nIn **classification** $\\mathcal{Y}$ is discrete and almost always finite. \n\nIn **regression** $\\mathcal{Y}$ may take on a continuous range of values. \n\nFor example $\\mathcal{X}$ may describe the vital stats of various toddlers.  Then $\\mathcal{Y}$ might be the height of the toddler at age 30.  ","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"b68b93","input":"### The Perceptron Learning Algorithm (PLA)\n\nWe have seen from the above that the decision boundary made by a perceptron is determined by the weight vector $\\bar{w}\\in \\mathbb{R}^{d+1}$.\n\nThe PLA works by finding progressively better $\\bar{w}$.\n\nThere are a series of time steps $t=0,1,\\ldots$.\n\nAt each time there is a weight vector $\\bar{w}_t$.\n\nThe initial weights $\\bar{w}_0$ can be random, or the zero vector.\n\n","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"bd3a2c","input":"### A simple example: Perceptron learning\n\nThe [perceptron](https://en.wikipedia.org/wiki/Perceptron) is an idea that dates back to 1958.  It is supposed to be somewhat like a single neuron.  Neural nets are made up of perceptrons.\n\nHowever here we will just focus on a single perceptron. \n\nIn principle you could use a perceptron for our cat classification problem.\n\nWhen we describe the perceptron we will basically be explaining $\\mathcal{X},\\mathcal{Y},\\mathcal{H}$, and $\\mathcal{A}$.\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"c40cd2","input":"[cat](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.redditmedia.com%2FjNFvI-jjCVq_7o-2LDDOBhLiT10SGz0Th48j0QGuR6c.jpg%3Fw%3D320%26s%3Df61235809979a6b5def841c38814b210&f=1&nofb=1)\n\n[cat?](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FfeB5tm4wM5U%2Fmaxresdefault.jpg&f=1&nofb=1)\n\n[cat?](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.UBtefQmk8W6OKrC5Udl1wgHaD4%26pid%3DApi&f=1)","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"c5e56d","input":"### Boundaries\n\nThe perceptron is a linear model that creates \"flat\" decision boundaries.\n\nWhen $d=2$ the decision boundary is a line in $\\mathbb{R}^2$.\n\nWhen $d=3$ the decision boundary is a plane in $\\mathbb{R}^3$.\n\nFor higher $d$ we say the boundary is a **hyperplane**.\n\nAlgebraically a hyperplane in $\\mathbb{R}^d$ is just the set of solutions to the equation\n\n$$[1,x_1,x_2,\\ldots,x_d][w_0,w_1,\\ldots,w_d]^T = 0$$\n\nGeometrically it is a subset that divides the space into two convex parts.\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"ca983b","input":"###  Multiclass classification\n\nThere are generic techniques which we will discuss later for extending binary classifiers to multiclass classifiers.\n\nSome algorithms, for example neural nets, \"naturally\" support multiclass classification. \n","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"cb79e6","input":"### The PLA update rule\n\nGeometrically the update rule moves the decision boundary toward the misclassified point. \n\nIt continues until there are no misclassified examples.\n\nFor linearly separable data it always eventually terminates. \n\n1. Initialization: $t=0$, $\\bar{w}_0 = \\bar{0}$ \n2. While a misclassified instance $(\\bar{x},y)$ exists in $\\mathcal{D}$:\n3. $\\,\\,\\,\\,\\bar{w}_{t+1} = \\bar{w}_{t}+y\\bar{x}$\n4. $\\,\\,\\,\\,t = t+1$","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"d2b372","input":"### Perceptron learning: $\\mathcal{X}$ and $\\mathcal{Y}$\n\nThe input space to a perceptron is $\\mathcal{X}=\\mathbb{R}^d$, where $\\mathbb{R}^d$ is $d$ dimensional Euclidean space.  \n\nWe could write $\\bar{x} = [x_1,x_2,\\ldots,x_d]^T$ where the $x_i$ are real numbers.\n\nThe meaning of $x_1,x_2,\\ldots,x_d$ depend on the features in the data (height, weight, shoe-size, etc.).\n\nWe use the compact notation $[x_1,x_2,\\ldots,x_d]^T$ to represent a column vector as a row.\n\n\\begin{align}\n    [x_1,x_2,\\ldots,x_d]^T &= \\begin{bmatrix}\n           x_{1} \\\\\n           x_{2} \\\\\n           \\vdots \\\\\n           x_{d}\n         \\end{bmatrix}\n\\end{align}\n  \nThere are two possible classifications, $\\mathcal{Y} = \\{+1,-1\\}$. \n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"d711b1","input":"###  Other kinds of learning\n\nIn the discussion until now we have discussed a very particular kind of learning.\n\n1. Supervised (labels are known)\n    1. Batch (because all data available at start)\n        1. Classification (because $\\mathcal{Y}$ is discrete)\n            1. Binary classification (because $|\\mathcal{Y}|=2$)\n\nBelow we give some definitions that sketch out the more general landscape of ML.\n\n","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"ecb0e8","input":"### A worked example\n\nBelow we write a little big of code to do one PLA update on some data.\n\nNotice that the update fixes the single misclassified point.\n","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"edd5c6","input":"### The data tells us a little about $f$\n\nWe can represent our dataset like this:\n\n$\\mathcal{D} = \\{(\\bar{x}_1,y_1),(\\bar{x}_1,y_1),\\ldots,(\\bar{x}_n,y_n)\\}$\n\nUsually we assume the labels are all correct.  This means that for all $i = 1,\\ldots,n$,\n\n$$f(\\bar{x}_i) = y_i$$\n\nIn other words $\\mathcal{D}$ is like a finite sample from $f$.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"fac74b","input":"### What if the data is not linearly separable?\n\nIf the data is not linearly separable then the algorithm will never terminate.\n\nThis is obvious because the set of misclassified points can by definition never be empty.\n\nAs long as a single point is misclassified, the algorithm keeps going.\n\nWe give an example of this below.\n","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"faf57c","input":"### Let's see a boundary!\n\nBelow we use matplotlib to look at some real boundaries when $d=2$.\n\nIf you want to see \"under the hood\" you can look in the file called `mystuff.py` which is in this directory.\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"fec68a","input":"### Predictions\n\nThe boundary is a way of visualizing the predictions that the algorithm $\\mathcal{A}$ will make on new instances that are not in $\\mathcal{D}$.\n\nAny $\\bar{x}$ that arises in the yellow region will be predicted as \"blue\" by $\\mathcal{A}$.\n\nAny $\\bar{x}$ that arises in the purple region will be predicted to be \"red\" by $\\mathcal{A}$. \n\nHopefully this prediction will be true as often as possible.\n","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"ff915d","input":"### An overview of learning\n\nWe can now describe how machine learning basically works.\n\nThere is something we want to learn, in this case the ideal cat classifier $f:\\mathcal{X}\\rightarrow \\mathcal{Y}$. \n\nWe have a finite set of labeled examples of the way $f$ behaves.  This is the training data $\\mathcal{D}$. \n\nWe decide on a hypothesis space $\\mathcal{H}$ which are the functions we will search for an approximation to $f$.\n\nThe learning algorithm $\\mathcal{A}$ searches $\\mathcal{H}$ and produces a final hypothesis $g$.\n\nWe hope that for randomly chosen images $\\bar{x}$, probably $g(\\bar{x}) = f(\\bar{x})$.\n\n","pos":13,"type":"cell"}
{"id":0,"time":1584631597519,"type":"user"}
{"last_load":1584631600536,"type":"file"}