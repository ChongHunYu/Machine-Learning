{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X,y = make_blobs(centers=2,random_state=11)\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)\n",
    "y = 2*y-1\n",
    "Xr = X[y==1]\n",
    "Xb = X[y==-1]\n",
    "plt.scatter(Xr[:,0],Xr[:,1])\"\"\n",
    "plt.scatter(Xb[:,0],Xb[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.c_[np.ones(X.shape[0]),X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "\n",
    "The basic idea of the SVM is to find a **maximum margin** separator. \n",
    "\n",
    "![img](margin.png)\n",
    "\n",
    "\n",
    "As with the linear model, the SVM is based on the hyperplane\n",
    "\n",
    "$$\\bar{w}^T\\bar{x}$$\n",
    "\n",
    "Because scaling affects the width of the margin, SVM methods are sensitive to data scaling. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In classification the hypothesis class is the same as linear classification:\n",
    "\n",
    "$$\\mathcal{H} = \\{sign(\\bar{w}^T\\bar{x}) : \\bar{w} \\in \\mathbb{R}^{d+1}\\}$$\n",
    "\n",
    "The difference is just the algorithm for finding the best $\\bar{w}$.\n",
    "\n",
    "---\n",
    "\n",
    "The above notation employs our usual convention that $\\bar{w} = [w_0,w_1,\\ldots,w_d]^T$ and $\\bar{x} = [x_0,x_1,\\ldots,x_d]^T$ where $x_0 = 1$ is the bias coordinate. \n",
    "\n",
    "But because of the way SVMs work it is more convenient to use the notation $b = w_0$, $\\bar{w} = [w_1,\\ldots,w_d]^T$ and $\\bar{x} = [x_1,\\ldots,x_d]^T$.\n",
    "\n",
    "Then we express the hyperplane like this:\n",
    "\n",
    "$$\\bar{w}^T\\bar{x} + b$$\n",
    "\n",
    "It says the same thing, just in different notation. \n",
    "\n",
    "To avoid confusion I will express this as \n",
    "\n",
    "$$\\bar{w}^T_{[1:]}\\bar{x}_{[1:]} + b$$\n",
    "\n",
    "with the understanding that $b=w_0$.\n",
    "\n",
    "Then we can keep our familiar notation and \n",
    "\n",
    "$$\\bar{w}^T\\bar{x} = \\bar{w}^T_{[1:]}\\bar{x}_{[1:]} + b$$\n",
    "\n",
    "\n",
    "\n",
    "#### Decision boundary\n",
    "\n",
    "As usual, the decision boundary is given by the equation\n",
    "\n",
    "$$\\bar{w}^T\\bar{x} = 0.$$\n",
    "\n",
    "The margin lines are (by definition) given by the equations \n",
    "\n",
    "$$\\bar{w}^T\\bar{x} = 1$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\bar{w}^T\\bar{x} = -1.$$\n",
    "\n",
    "To fully understand this, remember that the decision boundary is really the intersection of the **plane** $z = \\bar{w}^T\\bar{x}$ and the $xy$ plane.\n",
    "\n",
    "![img](2d.png)\n",
    "\n",
    "![img](3d2.png)\n",
    "\n",
    "In the above plot, the decision boundary is where the mesh plane intersects the $xy$ or $z=0$ plane.  \n",
    "\n",
    "The upper margin is where the mesh plane intersects the $z=1$ plane (projected down to $xy$).\n",
    "\n",
    "The lower margin is where the mesh plane intersects the $z=-1$ plane (projected down to $xy$).\n",
    "\n",
    "---\n",
    "\n",
    "#### Slope and margin width\n",
    "\n",
    "Notice that the locations of the margin depend on the slope of the mesh plane.\n",
    "\n",
    "The plane *must* go through the decision boundary, but it could do so with varying degrees of steepness.\n",
    "\n",
    "The steeper the angle, the narrower the margin will be.\n",
    "\n",
    "Observe that $\\nabla_{\\bar{x}_{[1:]}} \\bar{w}^T\\bar{x} = \\bar{w}_{[1:]}$.\n",
    "\n",
    "Therefore the direction of increase for the mesh plane is $\\bar{w}_{[1:]}$.\n",
    "\n",
    "And the steepness is controlled by $|\\!|\\bar{w}_{[1:]}|\\!|$.\n",
    "\n",
    "The smaller $|\\!|\\bar{w}_{[1:]}|\\!|$ is, more gradual the slope, the wider the margin.\n",
    "\n",
    "When $|\\!|\\bar{w}_{[1:]}|\\!|$ is large, the opposite is true -- the margins are narrow and the slope is steep.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Hard margin classification\n",
    "\n",
    "\n",
    "In hard margin classification we assume that the data is linearly separable.\n",
    "\n",
    "Then we try to find the boundary that admits the widest margin.\n",
    "\n",
    "We can phrase everything as an optimization problem.\n",
    "\n",
    "The goal is to maximize the margin width, while ensuring that the data is correctly classified by the \"thick\" decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "### The optimization problem\n",
    "\n",
    "Recall that maximizing margin width means minimizing $|\\!|\\bar{w}_{[1:]}|\\!|$.\n",
    "\n",
    "Equivalently we can minimize that more convenient expression $\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2$.\n",
    "\n",
    "And we want to do this subject to these constraints:\n",
    "\n",
    "$\\bar{w}^T\\bar{x}_n \\geq 1$ whenever $y_n = 1$\n",
    "\n",
    "and\n",
    "\n",
    "$\\bar{w}^T\\bar{x}_n \\leq -1$ whenever $y_n = -1$\n",
    "\n",
    "More succinctly, we want to\n",
    "\n",
    "**minimize**  $\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2$\n",
    "\n",
    "**subject to** $y_n(\\bar{w}^T\\bar{x}_n) \\geq 1$ for all $n=1,2,\\ldots,N$.\n",
    "\n",
    "There is one constraint for each training data point. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Solving the  optimization problem\n",
    "\n",
    "Before when we wanted to do optimization we used gradient descent or the normal equations (for linear regression).\n",
    "\n",
    "But here we will do something different.\n",
    "\n",
    "The optimization problem defined by an SVM is an example of a **quadratic programming** (QP) problem.\n",
    "\n",
    "Quadratic programming problems are very well understood and there are a number of QP solving libraries that can be used to train a SVM.\n",
    "\n",
    "---\n",
    "\n",
    "A discussion of QP is beyond the scope of our class. \n",
    "\n",
    "But the basic idea is to find the optimum of a linearly constrained set subject to a quadratic function.\n",
    "\n",
    "![img](quadratic-programming.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "There are a number of QP solvers available in python:  CVXOPT, CVXPY, Gurobi, MOSEK, qpOASES and quadprog.\n",
    "\n",
    "We will use quadprog.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the QP problem\n",
    "\n",
    "The following formally defines a QP problem.\n",
    "\n",
    "**Minimize**     $\\frac{1}{2} \\bar{w}^T G \\bar{w} - \\bar{a}^T \\bar{w}$\n",
    "\n",
    "**Subject to**   $A\\bar{w} \\geq \\bar{v}$\n",
    "\n",
    "The first line gives a general quadratic function of $\\bar{w}$.\n",
    "\n",
    "The second line gives a **system** of inequalities (one for each dimension of $\\bar{v}$).\n",
    "\n",
    "Here:\n",
    "\n",
    "1. $G$ is a $d\\times d$ matrix.\n",
    "2. $\\bar{a}$ is a $d$ dimensional vector.\n",
    "3. $A$ is a $k \\times d$ matrix.\n",
    "4. $\\bar{v}$ is a $k$ dimensional vector.\n",
    "\n",
    "I have expressed the variables to optimize as $\\bar{w}$ to give a big hint as to how this will work for hard margin classification.\n",
    "\n",
    "But be mindful that the variables could have other meanings (as they will in soft-margin classification).\n",
    "\n",
    "The QP problem and QP solvers do not care about any particular meaning of the elements as it relates to the SVM problem.\n",
    "\n",
    "---\n",
    "\n",
    "### Connecting QP and SVM\n",
    "\n",
    "Notice that the thing we want to minimize in the SVM problem is\n",
    "\n",
    "$\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2 = \\frac{1}{2}\\bar{w}_{[1:]}^T\\bar{w}_{[1:]}$\n",
    "\n",
    "We can realize this as $\\frac{1}{2} \\bar{w}^T G \\bar{w} - \\bar{a}^T \\bar{w}$ by \n",
    "\n",
    "1. making $G = I$, except $G_{0,0}$, which will be 0,\n",
    "2. and by making $\\bar{a} = \\bar{0}$.\n",
    "\n",
    "The constraints in the SVM problem are $y_n(\\bar{w}^T\\bar{x}_n) \\geq 1$ for all $n=1,2,\\ldots,N$.\n",
    "\n",
    "We can realize this as $A\\bar{w} \\geq \\bar{v}$ in the following way.\n",
    "\n",
    "First, $A$ will have $k=N$ rows.\n",
    "\n",
    "The $i$th row of $A$ will be $y_i\\bar{x}_i$.\n",
    "\n",
    "The vector $\\bar{v}$ can simply be a vector of $N$ ones.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To simplify the transition from the equations to the code, we will formally define the QP problem using the notation used in the quadprog documentation.\n",
    "\n",
    "You can see the docs by loading \n",
    "\n",
    "```\n",
    "import quadprog as qp\n",
    "qp.solve_qp?\n",
    "```\n",
    "\n",
    "Which produces\n",
    "\n",
    "```\n",
    "Docstring:\n",
    "Solve a strictly convex quadratic program\n",
    "\n",
    "Minimize     1/2 x^T G x - a^T x\n",
    "Subject to   C.T x >= b\n",
    "\n",
    "This routine uses the the Goldfarb/Idnani dual algorithm [1].\n",
    "\n",
    "References\n",
    "---------\n",
    "... [1] D. Goldfarb and A. Idnani (1983). A numerically stable dual\n",
    "    method for solving strictly convex quadratic programs.\n",
    "    Mathematical Programming, 27, 1-33.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "G : array, shape=(n, n)\n",
    "    matrix appearing in the quadratic function to be minimized\n",
    "a : array, shape=(n,)\n",
    "    vector appearing in the quadratic function to be minimized\n",
    "C : array, shape=(n, m)\n",
    "    matrix defining the constraints under which we want to minimize the\n",
    "    quadratic function\n",
    "b : array, shape=(m), default=None\n",
    "    vector defining the constraints\n",
    "meq : int, default=0\n",
    "    the first meq constraints are treated as equality constraints,\n",
    "    all further as inequality constraints (defaults to 0).\n",
    "factorized : bool, default=False\n",
    "    If True, then we are passing :math:`R^{âˆ’1}` (where :math:`G = R^T R`)\n",
    "    instead of the matrix G in the argument G.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "x : array, shape=(n,)\n",
    "    vector containing the solution of the quadratic programming problem.\n",
    "f : float\n",
    "    the value of the quadratic function at the solution.\n",
    "xu : array, shape=(n,)\n",
    "    vector containing the unconstrained minimizer of the quadratic function\n",
    "iterations : tuple\n",
    "    2-tuple. the first component contains the number of iterations the\n",
    "    algorithm needed, the second indicates how often constraints became\n",
    "    inactive after becoming active first.\n",
    "lagrangian : array, shape=(m,)\n",
    "    vector with the Lagragian at the solution.\n",
    "iact : array\n",
    "    vector with the indices of the active constraints at the solution.\n",
    "Type:      builtin_function_or_method\n",
    "```\n",
    "\n",
    "Notice that `quadprog` uses `x` for our $\\bar{w}$ and `b` for our $\\bar{v}$. \n",
    "\n",
    "We will not use the optional parameters `meq` or `factorized`.\n",
    "\n",
    "For reasons that are unclear `quadprog` prefers to think of our $A$ as being the transpose of another matrix called $C$.  \n",
    "\n",
    "We generally only care about the 0th entry of the vector returned by `quadprog`, which is our $\\bar{w}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Below, use quadprog to find the maximum margin $\\bar{w}$ for the blob data loaded above.\n",
    "\n",
    "The solver will not like it if a row or column of $G$ is all zeros.\n",
    "\n",
    "After you set up $G$ add a \"sprinkling\" of small constants on the diagonal:\n",
    "\n",
    "```\n",
    "G = G + np.eye(X.shape[1])*10**(-9)\n",
    "```\n",
    "\n",
    "This ensures that $G$ is *positive-definite*, meaning $\\bar{w}^TG\\bar{w} > 0$.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import quadprog as qp\n",
    "## You complete me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74c7db5447c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "left = np.min(X[:,1])\n",
    "right = np.max(X[:,1])\n",
    "xx = np.linspace(left,right)\n",
    "yy = -(w[0]+w[1]*xx)/w[2]\n",
    "plt.plot(xx,yy,c='r',label=\"max margin boundary\")\n",
    "yy_um = -(w[0]+w[1]*xx-1)/w[2]\n",
    "plt.plot(xx,yy_um,'--',label=\"margin\")\n",
    "yy_um = -(w[0]+w[1]*xx+1)/w[2]\n",
    "plt.plot(xx,yy_um,'--',c='b',label=\"margin\")\n",
    "\n",
    "plt.scatter(Xr[:,0],Xr[:,1])\n",
    "plt.scatter(Xb[:,0],Xb[:,1])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Support vectors\n",
    "\n",
    "Given that this learning algorithm is called a Support Vector Machine, we should mention support vectors.\n",
    "\n",
    "These are just the training points that are exactly on the margins.\n",
    "\n",
    "In other words they are the training points $(\\bar{x}_n,y_n)$ such that\n",
    "\n",
    "$$y_n\\bar{w}^T\\bar{x} = \\pm 1$$\n",
    "\n",
    "Below we find the support vectors for the above problem.\n",
    "\n",
    "Given their coordinates, you can find them in the plot above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## The support vectors... \n",
    "\n",
    "[(w.dot(x),(x[1],x[2])) for x in X if w.dot(x) == 1 or w.dot(x)==-1  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A 3D view\n",
    "\n",
    "Here is a 3-dimensional view of the same decision boundary.\n",
    "\n",
    "Try to use it to understand why $\\bar{w}^T\\bar{x} = \\pm 1$ gives different margins depending on $\\bar{w}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2, 2, 30)\n",
    "y = np.linspace(-2, 2, 30)\n",
    "\n",
    "XX, YY = np.meshgrid(x, y)\n",
    "ZZ = w[0]+w[1]*XX+w[2]*YY\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "#ax2 = fig.add_subplot(212, projection='3d')\n",
    "# Grab some test data.\n",
    "#X, Y, Z = axes3d.get_test_data(0.05)\n",
    "\n",
    "# Plot a basic wireframe.\n",
    "ax.plot_wireframe(XX, YY, ZZ, rstride=10, cstride=10,alpha=0.9)\n",
    "\n",
    "ax.plot_surface(XX, YY, 0*XX, rstride=10, cstride=10,alpha=0.2)\n",
    "\n",
    "\n",
    "ax.scatter(Xr[:,0], Xr[:,1], np.zeros(Xr.shape[0]),c='b')\n",
    "ax.scatter(Xb[:,0], Xb[:,1], np.zeros(Xr.shape[0]),c='r')\n",
    "\n",
    "\"\"\"\n",
    "ax2.plot_wireframe(XX, YY, 3*ZZ, rstride=10, cstride=10,alpha=0.9)\n",
    "\n",
    "ax2.plot_surface(XX, YY, 0*XX, rstride=10, cstride=10,alpha=0.2)\n",
    "\n",
    "\n",
    "ax2.scatter(Xr[:,0], Xr[:,1], np.zeros(Xr.shape[0]),c='r')\n",
    "ax2.scatter(Xb[:,0], Xb[:,1], np.zeros(Xr.shape[0]),c='b')\n",
    "\"\"\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Soft margin\n",
    "\n",
    "You can use the hard margin method for linearly separable data, but most real data is not perfectly linearly separable. \n",
    "\n",
    "Additionally the hard margin approach is sensitive to outliers.  \n",
    "\n",
    "An outlying data point can significantly change the best margin boundary.\n",
    "\n",
    "---\n",
    "\n",
    "To relax the hard margin assumption of linear separability, we have to allow **margin violations**.\n",
    "\n",
    "This means we have to allow points inside the margin, and even allow points on the wrong side of the decision boundary.  \n",
    "\n",
    "While doing this, we still want to stay true to the \"wide margin\" principle.  \n",
    "\n",
    "The key idea that makes this possible is a **slack variable**.\n",
    "\n",
    "We usually denote slack variables as $\\zeta$, the Greek letter zeta.\n",
    "\n",
    "---\n",
    "\n",
    "### Slack variables\n",
    "\n",
    "For every training point $(\\bar{x}_n, y_n)$ we will associate a slack variable $\\zeta_n$.  \n",
    "\n",
    "The slack variable measures how far $\\bar{x}_n$ is from being on the right side of the margin.\n",
    "\n",
    "If $\\bar{x}_n$ is actually on the correct side of the margin then $\\zeta_n = 0$.\n",
    "\n",
    "In hard margin we require $y_n\\bar{w}^T\\bar{x} \\geq 1$.\n",
    "\n",
    "With slack variables we only require $y_n\\bar{w}^T\\bar{x} \\geq 1-\\zeta_n$,\n",
    "\n",
    "where $\\zeta_n \\geq 0$.\n",
    "\n",
    "![img](slack.jpeg)\n",
    "\n",
    "Notice that slack variables are always nonnegative.\n",
    "\n",
    "---\n",
    "\n",
    "### The soft margin optimization problem\n",
    "\n",
    "In soft margin classification, we want to maximize the width of the margin, but we also want to minimize the amount of slack we need.  \n",
    "\n",
    "Mathematically we want to\n",
    "\n",
    "**minimize** $$\\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]} + C\\sum_{i=1}^N \\zeta_i$$\n",
    "\n",
    "**subject to**\n",
    "\n",
    "$$y_i\\bar{w}^T\\bar{x}_i \\geq 1-\\zeta_i$$ and $$\\zeta_i \\geq 0$$ for $i=1,2\\ldots,N$.\n",
    "\n",
    "\n",
    "The minimization is with respect to the variables $\\bar{w},\\zeta_1,\\zeta_2,\\ldots,\\zeta_N$.\n",
    "\n",
    "To be clear:  **we now want to minimize an expression that involves $d+1+N$ variables**.\n",
    "\n",
    "---\n",
    "\n",
    "The constant $C$ is a nonzero hyperparameter that lets us balance two competing wants:\n",
    "\n",
    "1. the widest possible margins\n",
    "2. the most benign possible margin violations\n",
    "\n",
    "If $C$ is large then margin violations will be heavily penalized, causing the margin to be smaller.\n",
    "\n",
    "If $C$ is small then margin violations are only lightly penalized, and the margin is allowed to be wider.\n",
    "\n",
    "We give some plots below showing the effect of shrinking $C$.\n",
    "\n",
    "![img](sm1.png)\n",
    "\n",
    "![img](sm2.png)\n",
    "\n",
    "\n",
    "### The soft margin QP problem\n",
    "\n",
    "The soft margin constraint is still a quadratic programming problem.\n",
    "\n",
    "**Minimize**     $\\frac{1}{2} \\bar{z}^T G \\bar{z} - \\bar{a}^T \\bar{z}$\n",
    "\n",
    "**Subject to**   $A\\bar{z} \\geq \\bar{v}$\n",
    "\n",
    "\n",
    "Here I have made the variables $\\bar{z}$ minimized by the QP solver into the deliberately abstract $\\bar{z}$.\n",
    "\n",
    "Now we will use $\\bar{z} = [w_0,w_1,w_2,\\ldots,w_d,\\zeta_1,\\zeta_2,\\ldots,\\zeta_N]$.\n",
    "\n",
    "We need to choose $G$ so that $\\bar{z}^T G \\bar{z} = \\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]}$.\n",
    "\n",
    "We need to chose $\\bar{a}$ so that $\\bar{a}^T\\bar{z} = -C\\sum_{i=1}^N\\zeta_i$.\n",
    "\n",
    "Finally we need to choose $A$ and $\\bar{v}$ so that $A\\bar{z} \\geq \\bar{v}$ expresses both $$y_i\\bar{w}^T\\bar{x}_i \\geq 1-\\zeta_i$$ as well as $\\zeta_i \\geq 0$ for all $i=1,2,\\ldots,N$.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "Implement a soft margin SVM in the function `svm-fit` below.  \n",
    "\n",
    "Hint:  Try to write $G$, $A$, and $\\bar{v}$ out in \"block matrix\" form to see what to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "X,y = make_blobs(centers=2,cluster_std = 4,random_state=11)\n",
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)\n",
    "y = 2*y-1\n",
    "Xr = X[y==1]\n",
    "Xb = X[y==-1]\n",
    "plt.scatter(Xr[:,0],Xr[:,1])\n",
    "plt.scatter(Xb[:,0],Xb[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eb06034be02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.c_[np.ones(X.shape[0]),X]\n",
    "X.shape\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## Soft margin\n",
    "import quadprog as qp\n",
    "\n",
    "def svm_fit(X,y,C_svm=1):\n",
    "    ## You complete me\n",
    "\n",
    "w = svm_fit(X,y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "for C in [10**(k) for k in [-4,-3,-2,-1,0,1,2]]:\n",
    "    w = svm_fit(X,y,C)\n",
    "\n",
    "    plt.figure(figsize=(8,10))\n",
    "    left = np.min(X[:,1])\n",
    "    right = np.max(X[:,1])\n",
    "    xx = np.linspace(left,right)\n",
    "    yy = -(w[0]+w[1]*xx)/w[2]\n",
    "    plt.plot(xx,yy,c='r',label=\"soft margin boundary\")\n",
    "    yy_um = -(w[0]+w[1]*xx-1)/w[2]\n",
    "    plt.plot(xx,yy_um,'--',label=\"margin\")\n",
    "    yy_um = -(w[0]+w[1]*xx+1)/w[2]\n",
    "    plt.plot(xx,yy_um,'--',c='b',label=\"margin\")\n",
    "\n",
    "    plt.scatter(Xr[:,0],Xr[:,1])\n",
    "    plt.scatter(Xb[:,0],Xb[:,1])\n",
    "    plt.title(\"C = {}\".format(C))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial features\n",
    "\n",
    "Just as with linear regression, it may help the SVM to add polynomial features.\n",
    "\n",
    "This allows a higher degree decision boundary at the cost of extra variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(7)\n",
    "X = pf.fit_transform(X[:,[1,2]])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "w = svm_fit(X,y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot_boundary(X,y,w,pf,title=None):\n",
    "    xmin = np.min(X[:,1])-0.5\n",
    "    xmax = np.max(X[:,1])+0.5\n",
    "    ymin = np.min(X[:,2])-0.5\n",
    "    ymax = np.max(X[:,2])+0.5\n",
    "    x1 = np.arange(xmin,xmax,0.01)\n",
    "    x2 = np.arange(ymin,ymax,0.01)\n",
    "\n",
    "    x1v,x2v = np.meshgrid(x1,x2)\n",
    "\n",
    "    varz = np.concatenate((x1v.reshape(-1,1),x2v.reshape(-1,1)),axis=1)\n",
    "    varzp = pf.fit_transform(varz)\n",
    "\n",
    "    z = (varzp.dot(w)).reshape(x1v.shape)\n",
    "\n",
    "    #zsoft = sigmoid(z)\n",
    "    z = np.sign(z)\n",
    "    plt.contourf(x1v,x2v,z,alpha=0.25)\n",
    "    Xg = X[y==1]\n",
    "    Xb = X[y==-1]\n",
    "\n",
    "    plt.scatter(Xg[:,1],Xg[:,2],c=\"b\",alpha=0.5)\n",
    "    plt.scatter(Xb[:,1],Xb[:,2],c='r',alpha=0.5)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.colorbar()\n",
    "    return plt\n",
    "\n",
    "plt = plot_boundary(X,y,w,pf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Regularization\n",
    "\n",
    "The hyperparameter $C$ can be adjusted to different values.  \n",
    "\n",
    "By increasing $C$ we prioritize points being classified correctly.\n",
    "\n",
    "This makes the boundary more \"curvy\" and increases variance.\n",
    "\n",
    "By lowering $C$ we tolerate misclassifications.\n",
    "\n",
    "This makes the boundary more stiff and decreases variance.\n",
    "\n",
    "\n",
    "If $C$ is **too** small there might be strange behavior,\n",
    "\n",
    "because we're then trying so solve an impossible problem on data that isn't linearly separable.\n",
    "\n",
    "\n",
    "In the plots below the model starts to overfit somewhere around $C=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n",
    "    print(\"C = {}\".format(C))\n",
    "    w = svm_fit(X,y,C)\n",
    "    plt = plot_boundary(X,y,w,pf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A real dataset\n",
    "\n",
    "We now try out our SVM classifier on a real dataset.\n",
    "\n",
    "A link to the data source on UCI is provided below.\n",
    "\n",
    "On this dataset our SVM outperforms our logistic regression model.\n",
    "\n",
    "In fact it seems to outperform the library SVM from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set\n",
    "\n",
    "df = pd.read_csv(\"divorce.csv\",delimiter=\";\")\n",
    "df.head()\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]*2-1\n",
    "\n",
    "pf = PolynomialFeatures(2)\n",
    "X = pf.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, random_state =42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(\"SVM performance (linear kernel, polynomial features)\")\n",
    "from sklearn.metrics import accuracy_score as score\n",
    "for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n",
    "    w = svm_fit(X_train,y_train,C)\n",
    "    yhat = np.sign(X_test.dot(w))\n",
    "    print(\"C = {}, score = {}\".format(C, score(y_test,yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(\"Linear Regression (for classification) same features\")\n",
    "for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n",
    "    A = np.eye(X.shape[1])\n",
    "    A[0][0]=1\n",
    "    w = np.linalg.inv(X_train.T.dot(X_train)+C*A).dot(X_train.T).dot(y_train)\n",
    "    yhat = np.sign(X_test.dot(w))\n",
    "    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n",
    "    model = SVC(C=C,kernel=\"linear\")\n",
    "    model.fit(X_train,y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n",
    "    model = SVC(C=C,kernel=\"rbf\")\n",
    "    model.fit(X_train,y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Support vector regression\n",
    "\n",
    "Much of the setup we did for SVM classification can be adapted to solve a regression problem.\n",
    "\n",
    "Rather than an \"empty\" margin between two classes, we now want a \"full\" margin around the regression line.\n",
    "\n",
    "The radius of this margin is a new hyperparameter called $\\epsilon$.\n",
    "\n",
    "We want $\\epsilon$ to be small, which may lead to some datapoints being outside the \"$\\epsilon$ tunnel\".\n",
    "\n",
    "Again we assign to each training variable a certain amount of slack (which we will then minimize using QP).\n",
    "\n",
    "Unfortunately absolute value is a nonlinear function, so we cannot express the criteria we really want in QP:\n",
    "\n",
    "$$|\\bar{w}^T\\bar{x}_n-y_n| < \\epsilon+\\zeta_n$$\n",
    "\n",
    "However we can just convert this into two linear inequalities:\n",
    "\n",
    "$$\\bar{w}^T\\bar{x}_n-y_n < \\epsilon + \\zeta^*_n$$\n",
    "\n",
    "$$y_n - \\bar{w}^T\\bar{x}_n < \\epsilon + \\zeta_n$$\n",
    "\n",
    "Now each datapoint has *two* slack variables. \n",
    "\n",
    "One is for being too far above the regression line (hyperplane).\n",
    "\n",
    "The other is for beeing too far below the regression line (hyperplane).\n",
    "\n",
    "The following picture shows graphically what we are trying to do:\n",
    "\n",
    "![img](corridor.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "In addition to requiring the slack variables to be positive, this gives us our linear constraints.\n",
    "\n",
    "We still want to minimize $|\\!|\\bar{w}_{[1:]}|\\!|$, because this now controls the \"flatness\" of the regression line (hyperplane).  \n",
    "\n",
    "We want to find the flattest line (hyperplane) fitting the data, subject to the constraints, with the fewest margin violations.\n",
    "\n",
    "This *is* a QP problem, and so we can express the support vector regression problem as\n",
    "\n",
    "\n",
    "**minimize** $$\\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]} + C\\sum_{i=1}^N (\\zeta_i+\\zeta^*_i)$$\n",
    "\n",
    "**subject to**\n",
    "\n",
    "$$\\bar{w}^T\\bar{x}_n-y_n < \\epsilon + \\zeta^*_n$$\n",
    "\n",
    "$$y_n - \\bar{w}^T\\bar{x}_n < \\epsilon + \\zeta_n$$\n",
    "\n",
    "$$\\zeta_n \\geq 0$$\n",
    "\n",
    "$$\\zeta^*_n \\geq 0$$\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Code this up using `quadprog` inside the `svr_fit` function below.\n",
    "\n",
    "Again, it will be helpful to sketch out what you need to do in block matrix form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_no_bias,y = make_regression(n_features=1,noise=10,random_state=100)\n",
    "\n",
    "plt.scatter(X_no_bias,y,alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = np.c_[np.ones(X_no_bias.shape[0]),X_no_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import quadprog as qp\n",
    "\n",
    "def svr_fit(X,y,epsilon=0.1,C_svr=1):\n",
    "    ## You complete me\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "w= svr_fit(X,y,epsilon=0.5,C_svr=100)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "y_hat = X.dot(w)\n",
    "plt.scatter(X[:,1],y)\n",
    "plt.plot(X[:,1],y_hat,c='r')\n",
    "plt.title(\"The regression line learned by the SVR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial features\n",
    "\n",
    "We now explore using our SVR with some toy data.\n",
    "\n",
    "We make the regression curve \"curvy\" using a polynomial transformation of the data.\n",
    "\n",
    "Then we explore the effect that $\\epsilon$ and $C$ have on bias and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "n_samples=20\n",
    "noise=0.5\n",
    "\n",
    "### There was too much variability in the sklearn version of this function\n",
    "### so I implemented a version\n",
    "def make_regression(n_features,noise,n_samples,random_state):\n",
    "    np.random.seed(random_state)\n",
    "    dom = np.linspace(0,3,3*n_samples)\n",
    "    dom = np.random.choice(dom,n_samples).reshape(n_samples,1)\n",
    "    nz = np.random.randn(n_samples,1)*noise\n",
    "    y = 2+1.7*dom+nz\n",
    "    return dom,y\n",
    "\n",
    "\n",
    "#from sklearn.datasets import make_regression\n",
    "noise=0.5\n",
    "Xnb,y = make_regression(n_features=1,noise=noise,n_samples=n_samples,random_state=42)\n",
    "y= y.ravel()\n",
    "Xnb = np.sqrt(Xnb-np.min(Xnb))\n",
    "plt.scatter(Xnb,y)\n",
    "plt.title(\"The data to be fit\")\n",
    "data = np.c_[Xnb,y]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "Xp = poly.fit_transform(Xnb)\n",
    "Xp[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "Xp.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "w= svr_fit(Xp,y,epsilon=0.5,C_svr=100)\n",
    "\n",
    "xx = np.linspace(np.min(Xnb),np.max(Xnb),100)\n",
    "XX = poly.fit_transform(xx.reshape(-1,1))\n",
    "pred = XX.dot(w)\n",
    "plt.scatter(Xnb[:,0],y)\n",
    "plt.plot(xx,pred,c='r')\n",
    "plt.title(\"The regression line learned by the SVR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A chance to overfit\n",
    "\n",
    "We now make the number of data points small compared to the complexity of the model.\n",
    "\n",
    "This will allow us to examine how the SVR deals with overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "noise=0.5\n",
    "n_samples=5\n",
    "Xnb,y = make_regression(n_features=1,noise=noise,n_samples=n_samples,random_state=42)\n",
    "y= y.ravel()\n",
    "Xnb = np.sqrt(Xnb-np.min(Xnb))\n",
    "plt.scatter(Xnb,y)\n",
    "plt.title(\"The data to be fit\")\n",
    "data = np.c_[Xnb,y]\n",
    "plt.show()\n",
    "\n",
    "poly = PolynomialFeatures(7)\n",
    "Xp = poly.fit_transform(Xnb)\n",
    "\n",
    "w= svr_fit(Xp,y,epsilon=0.5,C_svr=100)\n",
    "\n",
    "xx = np.linspace(np.min(Xnb),np.max(Xnb),100)\n",
    "XX = poly.fit_transform(xx.reshape(-1,1))\n",
    "pred = XX.dot(w)\n",
    "plt.scatter(Xnb[:,0],y)\n",
    "plt.plot(xx,pred,c='r')\n",
    "plt.title(\"The regression line learned by the SVR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Changing $\\epsilon$ and $C$\n",
    "\n",
    "In the plots below we examine the conseqences of changing $\\epsilon$ and $C$.\n",
    "\n",
    "Reducing $\\epsilon$ creates a narrower tube.\n",
    "\n",
    "To maintain the same level of slack the curve must become more curvy.\n",
    "\n",
    "This increases variance.  \n",
    "\n",
    "---\n",
    "\n",
    "Increasing $C$ makes the model hate margin violations more.\n",
    "\n",
    "With the same size tube the curve will become curvier to reduce slack.\n",
    "\n",
    "This increases variance.\n",
    "\n",
    "---\n",
    "\n",
    "If $\\epsilon$ is reduced and/or $C$ is increased, variance will increase.\n",
    "\n",
    "This means that $C$ will have lower training error but possibly overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "for epsilon in [0.01,0.1,.5,1]:\n",
    "    for C_svr in [0.001,0.1,1,100,1000]:\n",
    "        w= svr_fit(Xp,y,epsilon=epsilon,C_svr=C_svr)\n",
    "\n",
    "        xx = np.linspace(np.min(Xnb),np.max(Xnb),100)\n",
    "        XX = poly.fit_transform(xx.reshape(-1,1))\n",
    "        pred = XX.dot(w)\n",
    "        plt.scatter(Xnb[:,0],y)\n",
    "        plt.plot(xx,pred,c='r')\n",
    "        plt.plot(xx,pred+epsilon,'--',c='r')\n",
    "        plt.plot(xx,pred-epsilon,'--',c='r')        \n",
    "        plt.title(\"SVR, epsilon={},C={}\".format(epsilon,C_svr))\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}