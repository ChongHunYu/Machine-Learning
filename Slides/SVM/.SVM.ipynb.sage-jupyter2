{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83369984},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"0bee1a","input":"## Soft margin\nimport quadprog as qp\n\ndef svm_fit(X,y,C_svm=1):\n    ## You complete me\n\nw = svm_fit(X,y)\nw","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1f6de1","input":"\nfor epsilon in [0.01,0.1,.5,1]:\n    for C_svr in [0.001,0.1,1,100,1000]:\n        w= svr_fit(Xp,y,epsilon=epsilon,C_svr=C_svr)\n\n        xx = np.linspace(np.min(Xnb),np.max(Xnb),100)\n        XX = poly.fit_transform(xx.reshape(-1,1))\n        pred = XX.dot(w)\n        plt.scatter(Xnb[:,0],y)\n        plt.plot(xx,pred,c='r')\n        plt.plot(xx,pred+epsilon,'--',c='r')\n        plt.plot(xx,pred-epsilon,'--',c='r')        \n        plt.title(\"SVR, epsilon={},C={}\".format(epsilon,C_svr))\n        \n        plt.show()","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2bbc45","input":"from sklearn.svm import SVC\nfor C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n    model = SVC(C=C,kernel=\"rbf\")\n    model.fit(X_train,y_train)\n    yhat = model.predict(X_test)\n    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))    \n    ","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4bed89","input":"X = np.c_[np.ones(X_no_bias.shape[0]),X_no_bias]","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"507097","input":"from sklearn.model_selection import train_test_split\n\n#https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set\n\ndf = pd.read_csv(\"divorce.csv\",delimiter=\";\")\ndf.head()\nX = df.values[:,:-1]\ny = df.values[:,-1]*2-1\n\npf = PolynomialFeatures(2)\nX = pf.fit_transform(X)\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state =42)\n\n","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"55d0ee","input":"y_hat = X.dot(w)\nplt.scatter(X[:,1],y)\nplt.plot(X[:,1],y_hat,c='r')\nplt.title(\"The regression line learned by the SVR\")\nplt.show()","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6b68e0","input":"for C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n    print(\"C = {}\".format(C))\n    w = svm_fit(X,y,C)\n    plt = plot_boundary(X,y,w,pf)\n    plt.show()","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7076f3","input":"\nfrom mpl_toolkits.mplot3d import axes3d\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2, 2, 30)\ny = np.linspace(-2, 2, 30)\n\nXX, YY = np.meshgrid(x, y)\nZZ = w[0]+w[1]*XX+w[2]*YY\n\n\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot(111, projection='3d')\n#ax2 = fig.add_subplot(212, projection='3d')\n# Grab some test data.\n#X, Y, Z = axes3d.get_test_data(0.05)\n\n# Plot a basic wireframe.\nax.plot_wireframe(XX, YY, ZZ, rstride=10, cstride=10,alpha=0.9)\n\nax.plot_surface(XX, YY, 0*XX, rstride=10, cstride=10,alpha=0.2)\n\n\nax.scatter(Xr[:,0], Xr[:,1], np.zeros(Xr.shape[0]),c='b')\nax.scatter(Xb[:,0], Xb[:,1], np.zeros(Xr.shape[0]),c='r')\n\n\"\"\"\nax2.plot_wireframe(XX, YY, 3*ZZ, rstride=10, cstride=10,alpha=0.9)\n\nax2.plot_surface(XX, YY, 0*XX, rstride=10, cstride=10,alpha=0.2)\n\n\nax2.scatter(Xr[:,0], Xr[:,1], np.zeros(Xr.shape[0]),c='r')\nax2.scatter(Xb[:,0], Xb[:,1], np.zeros(Xr.shape[0]),c='b')\n\"\"\"\n\nplt.show()","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"80985f","input":"import quadprog as qp\n\ndef svr_fit(X,y,epsilon=0.1,C_svr=1):\n    ## You complete me\n    \n    return w","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"896f86","input":"left = np.min(X[:,1])\nright = np.max(X[:,1])\nxx = np.linspace(left,right)\nyy = -(w[0]+w[1]*xx)/w[2]\nplt.plot(xx,yy,c='r',label=\"max margin boundary\")\nyy_um = -(w[0]+w[1]*xx-1)/w[2]\nplt.plot(xx,yy_um,'--',label=\"margin\")\nyy_um = -(w[0]+w[1]*xx+1)/w[2]\nplt.plot(xx,yy_um,'--',c='b',label=\"margin\")\n\nplt.scatter(Xr[:,0],Xr[:,1])\nplt.scatter(Xb[:,0],Xb[:,1])\nplt.legend()\nplt.show()\n","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"89a5e7","input":"noise=0.5\nn_samples=5\nXnb,y = make_regression(n_features=1,noise=noise,n_samples=n_samples,random_state=42)\ny= y.ravel()\nXnb = np.sqrt(Xnb-np.min(Xnb))\nplt.scatter(Xnb,y)\nplt.title(\"The data to be fit\")\ndata = np.c_[Xnb,y]\nplt.show()\n\npoly = PolynomialFeatures(7)\nXp = poly.fit_transform(Xnb)\n\nw= svr_fit(Xp,y,epsilon=0.5,C_svr=100)\n\nxx = np.linspace(np.min(Xnb),np.max(Xnb),100)\nXX = poly.fit_transform(xx.reshape(-1,1))\npred = XX.dot(w)\nplt.scatter(Xnb[:,0],y)\nplt.plot(xx,pred,c='r')\nplt.title(\"The regression line learned by the SVR\")\nplt.show()","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8b3091","input":"\nX,y = make_blobs(centers=2,cluster_std = 4,random_state=11)\nss = StandardScaler()\nX = ss.fit_transform(X)\ny = 2*y-1\nXr = X[y==1]\nXb = X[y==-1]\nplt.scatter(Xr[:,0],Xr[:,1])\nplt.scatter(Xb[:,0],Xb[:,1])\nplt.show()\n","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8dd170","input":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(2)\nXp = poly.fit_transform(Xnb)\nXp[:2]","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8df172","input":"## The support vectors... \n\n[(w.dot(x),(x[1],x[2])) for x in X if w.dot(x) == 1 or w.dot(x)==-1  ]","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"911e3b","input":"from sklearn.datasets import make_regression\n\nX_no_bias,y = make_regression(n_features=1,noise=10,random_state=100)\n\nplt.scatter(X_no_bias,y,alpha=0.6)\nplt.show()","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9bb667","input":"for C in [10**(k) for k in [-4,-3,-2,-1,0,1,2]]:\n    w = svm_fit(X,y,C)\n\n    plt.figure(figsize=(8,10))\n    left = np.min(X[:,1])\n    right = np.max(X[:,1])\n    xx = np.linspace(left,right)\n    yy = -(w[0]+w[1]*xx)/w[2]\n    plt.plot(xx,yy,c='r',label=\"soft margin boundary\")\n    yy_um = -(w[0]+w[1]*xx-1)/w[2]\n    plt.plot(xx,yy_um,'--',label=\"margin\")\n    yy_um = -(w[0]+w[1]*xx+1)/w[2]\n    plt.plot(xx,yy_um,'--',c='b',label=\"margin\")\n\n    plt.scatter(Xr[:,0],Xr[:,1])\n    plt.scatter(Xb[:,0],Xb[:,1])\n    plt.title(\"C = {}\".format(C))\n    plt.legend()\n    plt.show()","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"bb2f34","input":"from sklearn.preprocessing import PolynomialFeatures\n\npf = PolynomialFeatures(7)\nX = pf.fit_transform(X[:,[1,2]])\n\nX.shape","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c217bc","input":"print(\"Linear Regression (for classification) same features\")\nfor C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n    A = np.eye(X.shape[1])\n    A[0][0]=1\n    w = np.linalg.inv(X_train.T.dot(X_train)+C*A).dot(X_train.T).dot(y_train)\n    yhat = np.sign(X_test.dot(w))\n    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c25880","input":"def plot_boundary(X,y,w,pf,title=None):\n    xmin = np.min(X[:,1])-0.5\n    xmax = np.max(X[:,1])+0.5\n    ymin = np.min(X[:,2])-0.5\n    ymax = np.max(X[:,2])+0.5\n    x1 = np.arange(xmin,xmax,0.01)\n    x2 = np.arange(ymin,ymax,0.01)\n\n    x1v,x2v = np.meshgrid(x1,x2)\n\n    varz = np.concatenate((x1v.reshape(-1,1),x2v.reshape(-1,1)),axis=1)\n    varzp = pf.fit_transform(varz)\n\n    z = (varzp.dot(w)).reshape(x1v.shape)\n\n    #zsoft = sigmoid(z)\n    z = np.sign(z)\n    plt.contourf(x1v,x2v,z,alpha=0.25)\n    Xg = X[y==1]\n    Xb = X[y==-1]\n\n    plt.scatter(Xg[:,1],Xg[:,2],c=\"b\",alpha=0.5)\n    plt.scatter(Xb[:,1],Xb[:,2],c='r',alpha=0.5)\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    if title:\n        plt.title(title)\n    plt.colorbar()\n    return plt\n\nplt = plot_boundary(X,y,w,pf)\nplt.show()","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d6b9c6","input":"w= svr_fit(Xp,y,epsilon=0.5,C_svr=100)\n\nxx = np.linspace(np.min(Xnb),np.max(Xnb),100)\nXX = poly.fit_transform(xx.reshape(-1,1))\npred = XX.dot(w)\nplt.scatter(Xnb[:,0],y)\nplt.plot(xx,pred,c='r')\nplt.title(\"The regression line learned by the SVR\")\nplt.show()","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d90603","input":"w = svm_fit(X,y)\nw","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"db104d","input":"w= svr_fit(X,y,epsilon=0.5,C_svr=100)\nw","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"de2cf9","input":"from sklearn.svm import SVC\nfor C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n    model = SVC(C=C,kernel=\"linear\")\n    model.fit(X_train,y_train)\n    yhat = model.predict(X_test)\n    print(\"lambda = {}, score = {}\".format(C, score(y_test,yhat)))    \n    ","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f00fcd","input":"Xp.shape,y.shape","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f2223e","input":"\nn_samples=20\nnoise=0.5\n\n### There was too much variability in the sklearn version of this function\n### so I implemented a version\ndef make_regression(n_features,noise,n_samples,random_state):\n    np.random.seed(random_state)\n    dom = np.linspace(0,3,3*n_samples)\n    dom = np.random.choice(dom,n_samples).reshape(n_samples,1)\n    nz = np.random.randn(n_samples,1)*noise\n    y = 2+1.7*dom+nz\n    return dom,y\n\n\n#from sklearn.datasets import make_regression\nnoise=0.5\nXnb,y = make_regression(n_features=1,noise=noise,n_samples=n_samples,random_state=42)\ny= y.ravel()\nXnb = np.sqrt(Xnb-np.min(Xnb))\nplt.scatter(Xnb,y)\nplt.title(\"The data to be fit\")\ndata = np.c_[Xnb,y]\nplt.show()","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f4563b","input":"","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fac11a","input":"print(\"SVM performance (linear kernel, polynomial features)\")\nfrom sklearn.metrics import accuracy_score as score\nfor C in [0.00001,0.0001,0.001,0.01,0.1,1,2,4,8,20,50,100]:\n    w = svm_fit(X_train,y_train,C)\n    yhat = np.sign(X_test.dot(w))\n    print(\"C = {}, score = {}\".format(C, score(y_test,yhat)))","pos":24,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"a51621","input":"from sklearn.datasets import make_blobs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\n\nX,y = make_blobs(centers=2,random_state=11)\nss = StandardScaler()\nX = ss.fit_transform(X)\ny = 2*y-1\nXr = X[y==1]\nXb = X[y==-1]\nplt.scatter(Xr[:,0],Xr[:,1])\"\"\nplt.scatter(Xb[:,0],Xb[:,1])\nplt.show()\n","output":{"0":{"data":{"text/plain":"<Figure size 640x480 with 1 Axes>"},"exec_count":1,"output_type":"execute_result"}},"pos":0,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"edb5b3","input":"X = np.c_[np.ones(X.shape[0]),X]\nX.shape\nA.shape","output":{"0":{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3eb06034be02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"80ca02","input":"X = np.c_[np.ones(X.shape[0]),X]","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"944cd8","input":"import quadprog as qp\n## You complete me\n","pos":4,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"309868","input":"w","output":{"0":{"ename":"NameError","evalue":"name 'w' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-74c7db5447c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"]}},"pos":5,"type":"cell"}
{"cell_type":"markdown","id":"0d40fd","input":"### Solving the  optimization problem\n\nBefore when we wanted to do optimization we used gradient descent or the normal equations (for linear regression).\n\nBut here we will do something different.\n\nThe optimization problem defined by an SVM is an example of a **quadratic programming** (QP) problem.\n\nQuadratic programming problems are very well understood and there are a number of QP solving libraries that can be used to train a SVM.\n\n---\n\nA discussion of QP is beyond the scope of our class. \n\nBut the basic idea is to find the optimum of a linearly constrained set subject to a quadratic function.\n\n![img](quadratic-programming.jpg)\n\n---\n\nThere are a number of QP solvers available in python:  CVXOPT, CVXPY, Gurobi, MOSEK, qpOASES and quadprog.\n\nWe will use quadprog.\n\n---\n\n### Defining the QP problem\n\nThe following formally defines a QP problem.\n\n**Minimize**     $\\frac{1}{2} \\bar{w}^T G \\bar{w} - \\bar{a}^T \\bar{w}$\n\n**Subject to**   $A\\bar{w} \\geq \\bar{v}$\n\nThe first line gives a general quadratic function of $\\bar{w}$.\n\nThe second line gives a **system** of inequalities (one for each dimension of $\\bar{v}$).\n\nHere:\n\n1. $G$ is a $d\\times d$ matrix.\n2. $\\bar{a}$ is a $d$ dimensional vector.\n3. $A$ is a $k \\times d$ matrix.\n4. $\\bar{v}$ is a $k$ dimensional vector.\n\nI have expressed the variables to optimize as $\\bar{w}$ to give a big hint as to how this will work for hard margin classification.\n\nBut be mindful that the variables could have other meanings (as they will in soft-margin classification).\n\nThe QP problem and QP solvers do not care about any particular meaning of the elements as it relates to the SVM problem.\n\n---\n\n### Connecting QP and SVM\n\nNotice that the thing we want to minimize in the SVM problem is\n\n$\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2 = \\frac{1}{2}\\bar{w}_{[1:]}^T\\bar{w}_{[1:]}$\n\nWe can realize this as $\\frac{1}{2} \\bar{w}^T G \\bar{w} - \\bar{a}^T \\bar{w}$ by \n\n1. making $G = I$, except $G_{0,0}$, which will be 0,\n2. and by making $\\bar{a} = \\bar{0}$.\n\nThe constraints in the SVM problem are $y_n(\\bar{w}^T\\bar{x}_n) \\geq 1$ for all $n=1,2,\\ldots,N$.\n\nWe can realize this as $A\\bar{w} \\geq \\bar{v}$ in the following way.\n\nFirst, $A$ will have $k=N$ rows.\n\nThe $i$th row of $A$ will be $y_i\\bar{x}_i$.\n\nThe vector $\\bar{v}$ can simply be a vector of $N$ ones.\n\n\n\n---\n\n\nTo simplify the transition from the equations to the code, we will formally define the QP problem using the notation used in the quadprog documentation.\n\nYou can see the docs by loading \n\n```\nimport quadprog as qp\nqp.solve_qp?\n```\n\nWhich produces\n\n```\nDocstring:\nSolve a strictly convex quadratic program\n\nMinimize     1/2 x^T G x - a^T x\nSubject to   C.T x >= b\n\nThis routine uses the the Goldfarb/Idnani dual algorithm [1].\n\nReferences\n---------\n... [1] D. Goldfarb and A. Idnani (1983). A numerically stable dual\n    method for solving strictly convex quadratic programs.\n    Mathematical Programming, 27, 1-33.\n\nParameters\n----------\nG : array, shape=(n, n)\n    matrix appearing in the quadratic function to be minimized\na : array, shape=(n,)\n    vector appearing in the quadratic function to be minimized\nC : array, shape=(n, m)\n    matrix defining the constraints under which we want to minimize the\n    quadratic function\nb : array, shape=(m), default=None\n    vector defining the constraints\nmeq : int, default=0\n    the first meq constraints are treated as equality constraints,\n    all further as inequality constraints (defaults to 0).\nfactorized : bool, default=False\n    If True, then we are passing :math:`R^{−1}` (where :math:`G = R^T R`)\n    instead of the matrix G in the argument G.\n\nReturns\n-------\nx : array, shape=(n,)\n    vector containing the solution of the quadratic programming problem.\nf : float\n    the value of the quadratic function at the solution.\nxu : array, shape=(n,)\n    vector containing the unconstrained minimizer of the quadratic function\niterations : tuple\n    2-tuple. the first component contains the number of iterations the\n    algorithm needed, the second indicates how often constraints became\n    inactive after becoming active first.\nlagrangian : array, shape=(m,)\n    vector with the Lagragian at the solution.\niact : array\n    vector with the indices of the active constraints at the solution.\nType:      builtin_function_or_method\n```\n\nNotice that `quadprog` uses `x` for our $\\bar{w}$ and `b` for our $\\bar{v}$. \n\nWe will not use the optional parameters `meq` or `factorized`.\n\nFor reasons that are unclear `quadprog` prefers to think of our $A$ as being the transpose of another matrix called $C$.  \n\nWe generally only care about the 0th entry of the vector returned by `quadprog`, which is our $\\bar{w}$.\n\n---\n\n### Exercise\n\nBelow, use quadprog to find the maximum margin $\\bar{w}$ for the blob data loaded above.\n\nThe solver will not like it if a row or column of $G$ is all zeros.\n\nAfter you set up $G$ add a \"sprinkling\" of small constants on the diagonal:\n\n```\nG = G + np.eye(X.shape[1])*10**(-9)\n```\n\nThis ensures that $G$ is *positive-definite*, meaning $\\bar{w}^TG\\bar{w} > 0$.\n.","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"209bf4","input":"### Soft margin\n\nYou can use the hard margin method for linearly separable data, but most real data is not perfectly linearly separable. \n\nAdditionally the hard margin approach is sensitive to outliers.  \n\nAn outlying data point can significantly change the best margin boundary.\n\n---\n\nTo relax the hard margin assumption of linear separability, we have to allow **margin violations**.\n\nThis means we have to allow points inside the margin, and even allow points on the wrong side of the decision boundary.  \n\nWhile doing this, we still want to stay true to the \"wide margin\" principle.  \n\nThe key idea that makes this possible is a **slack variable**.\n\nWe usually denote slack variables as $\\zeta$, the Greek letter zeta.\n\n---\n\n### Slack variables\n\nFor every training point $(\\bar{x}_n, y_n)$ we will associate a slack variable $\\zeta_n$.  \n\nThe slack variable measures how far $\\bar{x}_n$ is from being on the right side of the margin.\n\nIf $\\bar{x}_n$ is actually on the correct side of the margin then $\\zeta_n = 0$.\n\nIn hard margin we require $y_n\\bar{w}^T\\bar{x} \\geq 1$.\n\nWith slack variables we only require $y_n\\bar{w}^T\\bar{x} \\geq 1-\\zeta_n$,\n\nwhere $\\zeta_n \\geq 0$.\n\n![img](slack.jpeg)\n\nNotice that slack variables are always nonnegative.\n\n---\n\n### The soft margin optimization problem\n\nIn soft margin classification, we want to maximize the width of the margin, but we also want to minimize the amount of slack we need.  \n\nMathematically we want to\n\n**minimize** $$\\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]} + C\\sum_{i=1}^N \\zeta_i$$\n\n**subject to**\n\n$$y_i\\bar{w}^T\\bar{x}_i \\geq 1-\\zeta_i$$ and $$\\zeta_i \\geq 0$$ for $i=1,2\\ldots,N$.\n\n\nThe minimization is with respect to the variables $\\bar{w},\\zeta_1,\\zeta_2,\\ldots,\\zeta_N$.\n\nTo be clear:  **we now want to minimize an expression that involves $d+1+N$ variables**.\n\n---\n\nThe constant $C$ is a nonzero hyperparameter that lets us balance two competing wants:\n\n1. the widest possible margins\n2. the most benign possible margin violations\n\nIf $C$ is large then margin violations will be heavily penalized, causing the margin to be smaller.\n\nIf $C$ is small then margin violations are only lightly penalized, and the margin is allowed to be wider.\n\nWe give some plots below showing the effect of shrinking $C$.\n\n![img](sm1.png)\n\n![img](sm2.png)\n\n\n### The soft margin QP problem\n\nThe soft margin constraint is still a quadratic programming problem.\n\n**Minimize**     $\\frac{1}{2} \\bar{z}^T G \\bar{z} - \\bar{a}^T \\bar{z}$\n\n**Subject to**   $A\\bar{z} \\geq \\bar{v}$\n\n\nHere I have made the variables $\\bar{z}$ minimized by the QP solver into the deliberately abstract $\\bar{z}$.\n\nNow we will use $\\bar{z} = [w_0,w_1,w_2,\\ldots,w_d,\\zeta_1,\\zeta_2,\\ldots,\\zeta_N]$.\n\nWe need to choose $G$ so that $\\bar{z}^T G \\bar{z} = \\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]}$.\n\nWe need to chose $\\bar{a}$ so that $\\bar{a}^T\\bar{z} = -C\\sum_{i=1}^N\\zeta_i$.\n\nFinally we need to choose $A$ and $\\bar{v}$ so that $A\\bar{z} \\geq \\bar{v}$ expresses both $$y_i\\bar{w}^T\\bar{x}_i \\geq 1-\\zeta_i$$ as well as $\\zeta_i \\geq 0$ for all $i=1,2,\\ldots,N$.\n\n### Exercise:\n\nImplement a soft margin SVM in the function `svm-fit` below.  \n\nHint:  Try to write $G$, $A$, and $\\bar{v}$ out in \"block matrix\" form to see what to do.\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"2e93ea","input":"### A chance to overfit\n\nWe now make the number of data points small compared to the complexity of the model.\n\nThis will allow us to examine how the SVR deals with overfitting.\n","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"5eb4c0","input":"## Support Vector Machines\n\n\nThe basic idea of the SVM is to find a **maximum margin** separator. \n\n![img](margin.png)\n\n\nAs with the linear model, the SVM is based on the hyperplane\n\n$$\\bar{w}^T\\bar{x}$$\n\nBecause scaling affects the width of the margin, SVM methods are sensitive to data scaling. \n\n---\n\n\nIn classification the hypothesis class is the same as linear classification:\n\n$$\\mathcal{H} = \\{sign(\\bar{w}^T\\bar{x}) : \\bar{w} \\in \\mathbb{R}^{d+1}\\}$$\n\nThe difference is just the algorithm for finding the best $\\bar{w}$.\n\n---\n\nThe above notation employs our usual convention that $\\bar{w} = [w_0,w_1,\\ldots,w_d]^T$ and $\\bar{x} = [x_0,x_1,\\ldots,x_d]^T$ where $x_0 = 1$ is the bias coordinate. \n\nBut because of the way SVMs work it is more convenient to use the notation $b = w_0$, $\\bar{w} = [w_1,\\ldots,w_d]^T$ and $\\bar{x} = [x_1,\\ldots,x_d]^T$.\n\nThen we express the hyperplane like this:\n\n$$\\bar{w}^T\\bar{x} + b$$\n\nIt says the same thing, just in different notation. \n\nTo avoid confusion I will express this as \n\n$$\\bar{w}^T_{[1:]}\\bar{x}_{[1:]} + b$$\n\nwith the understanding that $b=w_0$.\n\nThen we can keep our familiar notation and \n\n$$\\bar{w}^T\\bar{x} = \\bar{w}^T_{[1:]}\\bar{x}_{[1:]} + b$$\n\n\n\n#### Decision boundary\n\nAs usual, the decision boundary is given by the equation\n\n$$\\bar{w}^T\\bar{x} = 0.$$\n\nThe margin lines are (by definition) given by the equations \n\n$$\\bar{w}^T\\bar{x} = 1$$\n\nand\n\n$$\\bar{w}^T\\bar{x} = -1.$$\n\nTo fully understand this, remember that the decision boundary is really the intersection of the **plane** $z = \\bar{w}^T\\bar{x}$ and the $xy$ plane.\n\n![img](2d.png)\n\n![img](3d2.png)\n\nIn the above plot, the decision boundary is where the mesh plane intersects the $xy$ or $z=0$ plane.  \n\nThe upper margin is where the mesh plane intersects the $z=1$ plane (projected down to $xy$).\n\nThe lower margin is where the mesh plane intersects the $z=-1$ plane (projected down to $xy$).\n\n---\n\n#### Slope and margin width\n\nNotice that the locations of the margin depend on the slope of the mesh plane.\n\nThe plane *must* go through the decision boundary, but it could do so with varying degrees of steepness.\n\nThe steeper the angle, the narrower the margin will be.\n\nObserve that $\\nabla_{\\bar{x}_{[1:]}} \\bar{w}^T\\bar{x} = \\bar{w}_{[1:]}$.\n\nTherefore the direction of increase for the mesh plane is $\\bar{w}_{[1:]}$.\n\nAnd the steepness is controlled by $|\\!|\\bar{w}_{[1:]}|\\!|$.\n\nThe smaller $|\\!|\\bar{w}_{[1:]}|\\!|$ is, more gradual the slope, the wider the margin.\n\nWhen $|\\!|\\bar{w}_{[1:]}|\\!|$ is large, the opposite is true -- the margins are narrow and the slope is steep.\n\n---\n\n\n### Hard margin classification\n\n\nIn hard margin classification we assume that the data is linearly separable.\n\nThen we try to find the boundary that admits the widest margin.\n\nWe can phrase everything as an optimization problem.\n\nThe goal is to maximize the margin width, while ensuring that the data is correctly classified by the \"thick\" decision boundary.\n\n---\n\n### The optimization problem\n\nRecall that maximizing margin width means minimizing $|\\!|\\bar{w}_{[1:]}|\\!|$.\n\nEquivalently we can minimize that more convenient expression $\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2$.\n\nAnd we want to do this subject to these constraints:\n\n$\\bar{w}^T\\bar{x}_n \\geq 1$ whenever $y_n = 1$\n\nand\n\n$\\bar{w}^T\\bar{x}_n \\leq -1$ whenever $y_n = -1$\n\nMore succinctly, we want to\n\n**minimize**  $\\frac{1}{2}|\\!|\\bar{w}_{[1:]}|\\!|^2$\n\n**subject to** $y_n(\\bar{w}^T\\bar{x}_n) \\geq 1$ for all $n=1,2,\\ldots,N$.\n\nThere is one constraint for each training data point. \n\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"6d21b6","input":"### Changing $\\epsilon$ and $C$\n\nIn the plots below we examine the conseqences of changing $\\epsilon$ and $C$.\n\nReducing $\\epsilon$ creates a narrower tube.\n\nTo maintain the same level of slack the curve must become more curvy.\n\nThis increases variance.  \n\n---\n\nIncreasing $C$ makes the model hate margin violations more.\n\nWith the same size tube the curve will become curvier to reduce slack.\n\nThis increases variance.\n\n---\n\nIf $\\epsilon$ is reduced and/or $C$ is increased, variance will increase.\n\nThis means that $C$ will have lower training error but possibly overfit.\n","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"715908","input":"### Polynomial features\n\nJust as with linear regression, it may help the SVM to add polynomial features.\n\nThis allows a higher degree decision boundary at the cost of extra variance.\n\n","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"7db92d","input":"### Support vector regression\n\nMuch of the setup we did for SVM classification can be adapted to solve a regression problem.\n\nRather than an \"empty\" margin between two classes, we now want a \"full\" margin around the regression line.\n\nThe radius of this margin is a new hyperparameter called $\\epsilon$.\n\nWe want $\\epsilon$ to be small, which may lead to some datapoints being outside the \"$\\epsilon$ tunnel\".\n\nAgain we assign to each training variable a certain amount of slack (which we will then minimize using QP).\n\nUnfortunately absolute value is a nonlinear function, so we cannot express the criteria we really want in QP:\n\n$$|\\bar{w}^T\\bar{x}_n-y_n| < \\epsilon+\\zeta_n$$\n\nHowever we can just convert this into two linear inequalities:\n\n$$\\bar{w}^T\\bar{x}_n-y_n < \\epsilon + \\zeta^*_n$$\n\n$$y_n - \\bar{w}^T\\bar{x}_n < \\epsilon + \\zeta_n$$\n\nNow each datapoint has *two* slack variables. \n\nOne is for being too far above the regression line (hyperplane).\n\nThe other is for beeing too far below the regression line (hyperplane).\n\nThe following picture shows graphically what we are trying to do:\n\n![img](corridor.jpeg)\n\n\n\nIn addition to requiring the slack variables to be positive, this gives us our linear constraints.\n\nWe still want to minimize $|\\!|\\bar{w}_{[1:]}|\\!|$, because this now controls the \"flatness\" of the regression line (hyperplane).  \n\nWe want to find the flattest line (hyperplane) fitting the data, subject to the constraints, with the fewest margin violations.\n\nThis *is* a QP problem, and so we can express the support vector regression problem as\n\n\n**minimize** $$\\frac{1}{2}\\bar{w}^T_{[1:]}\\bar{w}_{[1:]} + C\\sum_{i=1}^N (\\zeta_i+\\zeta^*_i)$$\n\n**subject to**\n\n$$\\bar{w}^T\\bar{x}_n-y_n < \\epsilon + \\zeta^*_n$$\n\n$$y_n - \\bar{w}^T\\bar{x}_n < \\epsilon + \\zeta_n$$\n\n$$\\zeta_n \\geq 0$$\n\n$$\\zeta^*_n \\geq 0$$\n\n\n### Exercise\n\nCode this up using `quadprog` inside the `svr_fit` function below.\n\nAgain, it will be helpful to sketch out what you need to do in block matrix form.\n\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"8d649d","input":"### A 3D view\n\nHere is a 3-dimensional view of the same decision boundary.\n\nTry to use it to understand why $\\bar{w}^T\\bar{x} = \\pm 1$ gives different margins depending on $\\bar{w}$.\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"a29f7c","input":"### Polynomial features\n\nWe now explore using our SVR with some toy data.\n\nWe make the regression curve \"curvy\" using a polynomial transformation of the data.\n\nThen we explore the effect that $\\epsilon$ and $C$ have on bias and variance.\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"ca9193","input":"### A real dataset\n\nWe now try out our SVM classifier on a real dataset.\n\nA link to the data source on UCI is provided below.\n\nOn this dataset our SVM outperforms our logistic regression model.\n\nIn fact it seems to outperform the library SVM from sklearn.\n","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"de05df","input":"### Regularization\n\nThe hyperparameter $C$ can be adjusted to different values.  \n\nBy increasing $C$ we prioritize points being classified correctly.\n\nThis makes the boundary more \"curvy\" and increases variance.\n\nBy lowering $C$ we tolerate misclassifications.\n\nThis makes the boundary more stiff and decreases variance.\n\n\nIf $C$ is **too** small there might be strange behavior,\n\nbecause we're then trying so solve an impossible problem on data that isn't linearly separable.\n\n\nIn the plots below the model starts to overfit somewhere around $C=1$.","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"fa9123","input":"### Support vectors\n\nGiven that this learning algorithm is called a Support Vector Machine, we should mention support vectors.\n\nThese are just the training points that are exactly on the margins.\n\nIn other words they are the training points $(\\bar{x}_n,y_n)$ such that\n\n$$y_n\\bar{w}^T\\bar{x} = \\pm 1$$\n\nBelow we find the support vectors for the above problem.\n\nGiven their coordinates, you can find them in the plot above. \n","pos":7,"type":"cell"}
{"id":0,"time":1589216824104,"type":"user"}
{"last_load":1589216826339,"type":"file"}