{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83398656},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"1491e0","input":"","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1d5f54","input":"import numpy as np\nimport matplotlib.pyplot as plt\n\nxx = np.linspace(-4,4)\nyy = np.linspace(-4,4)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = XX**4 + YY**4-16*XX*YY\n\nplt.contourf(XX,YY,Z)\nplt.colorbar()\nplt.title(r\"$f(x,y) = x^4+y^4-16xy$\")\nplt.show()","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2dbabd","input":"\nxx = np.linspace(-4,4)\nyy = np.linspace(-4,4)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = XX**2 + YY**2\nplt.contourf(XX,YY,Z)\nplt.colorbar()\n\nplt.scatter(path[:15,0],path[:15,1],c='r')\nplt.title(\"Everything just right\")\nplt.show()\n","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"38e866","input":"def f(w):\n    return w[1]**4+w[0]**4-16*w[0]*w[1]\nblue_err_diffs = np.diff(list(map(f,path4)))","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"394647","input":"plt.plot(np.arange(len(blue_err_diffs)),blue_err_diffs)\nplt.title(\"Error differences over time\")\nplt.ylabel(r\"$E_{in}(w_{t+1})-E_{in}(w_{t})$\")\nplt.xlabel(\"Time step\")\nplt.show()","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"435a7e","input":"## Last 10\nblue_gradients[-10:]","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"52d7d6","input":"## Last 10\nblue_err_diffs[-10:]","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5d5ca4","input":"for p in path[:15]:\n    print(p)","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"722b69","input":"\nw = np.array([3.5,-3])\nw,path=grad_descent(w,X,y,grad_paraboloid,eta=0.01)\n\nxx = np.linspace(-4,4)\nyy = np.linspace(-4,4)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = XX**2 + YY**2\nplt.contourf(XX,YY,Z)\nplt.colorbar()\n\nplt.scatter(path[:15,0],path[:15,1],c='r')\nplt.title(\"Learning rate too small\")\nplt.show()\n","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"73d382","input":"def grad_nonconv(w,X,y):\n    return np.array([4*w[0]**3-16*w[1],4*w[1]**3-16*w[0]])","pos":24,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7fd9ce","input":"def grad_ec_paraboloid(w,X,y):\n    return w*np.array([24,2])\n\n\nw = np.array([20.5,-28])\nprint(w)\nw,path=grad_descent(w,X,y,grad_ec_paraboloid,eta=0.1)\n\n","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"865c1e","input":"xx = np.linspace(-20,20)\nyy = np.linspace(-30,30)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = 12*XX**2 + YY**2\n\nplt.contourf(XX,YY,Z)\nplt.colorbar()\nplt.scatter(path[:3,0],path[:3,1],c='r')\nplt.title(\"Learning rate too big and data needs scaling\")\nplt.show()\n","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"90ffa7","input":"xx = np.linspace(-20,20)\nyy = np.linspace(-30,30)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = 12*XX**2 + YY**2\n\nplt.contourf(XX,YY,Z)\nplt.colorbar()\nplt.scatter(path[:,0],path[:,1],c='r',alpha=0.1)\nplt.title(\"Learning rate is fine but convergence is slow\")\nplt.show()","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"961ea2","input":"\nw = np.array([20.5,-28])\nprint(w)\nw,path=grad_descent(w,X,y,grad_ec_paraboloid,eta=0.01)\npath[:15]\n","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"978290","input":"#blue_gradients[:300]\nplt.plot(np.arange(len(blue_gradients)),blue_gradients)\nplt.title(\"Magnitude of gradient over time steps\")\nplt.ylabel(\"Gradient magnitude\")\nplt.xlabel(\"Time step\")\nplt.show()","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9ffcd4","input":"path[:15]","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e9163a","input":"L = [grad_nonconv(ww,X,y) for ww in path4]\nblue_gradients = np.linalg.norm(L,axis=1)","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f209c0","input":"w = np.array([3.5,-3])\nw,path=grad_descent(w,X,y,grad_nonconv,eta=0.001)\n\nw = np.array([-2,3])\nw,path2=grad_descent(w,X,y,grad_nonconv,eta=0.001)\n\nw = np.array([0,0])\nw,path3=grad_descent(w,X,y,grad_nonconv,eta=0.001)\n\nw = np.array([-3,0])\nw,path4=grad_descent(w,X,y,grad_nonconv,eta=0.001)\n\n\nxx = np.linspace(-4,4)\nyy = np.linspace(-4,4)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = XX**4 + YY**4-16*XX*YY\nplt.contourf(XX,YY,Z)\nplt.colorbar()\n\nsteps=30\n\nplt.scatter(path[:steps,0],path[:steps,1],c='r')\nplt.scatter(path2[:steps,0],path2[:steps,1],c='orange')\nplt.scatter(path3[:steps,0],path3[:steps,1],c='yellow')\nplt.scatter(path4[:steps,0],path4[:steps,1],c='blue')\nplt.title(\"Part way\")\nplt.show()\n\n\nplt.contourf(XX,YY,Z)\nplt.colorbar()\n\nsteps=300\n\nplt.scatter(path[:steps,0],path[:steps,1],c='r')\nplt.scatter(path2[:steps,0],path2[:steps,1],c='orange')\nplt.scatter(path3[:steps,0],path3[:steps,1],c='yellow')\nplt.scatter(path4[:steps,0],path4[:steps,1],c='blue')\nplt.title(\"All the way\")\nplt.show()\n\n","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"c48b1d","input":"import numpy as np\nimport matplotlib.pyplot as plt\n\nxx = np.linspace(-4,4)\nyy = np.linspace(-4,4)\n\nXX,YY = np.meshgrid(xx,yy)\n\nZ = XX**2 + YY**2\n\nplt.contourf(XX,YY,Z)\nplt.colorbar()\nplt.title(r\"$f(x,y) = x^2+y^2$\")\nplt.show()\n","output":{"0":{"data":{"image/png":"96a1572d662064b1fb70c5b2989189b44194de27","text/plain":"<Figure size 864x504 with 2 Axes>"},"exec_count":1,"metadata":{"image/png":{"height":429,"width":644},"needs_background":"light"},"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"24428b","input":"from mystuff import grad_descent\n\nX = np.array([])  ## In these toy examples X and y don't matter\ny = np.array([])  ## but in real error functions they do\n\n\ndef grad_paraboloid(w,X,y):\n    return 2*w\n\n\n\n\nimport mystuff as ms\n\n\nw = np.array([3.5,-3])\nw,path = grad_descent(w,X,y,grad_paraboloid)\n","output":{"0":{"ename":"ImportError","evalue":"bad magic number in 'mystuff': b'B\\r\\r\\n'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-017bfddb2064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmystuff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad_descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## In these toy examples X and y don't matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## but in real error functions they do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: bad magic number in 'mystuff': b'B\\r\\r\\n'"]}},"pos":4,"type":"cell"}
{"cell_type":"markdown","id":"060b6b","input":"### A harder case\n\nOur version of gradient descent worked great for a very simple function.\n\nLet's give it a \"worse\" $f$ to optimize. \n\nThis will be a paraboloid which is squeezed flat along the $x$-axis:\n\n$$f(w_0,w_1) = 12w_0^2 + w_1^2$$\n\nThis is what the error function might look like if your data is in need of scaling -- one dimension is dominating the others.\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"206661","input":"### The gradient\n\nBelow we compute \n\n$$\\nabla(x^4+y^4-16xy) = [4x^3-16y,4y^3-16x]^T$$.","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"22fce5","input":"### Hmmm\n\nThis time when we look at `path` the numbers don't seem to be converging.  This is what thrashing around looks like.\n","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"2fee57","input":"### Stopping conditions\n\nIn the above examples we just ran gradient descent for 1000 iterations and hoped for the best.\n\nNo fixed amount of iterations is appropriate for every problem.\n\n#### Small gradient\n\nAnother approach would be to stop when the magnitude of the gradient drops below a given amount $\\epsilon$.\n\n#### Error still decreasing\n\nStill another strategy would be to keep running the process as long as the error keeps decreasing.\n\n\n#### Hybrid strategy\n\nIn practice a hybrid combination of these strategies works well.\n\nWe run for at most some large number of iterations, but stop early if error stops decreasing or the gradient magnitude gets sufficiently small.\n\n\n### Example\n\nBelow we look at what happens to the magnitudes of the gradients along the blue path in the above plot.\n\nNote that they are initially large, but they decrease toward 0.","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"31fedb","input":"### Another visual\n\nWe try to visualize the movement of the \"ball\" as we did last time.\n\nHowever in this case we can only see 3 time steps before the ball \"flies offscreen\".\n\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"45c592","input":"### Learning rate too large!\n\nThe above situation was bad for two reasons.\n\nOne is that the learning rate is too big for this problem.  \n\nThe very steep gradient is overwhelming $\\eta=0.1$ and making the ball take huge leaps.\n\nThe leaps are so large that the ball is flying out of the basin. \n\n\nLet's try again with $\\eta$ an order of magnitude smaller.","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"738baa","input":"### Implementing Gradient Descent\n\nYou yourself will implement gradient descent, so I will not give explicit code here.\n\nBut we can still describe the form the function should take.\n\nIn our specs `grad_descent` will take two necessary parameters: `w` and `gradient`.\n\nThe first parameter is the initial value for $\\bar{w}$. This should generally be randomly generated from a normal distribution with small variance, though often the zero vector will work fine. \n\nThe `gradient` parameter is a *function*.  In python you can pass a function as a parameter just as you would pass any other parameter. \n\nIn the code below we have a function for the gradient of a paraboloid called `grad_paraboloid`.\n\nThis simple function just returns $\\nabla f = 2\\bar{w}$.\n\nYou should think about why $2[w_0,w_1]^T$ is the gradient of $f(w_0,w_1) = w_0^2 + w_1^2$.\n\nThis implementation of gradient descent also returns the path taken by the \"ball\" as it rolls down the hill. \n\nThis lets us make pretty pictures, but it can be useful for seeing whether convergence is happening also. \n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"9228d5","input":"### Gradient Descent\n\nIn these notes we give a simple example of the gradient descent optimization method.\n\nThis method is useful for solving the equation \n\n$$\\nabla E_{in}(\\bar{w}) = \\bar{0}.$$\n\nWe use this when $E_{in}(\\bar{w})$ does not have a simple form.\n\nIn linear regression $E_{in}(\\bar{w})$ was simple enough that we could solve the equation using linear algebra, but for many algorithms (such as logistic regression) the equation is too complex to solve.\n\nAdditionally this method has a \"cousin\" called Stochastic Gradient Descent which can be more efficient than even analytic solutions when the number of features $d$ is large.\n\n\nThe gradient descent method can actually minimize $f$ for a wide variety of functions $f(\\bar{w})$.\n\nBelow we use it to solve a simple paraboloid\n\n$$f(x,y) = x^2+y^2$$\n\nor\n\n$$f(w_0,w_1) = w_0^2+w_1^2$$\n\nThis is very similar to the actual form of $E_{in}(\\bar{w})$ in linear regression, which is also a quadratic in $\\bar{w}$.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"a4624f","input":"### Slow convergence\n\nIn the above image you can see that the ball quickly rolls down to the valley floor.\n\nBut the valley floor is kind of flat, and rolling the rest of the way to the minimum takes a long time.\n\nWhereas the ball reached the center in about 15 steps for the circularly symmetric error function,\n\nthis time it barely reaches the center even after 1000 steps.\n\n### Scaling\n\nThe effect of scaling the data here would be to go back to the circularly symmetric case.\n\nConvergence would happen much faster.\n\n**You should always scale the data when using gradient descent**\n\n### Intelligently controling the learning rate\n\nAnother technique we might try here is to dynamically alter the learning rate to try to \"roll fast\" over flat valleys.\n\nThat may or may not help depending on the problem.\n\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"a810ce","input":"### Numerical look\n\nBy printing our the path we can get a feel for whether convergence is happening.\n\nIn this case the algorithm is settling down to the true optimum, $\\bar{w} = [0,0]^T$.\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"ab43c9","input":"### Learning rate\n\nIn the above example we got the learning rate $\\eta=0.1$ just right for the function $f$.\n\nWe now consider what would happen if the learning rate were a lot smaller for this problem.\n\nWhat do you think will happen?\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"b36c20","input":"### The non-convex case\n\nIn the above examples the error function was **convex**.\n\nThis means that there is a **unique minimum or maximum**, which in the case of an error function will be the *global minimum*.\n\nThe error function for both linear regression and logistic regression are nice and convex.\n\n*However* in other cases the error function may not be convex.\n\nThis happens in particular for *neural networks*.\n\nWe have to be conscious that for non-convex functions gradient descent can converge to a **local minimum** which is not a **global miniumum**.\n\nBelow we consider gradient descent for a non-convex function which has 3 critical points, two of which are local minima.\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"ddfdec","input":"### Example: error differences\n\nBelow we look at how the change in error progresses as we go along the blue path.\n\nThese values are $f(p_{i+1})-f(p_i)$ for points $p_i$ along the path.\n\nBecause the path is decreasing into the bowl, these values are negative.\n\nNotice that the improvements are large at first, but eventually become small when the ball reaches the valley floor.","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"df0e57","input":"### Explanation of the algorithm\n\nFor a given input $\\bar{w}$, $\\nabla f(\\bar{w})$ is a vector **and** a function of $\\bar{w}$.\n\nIt has a **magnitude** and a **direction**.\n\nThe magnitude of $\\nabla f(\\bar{w})$ is the steepness of the hypersurface $z=f(\\bar{w})$ at the particular point $\\bar{w}$.\n\nThe direction of $\\nabla f(\\bar{w})$ is the direction of steepest increase.\n\nIf $f$ is an error function, then we actually want the direction of steepest **decrease** which is just $-\\nabla f(\\bar{w})$.\n\nWhen looking for the $\\bar{w}$ that minimizes a function $f$, one strategy is to start with some initial $\\bar{w}$ (such as the zero vector, or a random vector near the zero vector) and repeatedly **follow the gradient** \"downhill\".\n\n#### Learning rate\n\nUnfortunately sometimes the gradient is so big that you will jump right past the minimum.\n\nTo mitigate this, we \"shrink\" the gradient so that we only take a baby step.\n\nThe factor that scales down the gradient is called the **learning rate**.\n\nWe denote it by $\\eta$ and a good initial guess for a good $\\eta$ is 0.1.  \n\nAs we will see below, this may be too big or too small, and in that case it must be adjusted.\n\n#### Pseudocode\n\nThis brings us to the pseudocode for gradient descent:\n\n\n---\n\n$\\bar{w}$ = initial $\\bar{w}$\n\nuntil time to stop:\n\n$\\,\\,\\,\\,\\,\\bar{w} = \\bar{w} - \\eta\\bar{w}$\n\n---\n\nIn the slides below we discuss strategies for deciding when to stop.\n\nOne simple strategy that usually works pretty well is just to do 1000 iterations and hope for the best.\n","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"e0b7be","input":"### Four paths\n\nBelow we show the eventual fate of gradient descent for four different starting positions.\n\n#### Stuck on the head of a pin\n\nNotice that one starting point is (0,0), plotted in yellow.  \n\nThough this is not a max or a min, the process gets stalled there, because the gradient is zero at (0,0).\n\nThe ball is balanced in a very strange place.\n\nThis illustrates why $\\bar{w}=\\bar{0}$ can be problematic as initial starting weights.\n","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"e50a86","input":"### Picture time!\n\nWe now make a picture which superimposes the first 15 steps taken by the \"ball\" on the contour plot for $f$.\n\nYou can see that the ball is rapidly \"rolling downhill\".","pos":5,"type":"cell"}
{"id":0,"time":1589852981830,"type":"user"}
{"last_load":1589852982134,"type":"file"}