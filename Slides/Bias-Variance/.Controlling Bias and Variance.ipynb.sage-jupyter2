{"backend_state":"ready","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":20.131291028447674,"memory":44535808},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"type":"settings"}
{"cell_type":"code","exec_count":20,"id":"f542d7","input":"from sklearn.datasets import load_boston\nimport numpy as np\n\nX, y = load_boston(return_X_y=True)\nshuff = np.random.permutation(X.shape[0])\nX = X[shuff]\ny = y[shuff]","pos":1,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":21,"id":"139573","input":"from sklearn.model_selection import train_test_split\n#np.random.seed(54634)#389247 #654654321\nrs = np.random.randint(100000)\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=rs)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]\nX_train.shape,X_test.shape","output":{"0":{"data":{"text/plain":"((379, 14), (127, 14))"},"exec_count":21,"output_type":"execute_result"}},"pos":2,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":22,"id":"b8109a","input":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(X_train,y_train)\nreg.score(X_test,y_test), reg.score(X_train,y_train)","output":{"0":{"data":{"text/plain":"(0.7443930892271237, 0.7319344630386133)"},"exec_count":22,"output_type":"execute_result"}},"pos":3,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":23,"id":"5e42ee","input":"import mystuff as ms\nfrom sklearn.metrics import r2_score as R2\nfrom sklearn.metrics import mean_squared_error as MSE\nimport numpy as np\n\ndef mse(w,X,y):\n    return 1/X.shape[0]*(w.T.dot(X.T).dot(X).dot(w)-2*w.T.dot(X.T).dot(y)+y.dot(y))\n\ndef mse_gradient(w,X,y):\n    return 2/X.shape[0]*(X.T.dot(X).dot(w)-X.T.dot(y))\n\nw_init = np.random.randn(X_train.shape[1])\n\nwgd,path = ms.grad_descent(w_init,X_train,y_train,mse_gradient,eta=0.07,max_iter=10000)\n\ny_test_hat = X_test.dot(wgd)\n\nprint(\"w (gradient descent) = {}\".format(wgd))\nprint(\"R2 (gradient descent)= {}\".format(R2(y_test,y_test_hat)))","output":{"0":{"name":"stdout","output_type":"stream","text":"w (gradient descent) = [ 28.80321892 -10.83436169   4.32149668   0.80869171   2.28699621\n  -8.45605225  15.79126244  -0.85644796 -15.28751321   6.49199518\n  -5.92724429  -9.28384746   3.1803015  -18.47833904]\nR2 (gradient descent)= 0.7444570541099567\n"}},"pos":4,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":24,"id":"0a7a21","input":"test_scores = [mse(w,X_test,y_test) for w in path]\ntrain_scores = [mse(w,X_train,y_train) for w in path]\n\nR2test_scores = [R2(X_test.dot(w),y_test) for w in path]\nR2train_scores = [R2(X_train.dot(w),y_train) for w in path]","pos":5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":25,"id":"1a3fcb","input":"import matplotlib.pyplot as plt\n\ndom = range(len(test_scores))\n\nplt.ylim(min(test_scores)-0.5,min(test_scores)+2)\nplt.plot(dom,test_scores,label=\"test error (MSE)\")\n\n#plt.plot(dom,train_scores,label=\"train error\")\nplt.legend()\nplt.title(\"Test error at each time step\")\nplt.ylabel(\"MSE\")\nplt.xlabel(\"Iteration\")\nplt.show()\n\n\nplt.ylim(15,30)\n#plt.plot(dom,R2test_scores,label=\"test error (R2)\")\nplt.plot(dom,test_scores,label=\"test error (MSE)\")\nplt.plot(dom,train_scores,label=\"train error\")\nplt.legend()\nplt.title(\"Test error at each time step\")\nplt.ylabel(\"MSE\")\nplt.xlabel(\"Iteration\")\nplt.show()","output":{"0":{"data":{"image/png":"d789bb9e9021026a16be839f26bdd6160f499b7e","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"image/png":"abb00e498c7cf337b9a716fb4d525ae8f5689261","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":6,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":26,"id":"333769","input":"best_stop = np.argmin(test_scores)\nbest_w = path[best_stop]\nprint(best_stop)\nmse(best_w,X_test,y_test),mse(best_w,X_train,y_train)\n#R2(X_test.dot(best_w),y_test),R2(X_train.dot(best_w),y_train)","output":{"0":{"name":"stdout","output_type":"stream","text":"2553\n"},"1":{"data":{"text/plain":"(29.23689255089587, 19.93821861400058)"},"exec_count":26,"output_type":"execute_result"}},"pos":7,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":27,"id":"bba9cc","input":"last_w = path[-1]\nmse(last_w,X_test,y_test)","output":{"0":{"data":{"text/plain":"29.493187198303566"},"exec_count":27,"output_type":"execute_result"}},"pos":8,"state":"done","type":"cell"}
{"cell_type":"code","id":"7ba72f","input":"","pos":12,"state":"done","type":"cell"}
{"cell_type":"code","id":"c01e9f","input":"","pos":10,"state":"done","type":"cell"}
{"cell_type":"code","id":"f81b4a","input":"","pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"01d530","input":"### Best stopping time\n\nThe results above show that test error is minimized when we reach the 2553rd iteration of gradient descent (with these parameters).\n\nNote that this only applies to MSE in this example.\n\nWe could easily automate the selection of this best parameter (in fact we kind of did).\n\nAlso note that we \"snooped\" on the test set when we found the best $g \\in \\mathcal{H}$.  \n\nNow if we want to predict the true $E_{out}(g)$ we should use a reserve set of data (the validation set).","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d75768","input":"### Regularization\n\nIn some circumstances we might want to try a complex hypothesis on a small dataset.\n\nIn such a scenario bias will be low, but variance will be high.\n\nIn other words we will overfit.\n\nBut there are methods for avoiding overfitting by restricting the way in which $\\mathcal{H}$ is explored.\n\nIf we explore $\\mathcal{H}$ very conservatively maybe we can keep variance down while also keeping bias low.\n\n---\n\n### Early stopping\n\nOne easy way to reduce variance is to explore less of $\\mathcal{H}$ during gradient descent or stochastic gradient descent.\n\nThis method is called *early stopping*.\n\nWe give an example below.\n\n","pos":0,"state":"done","type":"cell"}
{"id":0,"time":1587071469932,"type":"user"}
{"last_load":1587070901933,"type":"file"}