{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":84078592},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"type":"settings"}
{"cell_type":"code","exec_count":373,"id":"8ffe7f","input":"#from sklearn.datasets import make_regression\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn_samples=20\nnoise=0.5\n\n### There was too much variability in the sklearn version of this function\n### so I implemented a version\ndef make_regression(n_features,noise,n_samples,random_state):\n    np.random.seed(random_state)\n    dom = np.linspace(0,3,3*n_samples)\n    dom = np.random.choice(dom,n_samples).reshape(n_samples,1)\n    nz = np.random.randn(n_samples,1)*noise\n    y = 2+1.7*dom+nz\n    return dom,y\n","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":374,"id":"bcad11","input":"#from sklearn.datasets import make_regression\nnoise=0.5\nXnb,y = make_regression(n_features=1,noise=noise,n_samples=n_samples,random_state=42)\nXnb = np.sqrt(Xnb-np.min(Xnb))\nplt.scatter(Xnb,y)\nplt.title(\"The data to be fit\")\ndata = np.c_[Xnb,y]\nplt.show()","output":{"0":{"data":{"image/png":"ffc9b545afe29eb0ba0c767ab22a88aadae4e903","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":375,"id":"0827c4","input":"#X = np.c_[np.ones(Xnb.shape[0]),Xnb[:,0]]\nXnb[:5]\n#y[:5]","output":{"0":{"data":{"text/plain":"array([[1.35296285],\n       [1.57845666],\n       [1.14979733],\n       [0.78113347],\n       [1.42614807]])"},"exec_count":375,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":376,"id":"93891d","input":"## Making degree 1 features turns out to be an easy way to add a bias column!\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(1)\nXp = poly.fit_transform(Xnb)\nXp[:2]","output":{"0":{"data":{"text/plain":"array([[1.        , 1.35296285],\n       [1.        , 1.57845666]])"},"exec_count":376,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":377,"id":"163b12","input":"from sklearn.metrics import r2_score\nw1 = np.linalg.pinv(Xp).dot(y)\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = np.c_[np.ones(xx.shape[0]),xx]\nyhat = x.dot(w1)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Linear fit to quadratic data\")\nplt.show()\nr2_score(Xp.dot(w1),y)","output":{"0":{"data":{"image/png":"8220a03202e43c0740b67a9f47ec46943da78ada","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.7770681429083474"},"exec_count":377,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":378,"id":"dd8d4c","input":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(2)\nXp = poly.fit_transform(Xnb)\nXp[:2]","output":{"0":{"data":{"text/plain":"array([[1.        , 1.35296285, 1.83050847],\n       [1.        , 1.57845666, 2.49152542]])"},"exec_count":378,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":379,"id":"8305b7","input":"w2 = np.linalg.pinv(Xp).dot(y)\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w2)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Quadratic fit to quadratic data\")\nplt.show()\nr2_score(Xp.dot(w2),y)","output":{"0":{"data":{"image/png":"32205a79a872f0758cfc642783d11b883677b4e6","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.8835903755842587"},"exec_count":379,"output_type":"execute_result"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":380,"id":"d27f43","input":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(5)\nXp = poly.fit_transform(Xnb)\nXp[:2]","output":{"0":{"data":{"text/plain":"array([[1.        , 1.35296285, 1.83050847, 2.47660996, 3.35076128,\n        4.53345553],\n       [1.        , 1.57845666, 2.49152542, 3.9327649 , 6.20769894,\n        9.79858372]])"},"exec_count":380,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":381,"id":"10c21f","input":"w5 = np.linalg.pinv(Xp).dot(y)\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w5)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Quintic fit to quadratic data\")\nplt.show()\nr2_score(Xp.dot(w5),y)","output":{"0":{"data":{"image/png":"9da8494094aab2441152cda057900793847a72cb","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.9212041566404681"},"exec_count":381,"output_type":"execute_result"}},"pos":15,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":382,"id":"7f816e","input":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(30)\nXp = poly.fit_transform(Xnb)\n","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":383,"id":"3206db","input":"w30 = np.linalg.pinv(Xp).dot(y)\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w30)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 30 fit to quadratic data\")\nplt.show()\nr2_score(Xp.dot(w30),y)","output":{"0":{"data":{"image/png":"78ce608f1530bfdcb61e50b202c0ab5fbc17eb5b","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.9596877390826393"},"exec_count":383,"output_type":"execute_result"}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":384,"id":"71bbc4","input":"plt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0],num=300)\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w30)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 30 fit to quadratic data\")\nplt.ylim(min(y)-0.1*(max(y)-min(y)),max(y)+0.1*(max(y)-min(y)))\nplt.show()\nr2_score(Xp.dot(w30),y)","output":{"0":{"data":{"image/png":"b0bb9e5927984d490ee026d9d9c598abb4dc3b2e","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.9596877390826393"},"exec_count":384,"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":385,"id":"e45bd7","input":"Xnb,y = make_regression(n_features=1,noise=noise,n_samples=1000,random_state=42)\nXnb = np.sqrt(Xnb-np.min(Xnb))\nplt.scatter(Xnb,y)\ndata = np.c_[Xnb,y]\nplt.title(\"A much larger sample of data from the same distribution\")\nplt.show()","output":{"0":{"data":{"image/png":"85edc6d420fc6d7d18902db89e5b4b8d3dd802be","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":386,"id":"a89d1a","input":"poly = PolynomialFeatures(30)\nXp = poly.fit_transform(Xnb)\n\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0],num=300)\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w30)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 30 fit to quadratic data\")\nplt.ylim(min(y)*1.5,max(y)*1.5)\nplt.show()\nr2_score(Xp.dot(w30),y)","output":{"0":{"data":{"image/png":"3cfa9b853070f35e64d7e63ae48db3d343e8ddde","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"-0.045522493223237426"},"exec_count":386,"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":387,"id":"03d194","input":"poly = PolynomialFeatures(5)\nXp = poly.fit_transform(Xnb)\n\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w5)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 5 fit to quadratic data\")\nplt.ylim(min(y)*1.5,max(y)*1.5)\nplt.show()\nr2_score(Xp.dot(w5),y)","output":{"0":{"data":{"image/png":"7f3d4367f00e0ebbaa1b2d27d94c005718186f02","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.7814373198336974"},"exec_count":387,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":388,"id":"37b26e","input":"poly = PolynomialFeatures(2)\nXp = poly.fit_transform(Xnb)\n\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w2)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 2 fit to quadratic data\")\n#plt.ylim(min(y)*1.5,max(y)*1.5)\nplt.show()\nr2_score(Xp.dot(w2),y)","output":{"0":{"data":{"image/png":"6d6a6f7f2bcdcde8105af3c9ecd89e05181b2a5a","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.8439397548066159"},"exec_count":388,"output_type":"execute_result"}},"pos":24,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":389,"id":"0eac5b","input":"poly = PolynomialFeatures(1)\nXp = poly.fit_transform(Xnb)\n\nplt.scatter(data[:,0],data[:,1])\nxx = np.linspace(start=np.min(data,axis=0)[0],stop=np.max(data,axis=0)[0])\nx = poly.fit_transform(xx.reshape(-1,1))\nyhat = x.dot(w1)\nplt.plot(xx,yhat,c='r')\nplt.title(\"Degree 1 fit to quadratic data\")\n#plt.ylim(min(y)*1.5,max(y)*1.5)\nplt.show()\nr2_score(Xp.dot(w1),y)","output":{"0":{"data":{"image/png":"6cb00b8093a4370c1b52ecdae1869970bc6da09a","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"data":{"text/plain":"0.6657674301366077"},"exec_count":389,"output_type":"execute_result"}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":390,"id":"e0b861","input":"from sklearn.metrics import mean_squared_error\n\ndef make_point(X_train,X_test,y_train,y_test):\n    w = np.linalg.pinv(X_train).dot(y_train)\n    error = mean_squared_error\n    #error = r2_score\n    s_tr = error(X_train.dot(w),y_train)\n    s_te = error(X_test.dot(w),y_test)\n    return np.array([s_tr,s_te])\n\ndef make_curve(X,y):\n    N = X.shape[0]\n    samp = 5\n    points = []\n    for n in range(3,N//2):\n        scores = np.zeros(2)\n        for s in range(samp):\n            shuff = np.random.permutation(N)\n            Xs = X[shuff]\n            ys = y[shuff]\n            mp = make_point(Xs[:n],Xs[-N//2:],ys[:n],ys[-N//2:])\n            scores += mp\n        scores /= samp\n        points.append(scores)\n    return np.array(points)\n\n","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":391,"id":"5d4416","input":"degree=1\npoly = PolynomialFeatures(degree)\nXp = poly.fit_transform(Xnb)\n\nN= Xp.shape[0]\npoints = make_curve(Xp,y)\nC1 = np.array(list(zip(np.arange(len(points)),points[:,0])))\nC2 = np.array(list(zip(np.arange(len(points)),points[:,1])))\n\n\nplt.plot(C1[:,0],C1[:,1],alpha=0.95,label=\"training error\")\n\nplt.plot(C2[:,0],C2[:,1],alpha=0.95,label=\"test error\")\nplt.ylim(0,.95)\nplt.legend()\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"Error\")\nplt.title(\"Learning Curve, Degree={}\".format(degree))\nplt.show()\nprint(\"High bias--average best hypothesis not great\\n Quickly converges but error is high\")","output":{"0":{"data":{"image/png":"70b428b24f29cc4ca3205678de17fc83b6c60bb1","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"High bias--average best hypothesis not great\n Quickly converges but error is high\n"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":392,"id":"1522a2","input":"poly = PolynomialFeatures(2)\nXp = poly.fit_transform(Xnb)\nN= Xp.shape[0]\npoints = make_curve(Xp,y)\nC1 = np.array(list(zip(np.arange(len(points)),points[:,0])))\nC2 = np.array(list(zip(np.arange(len(points)),points[:,1])))\nplt.plot(C1[:,0],C1[:,1],alpha=0.95,label=\"training error\")\n\nplt.plot(C2[:,0],C2[:,1],alpha=0.95,label=\"test error\")\nplt.ylim(0,.95)\nplt.legend()\nplt.title(\"Learning Curve, Degree=2\")\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"Error\")\nplt.show()\nprint(\"Ideal -- converge to low error, not sensitive to training set\")\n","output":{"0":{"data":{"image/png":"dd0255f50e682fa3e0d3cb4b24efc9510a77c2ce","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"Ideal -- converge to low error, not sensitive to training set\n"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":393,"id":"6b810b","input":"degree = 5\npoly = PolynomialFeatures(degree)\nXp = poly.fit_transform(Xnb)\nN= Xp.shape[0]\npoints = make_curve(Xp,y)\nC1 = np.array(list(zip(np.arange(len(points)),points[:,0])))\nC2 = np.array(list(zip(np.arange(len(points)),points[:,1])))\nplt.plot(C1[:,0],C1[:,1],alpha=0.95,label=\"training error\")\n\nplt.plot(C2[:,0],C2[:,1],alpha=0.95,label=\"test error\")\nplt.ylim(0,.95)\nplt.legend()\nplt.title(\"Learning Curve, Degree={}\".format(degree))\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"Error\")\nplt.show()\nprint(\"Converges to low error, but out of sample\\n performance beginning to depend on training set\")","output":{"0":{"data":{"image/png":"4422542f2841aadc306d3d5213ab78a528b64b5b","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"Converges to low error, but out of sample\n performance beginning to depend on training set\n"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":394,"id":"639cec","input":"degree = 10\npoly = PolynomialFeatures(degree)\nXp = poly.fit_transform(Xnb)\nN= Xp.shape[0]\npoints = make_curve(Xp,y)\nC1 = np.array(list(zip(np.arange(len(points)),points[:,0])))\nC2 = np.array(list(zip(np.arange(len(points)),points[:,1])))\nplt.plot(C1[:,0],C1[:,1],alpha=0.95,label=\"training error\")\n\nplt.plot(C2[:,0],C2[:,1],alpha=0.95,label=\"test error\")\nplt.ylim(0,.95)\nplt.legend()\nplt.title(\"Learning Curve, Degree={}\".format(degree))\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"Error\")\n\nplt.show()\nprint(\"Converges to low error, but out-of-sample\\n performance is dependent on training set\")","output":{"0":{"data":{"image/png":"abace88c494dcca516fb60dc39f88ad9a62e6e49","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"Converges to low error, but out-of-sample\n performance is dependent on training set\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":395,"id":"5869e1","input":"degree = 30\npoly = PolynomialFeatures(degree)\nXp = poly.fit_transform(Xnb)\nN= Xp.shape[0]\npoints = make_curve(Xp,y)\nC1 = np.array(list(zip(np.arange(len(points)),points[:,0])))\nC2 = np.array(list(zip(np.arange(len(points)),points[:,1])))\nplt.plot(C1[:,0],C1[:,1],alpha=0.95,label=\"training error\")\n\nplt.plot(C2[:,0],C2[:,1],alpha=0.95,label=\"test error\")\nplt.ylim(0,.95)\nplt.legend()\nplt.title(\"Learning Curve, Degree={}\".format(degree))\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"Error\")\nplt.show()\nprint(\"High variance--performance very dependent on training data\")","output":{"0":{"data":{"image/png":"7c8ff10fecf706cf5acfcff2bd84d6f903c0b19c","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"High variance--performance very dependent on training data\n"}},"pos":32,"type":"cell"}
{"cell_type":"code","id":"268286","input":"","pos":34,"type":"cell"}
{"cell_type":"code","id":"7a1069","input":"","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"0361c8","input":"### An even better fit...\n\nThe degree 5 curve fits the data even better than the degree 2 curve.\n\nHowever you can see even at this level that the curve is somewhat contorted.\n\nIt is specialized to *this particular* training set rather than the probability distribution that generates the training set.\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"0bc4f3","input":"### Degree 2\n\nWe now perform a degree 2 polynomial feature transformation.\n\n$$[x_1]^T \\rightarrow [x_1^0,x_1^1,x_1^2]^T = [1,x_1^1,x_1^2]^T$$","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"2326b8","input":"### Degree 1\n\nA degree 1 polynomial is just a line.  \n\nThe cell below achieves the variable transformation\n\n$$[x_1]^T \\rightarrow [x_1^0,x_1^1 ]^T = [1,x_1]^T$$\n\nIn other words, it just adds a bias column.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"3a25cc","input":"### Out of sample error\n\nWe now estimate the out of sample error for each of the above models.\n\nTo do this we produce more data generated from the same probability distribution $P(x,y)$ as the original data.\n\nThe performance of each model on this larger dataset will be very close to the true out-of-sample error of the model.\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"66ad86","input":"### The normal equations\n\nBelow and elsewhere we fit the data using the normal equations.\n\nThis is the solution $\\bar{w} = X^\\dagger y$, where $X^\\dagger$ is the pseudo-inverse.\n\nWe also print the quality of the fit under the $R^2$ error metric.","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"6c2057","input":"### Polynomial regression\n\nBelow we explore issues of under and overfitting data using polynomial regression on synthetic data.\n\nThe advantage to this presentation is that we can visualize the complexity of the fitted curve.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"7f2510","input":"### Degree 5\n\nWe now consider a degree 5 transformation of the data.\n\n$$[x_1]^T \\rightarrow [x_1^0,x_1^1,x_1^2,x_1^3,x_1^4,x_1^5]^T = [1,x_1^1,x_1^2,x_1^3,x_1^4,x_1^5]^T$$","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"93373a","input":"### Degree 30\n\nWe now fit the data with a degree 30 curve.\n\nThe in-sample error drops to almost zero.\n\nBut will it hold up when we introduce testing data?","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"c373b8","input":"### A better fit\n\nThe quadratic curve is a better fit for the data.\n\nBecause we constructed the data, we know that the quadratic curve is the \"actual\" complexity of the target function.\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"c40741","input":"### Summary thus far\n\nFor the data in this problem we saw that the degree 2 solution provides the **best generalization**. \n\nIt has both of our desirable properties:\n\n1. $E_{in}(g) \\approx E_{out}(g)$  (both are about 0.85)\n2. $E_{out}(g)$ is minimal (at least among the models considered).\n\n**For this data** the quadratic model is the one that optimizes the **tradeoff between bias and variance**.\n\n**bias**: Simple models are \"biased\" and may be incapable of low error if they cannot match the complexity of the target function.\n\n**variance**: Overly complex models are so flexible that they may \"memorize\" the training data rather than learning the statistical properties of $P(\\bar{x},y)$.\n\n\nIn high bias scenario:\n---\nDifferent training sets give same quality $g$, but performance is low.\n\nIn high variance scenario:\n---\n\nBest possible $g$ is very good, but which $g$ you get varies wildly with which training data is used.\n\n\n### Learning curves\n\n","pos":26,"type":"cell"}
{"id":0,"time":1585237710849,"type":"user"}
{"last_load":1585060307245,"type":"file"}