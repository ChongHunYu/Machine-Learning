{"backend_state":"ready","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":16.75977653631035,"memory":44548096},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"076c93","input":"##Bias/variance breakdown of high degree model\nb,v = bias(Gn,codomain),variance(Gn)\nb,v,b+v","pos":10,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"568ec5","input":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\n","pos":8,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"63c53b","input":"plt.plot(domain,codomain,label=\"f\",alpha=0.4)\n\nplt.plot(domain,np.mean(Gn,axis=0),label=\"degree {}\".format(deg),alpha=0.4)\nplt.plot(domain,np.mean(G,axis=0),label=\"degree {}\".format(1),alpha=0.4)\nplt.legend()\nplt.show()\nprint(\"The average degree 7 model and f are indistinguishable\")","pos":12,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"71c598","input":"mse(np.mean(Gn,axis=0),codomain)","pos":13,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"82f9e1","input":"","pos":17,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b17d05","input":"##Bias/variance breakdown of degree 1 model\nb,v = bias(G,codomain),variance(G)\nb,v,b+v","pos":9,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"bf43ec","input":"","pos":11,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f9ab11","input":"from sklearn.preprocessing import PolynomialFeatures\ndeg = 7\npoly = PolynomialFeatures(deg)\npolydom = poly.fit_transform(domain.reshape(-1,1))\n\ncodomain = (domain)**3-domain\nmodel = LinearRegression()\n\nplt.figure(figsize=(8,10))\nGn = gbar(K,N,model,polydom,codomain)\nfor gg in Gn:\n    plt.plot(domain,gg,alpha=0.1)\nplt.plot(domain,codomain,c='r',label=r'$f$')\nplt.plot(domain,np.mean(Gn,axis=0),c='b',label=r'$\\bar{g}$')\nplt.legend()\nplt.title(r\"Computing $\\bar{g}$ for \"+str(deg)+r\" dim $\\mathcal{H}$, cubic $f$\")\nplt.show()","pos":7,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":1,"id":"19d355","input":"import numpy as np\ndef gbar(K,N,model,domain,codomain):\n    \"\"\"     Parameters:\n        K = number of datasets to produce\n        N = size of each dataset\n        model = the model to evaluate\n        domain = the domain of the target f (1-D array)\n        codomain = the values of the target f (1-D array)\n            Output:\n        G is a 2-D numpy array.\n        The rows of G are g^D for various datasets D.\n        The columns of G are the x in domain(f)\n        An entry D,x is g^D(x).\n        The mean of G along the 0 axis is gbar\n    \"\"\"\n    \n    G=[]\n\n    for i in range(K):\n        X=[]\n        y=[]\n        for n in range(N):\n            index = np.random.randint(0,len(domain))\n            x = domain[index]\n            yy = codomain[index]\n            X.append(x)\n            y.append(yy)\n        X = np.array(X)\n        y = np.array(y)\n        model.fit(X,y)\n        g = model.predict(domain)\n        G.append(g)\n    return np.array(G)\n\ndef variance(G):\n    return np.mean((G-np.mean(G,axis=0))**2)\n\ndef bias(G,codomain):\n    return np.mean((np.mean(G,axis=0)-codomain)**2)\n\n","pos":4,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":2,"id":"3e1dc3","input":"K,N= 10000,10\n\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ndomain = np.linspace(-5,5)\ncodomain = (domain)**3-domain\nmodel = LinearRegression()\n\nplt.figure(figsize=(8,10))\nG = gbar(K,N,model,domain.reshape(-1,1),codomain)\nfor gg in G:\n    plt.plot(domain,gg,alpha=0.1)\nplt.plot(domain,codomain,c='r',label=r'$f$')\nplt.plot(domain,np.mean(G,axis=0),c='b',label=r'$\\bar{g}$')\nplt.legend()\nplt.title(r\"Computing $\\bar{g}$ for linear $\\mathcal{H}$, cubic $f$\")\nplt.show()\n","output":{"0":{"data":{"image/png":"e34d8909be0677208a3d6858e58a3cc6c43e5711","text/plain":"<Figure size 576x720 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":591,"width":495},"needs_background":"light"},"output_type":"execute_result"}},"pos":5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":3,"id":"c3dceb","input":"quaddom = np.c_[domain,domain**2]\ncodomain = (domain)**3-domain\nmodel = LinearRegression()\n\nplt.figure(figsize=(8,10))\nGq = gbar(K,N,model,quaddom,codomain)\nfor gg in Gq:\n    plt.plot(domain,gg,alpha=0.1)\nplt.plot(domain,codomain,c='r',label=r'$f$')\nplt.plot(domain,np.mean(Gq,axis=0),c='b',label=r'$\\bar{g}$')\nplt.legend()\nplt.title(r\"Computing $\\bar{g}$ for quadratic $\\mathcal{H}$, cubic $f$\")\nplt.show()","output":{"0":{"data":{"image/png":"300a8480bb8e3e45357ca9e14dcaf7c861613767","text/plain":"<Figure size 576x720 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":591,"width":495},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"10bddb","input":"![img](biasvariance2.png)","pos":14,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1d57ea","input":"### Bias vs Variance\n\nIn the slides on polynomial regression we encountered the central dilemma in machine learning: the bias/variance tradeoff.\n\nHere we make that idea formal and explore it on some synthetic and real datasets.\n\nThis discussion corresponds to pages 62-65 of Learning From Data.\n\n---\n\nFirst we fix $P(\\bar{x},y)$ which is the joint distribution on examples we might see.  \n\nIn our \"frogs in the woods\" metaphor this determines which frogs you are likely to see in the woods, and which species they are likely to be.\n\nThe central character in the story is $\\bar{g}$.  \n\nThis guy is the \"average best hypothesis\".  The average is over all the possible training sets $\\mathcal{D}$ that you might happen to get. \n\n$$\\bar{g}(\\bar{x}) = \\mathbb{E}_\\mathcal{D}[g^\\mathcal{D}(\\bar{x})]$$\n\nwhere $\\mathcal{D}$ ranges over all training sets and $g^\\mathcal{D}$ is the final hypothesis for training set $\\mathcal{D}$.\n\nIn terms of the metaphor, this is the average hypothesis you might produce after seeing the frogs that you see in your woods walk.\n\n---\n\nNow **bias** is how far $\\bar{g}$ is from the target function $f$.  \n\n${\\bf bias}(\\bar{x}) = (\\bar{g}(\\bar{x})-f(\\bar{x}))^2$\n\nIf the hypothesis space $\\mathcal{H}$ is complex then this will be low. \n\nIn an extreme case we might have $f \\in \\mathcal{H}$ in which case bias would be zero.\n\n---\n\nNow we will discuss variance. \n\nWe know that $\\bar{g}$ is the average best hypothesis where the average is over all datasets $\\mathcal{D}$.  \n\nBut for any particular $\\mathcal{D}$, we might get a best hypothesis $g^\\mathcal{D}$ which is not very close to the average.  This is **variance**.\n\n${\\bf variance}(\\bar{x}) = (g^\\mathcal{D}(\\bar{x}) - \\bar{g}(\\bar{x}))^2$.\n\n---\n\nWe saw this in polynomial regression where the high degree fit of a small dataset varied wildly depending on which points we happened to draw for our $\\mathcal{D}$.\n\nLearning algorithms with a complex $\\mathcal{H}$ tend to have high variance.  \n\nIntuitively this is because there are lots of hypotheses and for any given dataset one of them will through \"pure luck\" happen to be perfectly suited to it.\n\nBut because it is so tailored to the particular $\\mathcal{D}$ it may not be very much like the \"typical\" best hypothesis $\\bar{g}$.\n\n---\n\nNow we define\n\n${\\bf bias + variance} = \\mathbb{E}_\\bar{x}[{\\bf bias}(\\bar{x}) + {\\bf variance}(\\bar{x})]$.\n\nOne can prove that the expected out of sample MSE error is bias+variance.\n\n$\\mathbb{E}_\\mathcal{D}[E_{out}(g^\\mathcal{D})] = {\\bf bias + variance}.$\n\nThis is saying that the average of $E_{out}(g)$ over all possible datasets is literally bias plus variance.\n\n---\n\nWhile the derivation is based on MSE error, something analogous will be true for other error measures.\n\n### Summary\n\nIn common usage **bias** is the penalty paid for not having a rich set of hypotheses\n\nand **variance** is the penalty paid for having such a complex $\\mathcal{H}$ that training data is overfit.\n\nThere is a **tradeoff** because when bias goes down variance goes up, and conversely.\n","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"2e330b","input":"#hi\n<img src=\"derivation.jpg\" alt=\"Flowers in Chania\",width=300>\n","pos":16,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"51bcb8","input":"### Finding $\\bar{g}$\n\nBelow we write some code for computing $\\bar{g}$ as a function of $\\mathcal{H}$, $P(\\bar{x},y)$ and the learning algorithm.\n\nThe idea is to generate a bunch of datasets $\\mathcal{D}_1,\\mathcal{D}_2,\\mathcal{D}_3,\\ldots,\\mathcal{D}_K$.\n\nThen for each $\\mathcal{D}_i$ we learn a best hypothesis $g_i$.\n\nThen\n\n$$\\bar{g}(\\bar{x}) = \\frac{1}{K}\\sum_{i=1}^K g_i(\\bar{x}).$$\n\nThis has nothing to do with $f$!\n\nIt's just a property of the learning algorithm.","pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d6ca65","input":"![img](derivation.jpg)","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fa8d1b","input":"![img](bv.png)","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fb74c2","input":"### Let's see\n\nLet's write some code to verify the theory.\n\nWe will create a toy example in which we know $f$, $P(\\bar{x})$, and can verify all the quantities.\n\n","pos":2,"state":"done","type":"cell"}
{"id":0,"time":1587071465677,"type":"user"}
{"last_load":1587070958664,"type":"file"}