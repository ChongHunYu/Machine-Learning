{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83505152},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"type":"settings"}
{"cell_type":"code","id":"37ccfd","input":"","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"1076b9","input":"## Forms of weight decay...\n\nThese error functions all penalize \"complex\" hypotheses by punishing weights $\\bar{w}$ with a large norm.\n\nThere are three basic strategies:\n\n1. Punish $|\\!|\\bar{w}|\\!|^2$ where $|\\!|\\bar{w}|\\!|$ is the [L2 norm](https://mathworld.wolfram.com/L2-Norm.html)\n2. Punish $|\\!|\\bar{w}|\\!|_1$ where $|\\!|\\bar{w}|\\!|_1$ is the [L1 norm](https://mathworld.wolfram.com/L1-Norm.html)\n3. Punish a weighted combination of the options from (1) and (2)\n\nNote that\n\n$|\\!|\\bar{w}|\\!|^2 = \\sum_{i=0}^d w_i^2$\n\nand\n\n$|\\!|\\bar{w}|\\!|_1 = \\sum_{i=0}^d |w_i|$\n\nWhen the underlying model is linear regression, \n\n(1) is called Ridge regression\n\n(2) is called LASSO\n\n(3) is called [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n\nThere is a nice discussion of Ridge and LASSO here: https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n\n\nThe LASSO approach tends to perform \"feature selection\" because in the resulting solution many dimensions of the weight vector will be zero.  (picture from [here](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b))\n\n![img](L1_vs_L2.png)\n\n\nMany interesting and informative plots on regularization can be found here:\n\n\nhttps://github.com/ageron/handson-ml/blob/master/04_training_linear_models.ipynb\n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"71a54e","input":"### Formulas\n\n#### L2\n\n$E_{aug}(\\bar{w}) = E_{in}(\\bar{w}) + \\lambda\\sum_{i=0}^d w_i^2$\n\n#### L1\n\n$E_{aug}(\\bar{w}) = E_{in}(\\bar{w}) + \\lambda\\sum_{i=0}^d |w_i|$\n\n#### Elastic Net\n\n$E_{aug}(\\bar{w}) = E_{in}(\\bar{w}) + \\lambda(r\\sum_{i=0}^d |w_i| +(1-r)\\sum_{i=0}^d w_i^2)$\n\nwhere $0 \\leq r \\leq 1$.\n\n","pos":1,"type":"cell"}
{"id":0,"time":1588946197606,"type":"user"}
{"last_load":1588946199871,"type":"file"}