{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83685376},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"032175","input":"\n## Visualizing the learned weights...\n\nplt.imshow(w.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\nplt.title(\"A picture of w\")\nplt.colorbar()\nplt.show()","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"15e199","input":"data = df.values","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"19b2f1","input":"\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = StandardScaler()\n#scaler = MinMaxScaler()\n\n\nscaler.fit(X_tf)\nX_tf_n = scaler.transform(X_tf)","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"209c31","input":"clf = Perceptron(tol=1e-3, random_state=0)\nshuffle = np.random.permutation(X.shape[0])\nplt=ms.plot_learning_curve(clf, \"Learning curve for perceptron on MNIST\", X[shuffle], y[shuffle],cv=4,n_jobs=4)\nplt.show()","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"23b92d","input":"import pandas as pd\ndf=pd.read_csv(\"housing.data\",header=None,delim_whitespace=True)\ndf.head()","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2699f0","input":"type1 = X_tf_n[y=='3']\ntype2 = X_tf_n[y=='1']\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(type1[:,1],type1[:,2],c='b',alpha=0.1)\nplt.scatter(type2[:,1],type2[:,2],c='r',alpha=0.1)\nplt.title(\"Scaled scatterplot of symmetry vs intensity\")\nplt.show()\n","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"301972","input":"### Passing to xxd shows that it's probably space delimited\n!head -1 housing.data | xxd\n## space = 0x20","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"382d84","input":"threeOrOne = three+one\n\nX_f = X[threeOrOne]\ny_f = y[threeOrOne]\n\nX_train,X_test,y_train,y_test = train_test_split(X_f,y_f)\n\n\nclf = Perceptron(tol=1e-3, random_state=0)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\nw=clf.coef_[0]\n","pos":24,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5e54d3","input":"\ndef hflip(M):\n    return M[:,np.array(list(reversed(range(M.shape[1]))))]\n\ndef sym(M):\n    \"\"\"The average intensity of the difference of X and its horizontal reflection\"\"\"\n    P = hflip(M)\n    return np.mean(np.abs(P-M)/np.mean(M))\n\ndef intensity(M):\n    return np.mean(M)\n\nX_tf = []\nfor x in X:\n    M = x.reshape(28,28)\n    X_tf.append([1,-sym(M),intensity(M)])\nX_tf = np.array(X_tf)","pos":14,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"77e275","input":"type1 = X_tf[y=='3']\ntype2 = X_tf[y=='1']\n\n","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"77eeb9","input":"ybin = (y_test=='3')*2-1\n\nms.lin_boundary(w,X_test,ybin)\n","pos":22,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7a1956","input":"","pos":56,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7c0906","input":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnp.random.seed(22)\nX_train,X_test,y_train,y_test = train_test_split(X,y)\nscaler.fit(X_train)\nX_train=scaler.transform(X_train)\nX_test =scaler.transform(X_test)\n\n### The stupid scaler broke the bias column...\nX_train[:,0] = np.ones(X_train.shape[0])\nX_test[:,0] = np.ones(X_test.shape[0])\n\nw = np.linalg.pinv(X_train).dot(y_train)\ny_test_hat = X_test.dot(w)\nms.R2(y_test_hat,y_test)","pos":54,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"80bd4d","input":"import matplotlib.pyplot as plt\n\nplt.scatter(type1[:,1],type1[:,2],c='b',alpha=0.1)\nplt.scatter(type2[:,1],type2[:,2],c='r',alpha=0.1)\nplt.title(\"Unscaled scatterplot of symmetry vs intensity\")\nplt.show()","pos":17,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8ee68d","input":"\nms.R2(y,yhat)","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"9e52f3","input":"#clf = Perceptron(tol=1e-3, random_state=0)\n\n#ms.plot_learning_curve(clf, \"Learning curve for perceptron on MNIST\", X, y,cv=4,n_jobs=4)","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ae9cc5","input":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef noisycurve(f,dom,noise):\n    y =f(dom)+noise\n    plt.scatter(dom,y,alpha=0.5)\n    return y\n\nN = 30\nnoise = np.random.randn(N)*0.1\ndom = np.random.choice(np.arange(0,10,.1),N)\ny=noisycurve(np.sqrt,dom,noise)\nplt.show()","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b04a4b","input":"","pos":57,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b1405a","input":"import mystuff as ms\n\n\nX = np.ones((N,2))\nX[:,1] = dom\nw = np.array([0,3])\nH = ms.getH(X)\nyhat = H.dot(y)\nnoisycurve(np.sqrt,dom,noise)\nplt.plot(dom,yhat,color='red',label=\"regression line\")\nplt.legend()\nplt.show()","pos":37,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b3a96c","input":"ms.RMSE(y,yhat)","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c0d5f8","input":"clf = Perceptron(tol=1e-3, random_state=0)\nclf.fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\nw=clf.coef_[0]\nw","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c1ec55","input":"image = X[1007].reshape(28,28)\nplt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\nplt.imshow(hflip(image), cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\nplt.imshow(hflip(image)-image, cmap=plt.cm.gray_r, interpolation='nearest')\nplt.colorbar()\nplt.show()\nplt.imshow(np.abs(hflip(image)-image), cmap=plt.cm.gray_r, interpolation='nearest')\nplt.colorbar()\nplt.show()","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c204cc","input":"### Look at the first 3 lines of the data.  Is it space or tab delimited?\n### Notice that there is no header line.\n!head -3 housing.data","pos":44,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c94370","input":"X = data[:,:-1]\ny = data[:,-1]\nnp.random.seed(22)\nX_train,X_test,y_train,y_test = train_test_split(X,y)\nw = np.linalg.pinv(X_train).dot(y_train)\ny_test_hat = X_test.dot(w)\nms.R2(y_test_hat,y_test)","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"cc56c7","input":"from sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nthree = y=='3'\none = y=='1'\n\nthreeOrOne = three+one\n\nX_f = X_tf_n[threeOrOne]\ny_f = y[threeOrOne]\n\nX_train,X_test,y_train,y_test = train_test_split(X_f,y_f)\nX_train.shape, X_f.shape,y_train.shape,y_f.shape","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d902a4","input":"\ndf = ms.add_bias(df)    \ndf.head()","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e28ba9","input":"### Visualize data\n\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(X, y))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: {}'.format(label))\nfor ax, (image, label) in zip(axes[1, :], images_and_labels[4:8]):\n    ax.set_axis_off()\n    ax.imshow(image.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: {}'.format(label))    ","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f561bf","input":"### Get MNIST dataset\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX.shape","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"a2726d","input":"import numpy as np\n\nd = 5\nN = 10*d\n\nnp.sqrt(d/N*np.log(N))","output":{"0":{"data":{"text/plain":"0.6254616699229575"},"exec_count":1,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"cfaa7b","input":"import matplotlib.pyplot as plt\nN=np.arange(100,1000)\nfor d in [2,5,10]:\n    plt.plot(N,np.sqrt(d/N*np.log(N)),label=r\"$d$={}\".format(d))\nplt.legend()    \nplt.title(r\"Theoretical bound on $E_{out}-E_{in}$ for linear models\")\nplt.ylabel(r\"$E_{out}-E_{in}$\")\nplt.xlabel(r\"$N$\")\nplt.show()    ","output":{"0":{"data":{"image/png":"8273e12343de97f62482fc0fc5dd55c0f33b6265","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":2,"metadata":{"image/png":{"height":441,"width":721},"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"464eb7","input":"import mystuff as ms\n\nX,y,w=ms.myblobs(sig_yes=1.2,sig_no=2)\nms.lin_boundary(w,X,y)","output":{"0":{"data":{"image/png":"a2ece92c7d9e9e3acfb70f3199d182e8504e25d1","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":440,"width":719},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"4bb12c","input":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nX,y = ms.make_nonlinear(sig=5)\n","output":{"0":{"data":{"image/png":"e258ad021208ae6af44f8a65ffbf4beed3aa7769","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":4,"metadata":{"image/png":{"height":440,"width":725},"needs_background":"light"},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"markdown","id":"03fa3b","input":"### Feature extraction\n\nThe book here goes off on a bit of a tangent with the MNIST handwritten digit dataset.\n\nWe follow them.\n\nFirst we load the data, and visualize it.\n\nThen we make the data 2D and visualize again.\n\nWe use the features of **intensity** and **symmetry** just as the book does.  \n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"0fc786","input":"### Adding a bias column\n\nLike other linear techniques, linear regression requires a bias column.\n\nWe quickly add one to the data.\n","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"1d043f","input":"### Learning curve analysis\n\nChecking the learning curve on this data it looks like we have high bias.\n\nThis means that our data is sufficient to allow a more complex hypothesis space.\n\nWe could make our $\\mathcal{H}$ more complex by introducing some extra derived variables (such as intensity).\n\nIt also seems like the default `max_iter` for the sklearn Perceptron is too high for this data.\n\n$E_{out}$ is minimized when we fit the perceptron for about 9 time units.\n\nAfter that overfitting may be occurring. \n\nWe could fix that by decreasing `max_iter`.\n\n(But this would decrease the complexity of $\\mathcal{H}$... The indications here are a little contradictory.)\n\nAnother possibility is that we just need to shuffle the data.\n","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"2dc5bb","input":"### Finding an optimal linear boundary for nonlinearly separable data\n\nIf data is linearly separable then the perceptron finds a decision boundary with $E_{in}(g)=0$.\n\nIf the data is not linearly separable then the perceptron does not do well.\n\nSuppose we want to find the absolute best hypothesis. \n\nWe need to solve the combinatorial optimization problem\n\n$$\\min_{\\bar{w}\\in\\mathbb{R}^{d+1}}\\frac{1}{N} \\sum_{n=1}^N [\\![ \\text{sign}(\\bar{w}^T\\bar{x}_n) \\neq y_n]\\!]$$\n\nBecause of the discrete nature of $[\\![\\cdot]\\!]$ and `sign` we can't optimize this with calculus as we normally would.  \n\nThis problem is known to be NP-hard.\n","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"381b13","input":"![slc](shuffled_lc.png)","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"450527","input":"### Yep, that was it :)","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"58d4f6","input":"### Linear Classification\n\nAs we've seen before, linear models have the following hypothesis space:\n\n$\\mathcal{H} = \\{\\text{sign}(\\bar{w}^T\\bar{x}) : \\bar{w} \\in \\mathbb{R}^{d+1}\\}$\n\nThis is where the data has $d$ features, and $\\mathcal{X} = \\{1\\}\\times\\mathbb{R}^{d+1}$\n\nThe $\\{1\\}$ refers to the *bias column* in the data. \n\nIf $\\bar{x} = (x_0,x_1,\\ldots,x_d)$, we always have $x_0 =1$.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"591029","input":"### A good $g$\n\nThe above proves that with linear models, even though $\\mathcal{H}$ is infinite,\n\n$$E_{in}(g) \\approx E_{out}(g).$$\n\nNow, can we make $E_{in}(g)$ small?\n\nThis depends on the data.\n\nIf the data is linearly separable, then $E_{in}(g) = 0$ (for the perceptron).\n\nBut the data might not be linearly separable.\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"5e837b","input":"### Almost linearly separable\n\nOften real world data is \"almost\" linearly separable.\n\nThis is not a formal concept, but the data tends to be almost separable by a hyperplane, except for a few noisy outliers.\n\nA linear model might still work well on this data.\n\nSometimes though we might have data that is \"inherently\" nonlinearly separable.\n\n(You can actually make linear models work in this case too, by transforming variables.)\n\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"602ba4","input":"![lc](learning_curve.png)","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"6500c4","input":"### Loading the data\n\nLoading the data is not always trivial when you don't have a lot of experience.\n\nWe quickly run through a few considerations and then load the dataset with pandas.\n","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"740f07","input":"### Improving the perceptron\n\nIt turns out there is an easy tweak to the perceptron that works pretty well for linear classification.\n\nThe *Pocket Algorithm* is like the PLA except that it terminates after a specified number of iterations (*e.g.* 1000).\n\nRather than return the last $\\bar{w}$ considered by the PLA, it returns the **best** w that was encountered during all of the iterations.  \n\nTo implement the pocket algorithm you simply alter the PLA so that it remembers the best weight vector found so far. \n\nThe name comes from keeping the best hypothesis \"in your pocket\".\n\nIf we haven't already, let's look at HW2...","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"756f65","input":"##  Regression \n\nWe now switch from classification to the problem of regression.\n\nIn classification we tried to learn a (possibly noisy) function\n\n$$f:\\mathcal{X} \\rightarrow \\mathcal{Y}$$\n\nwhere $\\mathcal{Y}$ was a discrete set of categories (cat or not).\n\nNow, in regression, we want to learn a (probably noisy) function\n\n$$f:\\mathcal{X} \\rightarrow \\mathcal{Y}$$\n\nwhere $\\mathcal{Y}$ was a continuous range of values (height, price, *etc*).","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"7d375e","input":"### Convert to numpy\n\nWorking data scientists might work entirely with pandas and sklearn but we want to understand the underlying algorithms.\n\nFor that reason we convert to numpy so that we can implement LR ourselves.\n","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"7ea103","input":"### No! \n\nScaling doesn't seem to help. \n\nThis R2 score is not great.  Is it us, or is it the data?\n\nIn a real application we would never know.\n\nBut for this toy data it's easy to compare our results with others:\n\nhttps://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n\nOur score is pretty typical for linear regression on this dataset.\n","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"817cce","input":"## Regression on a real dataset\n\nWe now perform regression on a commonly used dataset called the Boston Housing Dataset (described below).\n\nThe goal is to predict median housing value in suburbs of Boston based on their attributes.\n\nHere are a few of the features considered:\n\n```\n    1. CRIM      per capita crime rate by town\n    2. ZN        proportion of residential land zoned for lots over \n                 25,000 sq.ft.\n    3. INDUS     proportion of non-retail business acres per town\n    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n                 river; 0 otherwise)\n    5. NOX       nitric oxides concentration (parts per 10 million)\n    6. RM        average number of rooms per dwelling\n    7. AGE       proportion of owner-occupied units built prior to 1940\n    8. (etc)\n    ...\n    14. MEDV     Median value of owner-occupied homes in $1000's\n```    ","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"8ffe05","input":"### Let's go!\n\nNow we do a test/train split, do linear regression, and then check the R2 score.\n","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"b79112","input":"Below we give a made up example in which we try to use linear regression to learn data produced by a noisy square root.","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"bce138","input":"#### Coefficient of determination $R^2$\n\nA less data dependent notion of error is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$.\n\nIn \"simple\" univariate linear regression this is just $r^2$ where $r$ is the Pearson correlation coefficient.\n\nIn multivariate linear regression $R$ is the [coefficient of multiple correlation](https://en.wikipedia.org/wiki/Multiple_correlation).\n\nIntuitively $R^2$ is the proportion of variance in the target variable that is explained by the hypothesis.\n\nThe simplest formula is \n\n$$R^2 = 1-\\frac{SS_{res}}{SS_{tot}}$$\n\nWhere \n\n$$SS_{tot} = \\sum (y_i-\\bar{y})^2$$\n\nand \n\n$$SS_{res} = \\sum (y_i-\\hat{y}_i)^2.$$\n\nNote that, for all datasets,\n\n$$0 \\leq R^2 \\leq 1$$\n\nAn $R^2$ near 1 is good, and an $R^2$ near 0 is bad.\n\nThe definition of $R^2$ is unrelated to scale, so you can see if scaling improves this score in a meaningful way.\n\n","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"c5fd16","input":"### Full dimensionality\n\nReducing dimensionality almost always destroys information.\n\nWe can see from the code below that learning is more effective when all the columns are retained.\n\nBut this doesn't mean that you shouldn't add derived columns or transform existing columns -- often this will improve performance.\n\nAlso it is sometimes necessary to decrease dimensionality for the purposes of visualization or to speed up the learning process.\n\n","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"ccc4ef","input":"![ilc](ideal_lc.png)","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"dae721","input":"### Linear regression\n\nLinear regression is a very powerful and elegant regression algorithm.\n\nIn it's basic form it tries to fit the data with a hyperplane.\n\nThis assumes a certain flatness to the data.\n\nHowever we will see that by transformations of variables, LR can fit the data with many types of surface (curve in 2d).\n\n### The hat matrix\n\nWe have already seen how to perform linear regression.\n\nIn fact you have implemented it yourself.\n\nLinear regression is performed simply by computing the hat matrix\n\n$$H = X^T(XX^T)^{-1}X$$\n\nand then applying $H$ to the dependent variable $y$\n\n$$\\hat{y} = Hy$$\n\nNow $\\hat{y}$ are the predictions made by linear regression for each of the rows of $X$ by our hypothesis $h \\approx f$.\n\n$$\\hat{y}_i = h(\\bar{x}_i)$$\n$$ y_i = f(\\bar{x}_i)$$\n$$ \\hat{y}_i \\approx y_i$$\n\nWe will give the proof of this after a quick example.\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"de3e34","input":"### Linear Models\n\nLinear models are fast and simple algorithms.\n\nThey can do classification, regression, and probability estimation.\n\nWhen presented with a new ML problem, trying a linear model is a good starting place.\n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"e3cd5f","input":"### $E_{out}$ and $E_{in}$ \n\nIn a linear setting you can prove that (with high probability)\n\n$$E_{out}(g) = E_{in}(g) + O\\left (\\sqrt{\\frac{d}{N} \\ln N} \\right ) $$\n\nAs usual with distribution free bounds, this is probably pessimistic.","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"ef4b2d","input":"### Evaluating error\n\nHow good a fit is the line we found?  \n\nA common measure of error is the mean squared error (MSE):\n\n$$MSE(y,\\hat{y}) = \\frac{1}{N}\\sum (y_i - \\hat{y}_i)^2$$\n\nTo \"fix the units\" often the root is taken to form root mean squared error (RMSE):\n\n$$RMSE(y,\\hat{y}) = \\sqrt{\\frac{1}{N}\\sum (y_i - \\hat{y}_i)^2}$$\n\nThe problem with both of these measures is that they are dependent on the range of $y$.\n\nIf $y$ contains large values then RMSE will be large.\n\nIf $y$ contains small values then RMSE will be small. \n\nIt's hard to compare performance on different datasets (consider scaling).\n","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"ff158c","input":"### Does scaling help?\n\nWe try scaling the data to see if that will improve performance.","pos":53,"type":"cell"}
{"id":0,"time":1589853078469,"type":"user"}
{"last_load":1589853079203,"type":"file"}