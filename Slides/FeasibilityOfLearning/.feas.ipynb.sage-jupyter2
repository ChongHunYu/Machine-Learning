{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":80969728},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"156871","input":"","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":1,"id":"82076c","input":"import mystuff as ms\nimport numpy as np\n\n\nX,y,w = ms.myblobs(N=30)\nms.lin_boundary(w,X,y)\n","output":{"0":{"data":{"image/png":"d6d7e33b3e303e59451a43a54da0f23c9cb1186a","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":1,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"99e3b6","input":"import numpy as np\n## |D|=10, |H|=100\nflips = np.random.randint(0,2,100*10).reshape(10,100)\n\nmu = np.mean(flips,axis=0)\nmu","output":{"0":{"data":{"text/plain":"array([0.4, 0.1, 0.6, 0.5, 0.3, 0.4, 0.6, 0.5, 0.7, 0.4, 0.5, 0.6, 0.5,\n       0.5, 0.4, 0.6, 0.7, 0.5, 0.2, 0.6, 0.5, 0.3, 0.5, 0.5, 0.3, 0.4,\n       0.4, 0.2, 0.8, 0.5, 0.5, 0.7, 0.4, 0.6, 0.5, 0.6, 0.3, 0.4, 0.7,\n       0.6, 0.6, 0.4, 0.5, 0.6, 0.5, 0.5, 0.4, 0.4, 0.2, 0.4, 0.6, 0.6,\n       0.7, 0.6, 0.4, 0.3, 0.3, 0.6, 0.5, 0.4, 0.9, 0.4, 0.6, 0.6, 0.6,\n       0.4, 0.5, 0.8, 0.8, 0.5, 0.5, 0.3, 0.6, 0.5, 0.5, 0.4, 0.5, 0.7,\n       0.8, 0.7, 0.4, 0.6, 0.5, 0.4, 0.5, 0.6, 0.6, 0.5, 0.6, 0.7, 0.3,\n       0.4, 0.4, 0.5, 0.5, 0.3, 0.6, 0.2, 0.6, 0.3])"},"exec_count":10,"output_type":"execute_result"}},"pos":32,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"f1e8f6","input":"best_person = np.argmax(mu)\nbest_person, mu[best_person]","output":{"0":{"data":{"text/plain":"(60, 0.9)"},"exec_count":11,"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"97c72a","input":"flips = np.random.randint(0,2,100*10).reshape(10,100)\nmu = np.mean(flips,axis=0)\nmu[best_person]","output":{"0":{"data":{"text/plain":"0.6"},"exec_count":12,"output_type":"execute_result"}},"pos":35,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"957bdf","input":"answer=2*np.exp(-2*0.1**2*100)\nprint(\"If Hoeffding holds for the best flipper,\\n then the difference between E_in and E_out \\n will exceed epsilon at most {:0.2f}% of the time\".format(100*answer))","output":{"0":{"name":"stdout","output_type":"stream","text":"If Hoeffding holds for the best flipper,\n then the difference between E_in and E_out \n will exceed epsilon at most 27.07% of the time\n"}},"pos":38,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"3cc24e","input":"num_experiments = 10**4\nepsilon=0.1\nepsilon_exceeded = 0\nfor i in range(num_experiments):\n    flips = np.random.randint(0,2,100*1000).reshape(100,1000)\n    mu = np.mean(flips,axis=0)\n    best_person = np.argmax(mu)\n    best_record = np.max(mu)\n    diff = np.abs(best_record-0.5)\n    if diff > epsilon:\n        epsilon_exceeded +=1\nanswer = epsilon_exceeded/num_experiments                \nprint(\"The difference exceeded epsilon {:0.2f}% of the time\".format(100*answer))        \n","output":{"0":{"name":"stdout","output_type":"stream","text":"The difference exceeded epsilon 100.00% of the time\n"}},"pos":39,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"c48a95","input":"import numpy as np\n\nX = np.arange(8*3).reshape(8,3)\nprint(\"The shape of X is {}\".format(X.shape))\nprint(\"X before shuffle: \\n{}\".format(X))\ns=np.random.shuffle(X)\nprint(\"X after shuffle: \\n{}\".format(X))\ncutoff = int(0.8*X.shape[0])\nX_train = X[:cutoff,:]\nprint(\"X_train is \\n{}\".format(X_train))\nX_test = X[cutoff:,:]\nprint(\"X_test is \\n{}\".format(X_test))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"The shape of X is (8, 3)\nX before shuffle: \n[[ 0  1  2]\n [ 3  4  5]\n [ 6  7  8]\n [ 9 10 11]\n [12 13 14]\n [15 16 17]\n [18 19 20]\n [21 22 23]]\nX after shuffle: \n[[12 13 14]\n [ 9 10 11]\n [ 0  1  2]\n [15 16 17]\n [18 19 20]\n [ 6  7  8]\n [21 22 23]\n [ 3  4  5]]\nX_train is \n[[12 13 14]\n [ 9 10 11]\n [ 0  1  2]\n [15 16 17]\n [18 19 20]\n [ 6  7  8]]\nX_test is \n[[21 22 23]\n [ 3  4  5]]\n"}},"pos":45,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"c89e99","input":"y = np.random.choice([0,1],X.shape[0])\nprint(\"y = \\n{}\".format(y))\nchoices = np.random.choice(np.arange(X.shape[0]),cutoff,replace=False)\nprint(\"row choices = {}\".format(choices))\nnonchoices = np.array(list(set(np.arange(X.shape[0])).difference(set(choices))))\nprint(\"non choices = {}\".format(nonchoices))","output":{"0":{"name":"stdout","output_type":"stream","text":"y = \n[1 0 1 1 0 0 1 0]\nrow choices = [4 7 5 3 1 2]\nnon choices = [0 6]\n"}},"pos":47,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"61920a","input":"X_train = X[choices]\ny_train = y[choices]\nX_test = X[nonchoices]\ny_test = y[nonchoices]\n\nprint(\"X_train = \\n{}\".format(X_train))\nprint(\"y_train = \\n{}\".format(y_train))\nprint(\"X_test = \\n{}\".format(X_test))\nprint(\"y_test = \\n{}\".format(y_test))","output":{"0":{"name":"stdout","output_type":"stream","text":"X_train = \n[[18 19 20]\n [ 3  4  5]\n [ 6  7  8]\n [15 16 17]\n [ 9 10 11]\n [ 0  1  2]]\ny_train = \n[0 0 0 1 0 1]\nX_test = \n[[12 13 14]\n [21 22 23]]\ny_test = \n[1 1]\n"}},"pos":48,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"dc7d32","input":"numwrong = np.sum(np.sign(X.dot(w)) != y)\nnumX = X.shape[0]\nE_in = numwrong/numX\nE_in","output":{"0":{"data":{"text/plain":"0.1"},"exec_count":2,"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"54982f","input":"import numpy as np\nimport mystuff as ms\n\nnp.random.seed(101)\nX=np.array([]).reshape(0,3)\ny=np.array([])\nw = np.array([1,-2,3])\nX,y,w = ms.myblobs(N=60,sig_yes=0.7,sig_no=0.7)\nw=[0.5,7,1]\nwp = np.linalg.pinv(X).dot(y)\nms.compare_boundary(w,wp,X,y)\n#ms.lin_boundary(wp,X,y)","output":{"0":{"data":{"image/png":"8e7ddd8ec45d04b2200ab62a049c50b306985e04","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":3,"metadata":{"image/png":{"height":277,"width":384}},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"fd503b","input":"import numpy as np\nN=1000\nE_out = 0.1\nred, green = 1,0\nD = np.random.choice([red,green],N,p=[E_out,1-E_out])\nE_in = np.sum(D)/N\nprint(\"E_in = {}\".format(E_in))\nprint(\"E_out = {}\".format(E_out))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"E_in = 0.1\nE_out = 0.1\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"0ac4ee","input":"N=1000\nE_out = 0.1\nred, green = 1,0\nE_ins = []\nfor tries in range(100000):\n    D = np.random.choice([red,green],N,p=[E_out,1-E_out])\n    E_in = np.sum(D)/N\n    E_ins.append(E_in)\n\nimport matplotlib.pyplot as plt\nE_ins = np.array(E_ins)\nplt.hist(E_ins,bins=50,density=True)\nplt.title(r\"Empirically $E_{out}$ clusters around $E_{in}$\")\nplt.show()","output":{"0":{"data":{"image/png":"4c3f0c87eadeb67581d90a22c2bcdfd21f5c5bda","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":264,"width":369}},"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"242dcb","input":"import matplotlib.pyplot as plt\n\nfor N in [10,100,1000,10000]:\n\n    epsilon = np.logspace(0,-5)\n    delta = 2*np.exp(-2*epsilon**2*N)\n    plt.plot(epsilon,delta,label=\"N={}\".format(N))\nplt.xlabel(r\"$\\epsilon$\")\nplt.ylabel(r\"$\\delta$\")\nplt.legend()\nx1,x2,y1,y2 = plt.axis()\n\nplt.axis((x1,x2,y1,1))\nplt.title(r\"Confidence as a function of $\\epsilon$\")\nplt.show()","output":{"0":{"data":{"image/png":"ebc2bf2f96fdfab4a0c8a00d52f4abcdc81ed08f","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":277,"width":385}},"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"8286a2","input":"for epsilon in [0.1,0.05,0.025,0.01,0.001]:\n    N = np.logspace(4,12,base=2)\n    delta = 2*np.exp(-2*epsilon**2*N)\n    plt.plot(N,delta,label=r\"$\\epsilon= {}$\".format(epsilon))\nplt.ylabel(r\"$\\delta$\")\nplt.xlabel(r\"$N$\")\nplt.legend()\nplt.title(r\"Confidence as a function of $N$ \")\nplt.show()","output":{"0":{"data":{"image/png":"5310d7d67b202f6ea51594378bf55c8c3aab4ee3","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":7,"metadata":{"image/png":{"height":277,"width":392}},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"3dcf1e","input":"N=1000\nE_out = 0.1\nred, green = 1,0\ndifs = []\nfor tries in range(100000):\n    D = np.random.choice([red,green],N,p=[E_out,1-E_out])\n    E_in = np.sum(D)/N\n    difs.append(np.abs(E_in-E_out))\n\nimport matplotlib.pyplot as plt\ndifs = np.array(difs)\nhist=plt.hist(difs,bins=50,density=True)\n\nplt.title(r\"The distribution of $E_{out} - E_{in}$ \")\nplt.show()","output":{"0":{"data":{"image/png":"c4619eac5abed274a8d9d486bca5bc90c7fde13e","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":8,"metadata":{"image/png":{"height":264,"width":369}},"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"d47a08","input":"import numpy as np\nN = 100\nE_out = 0.1\nred, green = 1,0\n\nresults = []\nnumtries = 10000\nfor tries in range(numtries):\n    D = np.random.choice([red,green],N,p=[E_out,1-E_out])\n    E_in = np.sum(D)/N\n    results.append(E_in)\n\nE_ins = np.array(results)\nepsilon = 0.09\nexceeded = np.abs(E_ins-E_out)>epsilon\nprint(\"Difference exceeds epsilon {}% of time\".format(100*np.mean(exceeded)))\nprint(\"Hoeffding bound: at most {0:.2f}% of time\".format(200*np.exp(-2*epsilon**2*N)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Difference exceeds epsilon 0.27% of time\nHoeffding bound: at most 39.58% of time\n"}},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"084758","input":"### So what have we learned?\n\nWe have learned that in the special case $\\mathcal{H} = \\{h\\}$\n\nif $\\mathcal{D}$ is large\n\nand\n\n**if** $E_{in}(h)$ is small\n\nThen $E_{out}(h)$ is *probably* also small, \n\nand therefore when we see a new instance $\\bar{x} \\in \\mathcal{X}$ it is likely that\n\n$h(\\bar{x}) = f(\\bar{x})$.\n","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"0b166a","input":"### Feasibility of learning\n\nLet  $\\mathcal{X}=\\mathbb{Z}$ and $\\mathcal{Y} = \\{0,1\\}$.\n\nSuppose $\\mathcal{D} = \\{(1,0),(2,0),(3,0)\\}$.\n\nThat means $f(1)=f(2)=f(3)=0$.\n\nConsider $f(4)$. \n\nWhat should we guess that it is?  \n\nWhy?  How do we know that $f$ is not a very complicated rule?\n\nIf $f$ can really be anything then there is no reason to have any belief about the values of $f$ outside $\\mathcal{D}$.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"1c3122","input":"### Hoeffding is \"distribution free\"\n\nThe amazing thing about the Hoeffding inequality is that the right hand side does not depend on $P$.\n\n$$P[|E_{in}(h)-E_{out}(h)| > \\epsilon] \\leq 2e^{-2\\epsilon^2N}$$\n\nIt is true for *all* $P$, even the worst possible $P$.\n\nFor that reason, the bound is usally *very* conservative.","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"22e13c","input":"### So what have we learned?\n\nIf $E_{in}(h)$ is not small then there's really not much we can do with $h$.\n\nAnd if $\\mathcal{H}$ only has one hypothesis in it, we would really be pretty lucky if it were any good.\n\nWhat we need is some guarantee that \n\nif $\\mathcal{H}$ is big\n\nand we take the $g \\in \\mathcal{H}$ for which $E_{in}$ **is least**\n\n$$g = argmin_{h \\in \\mathcal{H}}E_{in}(h)$$\n\nthen $E_{in}(g)$ is *still* close to $E_{out}(g)$","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"276979","input":"### A crude bound\n\nWe **can** come up with a Hoeffding like bound assuming that $\\mathcal{H}$ contains $M$ hypotheses. \n\nBecause the probability that the **best** $g$ has $E_{in}$ within $\\epsilon$ of $E_{out}$\n\nis at most the probability that **some** $h$ has $E_{in}$ within $\\epsilon$ of $E_{out}$.\n\n$$P[|E_{in}(g)-E_{out}(g)| > \\epsilon] \\leq \\sum_{h \\in \\mathcal{H}} P[|E_{in}(h)-E_{out}(h)| > \\epsilon]$$\n\nBut Hoeffding **does** apply to each summand in RHS because each $h$ was picked before $\\mathcal{D}$.  Thus\n\n$$P[|E_{in}(g)-E_{out}(g)|>\\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"2e9bd8","input":"### In sample error\n\n\nWe use $\\mathcal{D}$ to compute the **in sample error**:\n\n$$E_{in}(h) = \\frac{1}{N}\\sum_{n=1}^N [\\![ h(\\bar{x}_n) \\neq f(\\bar{x}_n)]\\!]$$\n\nThis is just the proportion of $\\mathcal{D}$ for which $h$ says the same thing as $f$.\n\nFor example if $\\mathcal{D} = \\{(a,0),(b,1),(c,0)\\}$ and $h(a)=h(b)=h(c)=0$, then\n\n$$E_{in}(h) = \\frac{1}{3}(0+1+0) = \\frac{1}{3}$$","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"394170","input":"### But are we happy with that?\n\nWe are not very happy with the bound \n\n$$P[|E_{in}(g)-E_{out}(g)|>\\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n\nbecause $\\mathcal{H}$ is often very large, or even infinite.\n\nThe perceptron, for example, has an infinite hypothesis class.\n\nThis bound was improved in the 70's by Vapnik and Chervonenkis to apply\n\nto some infinite $\\mathcal{H}$.\n\nThe ones with finite VC dimension (see Ch 2).\n","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"4fd07c","input":"### Why reserve a testing set?\n\nWe saw above that a finite $\\mathcal{H}$ will have $E_{in}(g) \\approx E_{out}(g)$ for very large $N=|\\mathcal{D}|$.\n\nSo why use a test set?  \n\nThe basic reason is that $g = argmin_{h \\in \\mathcal{H}}E_{in}(h)$ can be chosen based only on the training data.  \n\nThen $g$ is chosen without looking at the testing data.  \n\nThis means that the original Hoeffding bound (not the union bound) will apply to $g$ on the test set\n\nTherefore $E_{in}^{test}(g)$ (the in-sample error of $g$ on the test set) is likely to be close to $E_{out}(g)$, provided the test set is big enough.\n\nThis is true even when $\\mathcal{H}$ is infinite!\n\nThe in-sample error of $g$ on the test set is the best measure we have of how \"good\" $g$ really is.\n\n","pos":49,"type":"cell"}
{"cell_type":"markdown","id":"51029f","input":"### Shoulda woulda coulda\n\nWe said that $E_{in}(h)$ and $E_{out}(h)$ \"should\" be close in value, and we can see from our little program (above) that they actually are.\n\nThey **might** differ by a lot if you are super unluckly.\n\nBut it is very unlikely that you will be extremely unlucky.\n\n$E_{in}(h)$ and $E_{out}(h)$ are **probably approximately equal**.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"568853","input":"### How good a hypothesis is $h$?\n\nUnfortunately, it is generally impossible to know $E_{out}(h)$, because we don't know $P$ or $f$.\n\nHowever $\\mathcal{D}$ gives us data about both $f$ and $P$.\n\nWe assume that the $\\bar{x}$ in $\\mathcal{D}$ were drawn independently according to $P$.\n\nAnd of course in supervised learning $\\mathcal{D}$ also tell us $y = f(\\bar{x})$.\n\nThe bigger $\\mathcal{D}$ is, the more we know about $f$ and $P$.\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"5a3bb4","input":"### A different question\n\nRather than tackling this tough problem head on we will try to answer an easier one.\n\n* Given a function $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$, how similar is $h$ to $f$?\n\nTo make *this* question make sense, we assume that there is some probability distribution $P$ over $\\mathcal{X}$ from which the examples in $\\mathcal{D}$ are drawn. \n\nThis corresponds to real life:  There seems to be some real probability distribution that determines the features of the people you meet, the cars you see, the trees in the forest, etc.\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"641b9e","input":"### $\\mathcal{H} = \\{h_1,h_2,\\ldots,h_m\\}$\n\nConsider a hypothesis class $\\mathcal{H}$ with $m$ hypotheses.\n\nEach $h_i$ has an out of sample error rate $E_{out}(h_i)$.\n\nEach $h_i$ also has an in sample error rate $E_{in}(h_i)$.\n\nIf $\\mathcal{H}$ is big compared to $\\mathcal{D}$, then through luck one of the $h_i$ may have $E_{in}(h_i)$ freakishly small, while\n\n$E_{in}(h_i) \\not \\approx E_{out}(h_i)$.\n\nWe might naively **think** that this hypothesis is best, because it does best on $\\mathcal{D}$.\n\nBut really the performance on $\\mathcal{D}$ was a fluke.","pos":30,"type":"cell"}
{"cell_type":"markdown","id":"68e578","input":"### BUT\n\nWe have learned that in the special case $\\mathcal{H} = \\{h\\}$\n\nif $\\mathcal{D}$ is large\n\nand\n\nif $E_{in}(h)$ is **large**\n\nThen $E_{out}(h)$ is *probably* also **large**, \n\nand therefore when we see a new instance $\\bar{x} \\in \\mathcal{X}$ it **not likely** that\n\n$h(\\bar{x}) = f(\\bar{x})$.\n","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"6aca52","input":"### Uh oh, must have been the pressure?\n\nObviously the ability to flip heads does not persist.\n\nThe best person's performance was all luck and no skill.\n\nWhen we tested the person on fresh data the luck vanished and only the skill remained.\n\nWith only skill and no luck to rely on, performance decreased.\n\nThis is called **regression to the mean**.\n\nWe have to be very careful about this in ML when evaluating model performance (and in life too).\n\nYou want the best model, not the luckiest one.","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"71c08f","input":"### Probably approximately equal\n\nThe Hoeffding bound is a mathematical formula that rigorously defines \"probably approximately equal\".\n\nIt says that, for any $\\epsilon > 0$,\n\n$$P[|E_{in}(h)-E_{out}(h)| > \\epsilon] \\leq 2e^{-2\\epsilon^2N}$$\n\nIn words:  The probability that $E_{in}(h)$ and $E_{out}(h)$ differ by more than a tiny amount $\\epsilon$ decreases exponentially as $N \\rightarrow \\infty$.\n\nBut for small $\\epsilon$, the term $\\epsilon^2$ will be **really** small and slow down the convergence.","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"79bd47","input":"### Training, Testing (and Validation)\n\nOne way to help prevent overfitting is to partition the data into a training set and a testing set.\n\n$$\\mathcal{D} = \\mathcal{D}_{train} \\cup \\mathcal{D}_{test}$$\n\nUsually the training set is about 80-85% of the data, and the testing set is 20-15% of the data.\n\nIt is important that this partition be random.\n\nIn numpy you might randomly shuffle the rows of $X$ before taking the first 80% of rows to be $\\mathcal{D}_{train}$.\n","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"833537","input":"### Feasibility of learning\n\nConsider the basic problem in supervised learning:\n\nThe learner gets to see a finite sample $\\mathcal{D} = \\{(\\bar{x}_1,y_1,\\ldots,\\bar{x}_n,y_n)\\}$\n\nfrom an unknown target function $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$.\n\nYou want to predict what $f$ would do on a new instance $\\bar{x} \\in \\mathcal{X}$ which is not in the sample.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"adeb7d","input":"### You have to urn it\n\nSuppose that every $\\bar{x} \\in \\mathcal{X}$ is a ball in an urn.\n\nWe color the ball green if $h(\\bar{x}) = f(\\bar{x})$ and red if $h(\\bar{x}) \\neq f(\\bar{x})$\n\n$E_{out}(h)$ is the probability of pulling a red ball out of the urn. \n\n$E_{in}(h)$ is the proportion of red balls you see if you actually pull $N=|\\mathcal{D}|$ of them out and check what you get. \n\nThe (frequentist) **meaning** of $E_{out}$ is just the limit of $E_{in}$ as $N \\rightarrow \\infty$.\n\nSo for big $N$ they should be close together.\n\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"b495ea","input":"### However\n\nHowever, the bound \n\n$$P[|E_{in}(g)-E_{out}(g)|>\\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n\nhas in many ways answered our original question about *how* learning can be possible.\n\nIt says that if you have any finite $\\mathcal{H}$ and enough data, then indeed $E_{in}(g) \\approx E_{out}(g)$.\n\nThis is because $e^{-2\\epsilon^2N}$ eventually dominates $M$ and makes the RHS tend to zero as $N \\rightarrow \\infty$.\n\n**If** you can **also** make $E_{in}(g)$ low with this $\\mathcal{H}$, then you have succeeded in learning $f$.\n","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"c0cc31","input":"### The Graph Above\n\nThe above graph shows $\\epsilon$ vs the Hoeffding bound on the proportion of samples that have $E_{out}$ and $E_{in}$ more than $\\epsilon$ apart.\n\nThis also depends on $N$, which is the \"size of the draw\".\n\nFor small draws you can get unlucky ($E_{out}$ far from $E_{in}$)\n\nBut this is increasingly unlikely for draws of larger size.","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"c37dce","input":"### The Graph Above\n\nThis graph shows how the increase of $N$ (the size of the draw) causes the likelihood of a \"bad\" draw to decrease.\n\nA \"bad\" draw is one in which $E_{in}$ and $E_{out}$ are far apart.\n\nBecause \"far\" depends on $\\epsilon$, you get different curves as the notion of \"far\" becomes more and more strict.\n\nNotice that for $\\epsilon=0.01$ and $\\epsilon=0.001$ the Hoeffding bound is not really a bound at all until $N$ exceeds 4000.  \n","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"cbac87","input":"### The distribution on $\\mathcal{X}$\n\nIn our cat classification exercise we imagined that $\\mathcal{X}$ is all 256x256 pixel images.\n\nWhich of those are you likely to actually see on the internet, or as a result of taking a photograph?\n\n\n$\\mathcal{D}$ gives us a sample showing which $\\bar{x}\\in \\mathcal{X}$ are likely to be encountered.  \n\nThis is what $P$ is.\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"cebcec","input":"### Luck vs skill  $(E_{in} vs E_{out}?)$\n\nEntities that are exceptional are exceptional for two reasons: luck and skill.\n\nSkill persists but luck does not.  \n\nConsider 100 people, each of whom flip a fair coin ten times.\n\nWe simulate this below.  Each column is a \"person\".\n\nThe row corresponding to a person is their flipping record.\n","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"d0bbf7","input":"### What about $y$?\n\nBut when you shuffle $X$ you want to apply the same permutation to $y$ so that the outputs and inputs continue to match up.\n\nIf $y$ has not yet been selected out from $X$ you can use the above method.\n\nIf $y$ is given separately then you can do something like the code below.\n\nThis uses `np.random.choice` to choose random rows for the training/test partition.\n","pos":46,"type":"cell"}
{"cell_type":"markdown","id":"d31c40","input":"![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkT3sktb6U0E%2Fhqdefault.jpg&f=1&nofb=1)","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"d75719","input":"### Wow, what a performance!\n\nSomeone has a real tendency to flip heads.\n\nOr do they?\n\nDo you think their luck will hold up in ten more flips?\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"da8b8b","input":"### How good is a hypothesis $h$?\n\nThe measure of how well $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$ approximates $f$ is:\n\n$$E_{out}(h) = P[h(\\bar{x}) \\neq f(\\bar{x})]$$\n\nwhere $P$ is the probability distribution over $\\mathcal{X}$ that generates examples (i.e. $\\mathcal{D}$).\n\nIn other words if we pick some $\\bar{x} \\in \\mathcal{X}$ at random (according to the distribution $P$) how likely is it that $f$ and $h$ agree on this instance?\n\nThis is **out of sample error** and it is the ultimate measure of the quality of a hypothesis. ","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"de3cd7","input":"### The distribution on $\\mathcal{X}$\n\nSuppose that $\\mathcal{X}$ are the features of frogs: length, weight, tongue length, color\n\nAll of these features could take on any value in principle.\n\nBut which ones are you likely to actually see?\n\nThose are the ones in $\\mathcal{D}$. ","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"ded183","input":"### The central dilemma...\n\nThis brings us to the main dilemma in machine learning...\n\n1. If you make $\\mathcal{H}$ complex enough relative to $f$, you can make $E_{in}(g)$ low.\n2. But the more complex you make $\\mathcal{H}$, the bigger $\\mathcal{D}$ must be to ensure $E_{out}(g) \\approx E_{in}(g)$.\n\nYou should pick the most complex $\\mathcal{H}$ you can \"afford\" given the size of your training data.\n\nIf you make $\\mathcal{H}$ too complex for the data then you will get the luckiest hypothesis, not the best.\n\nThis is called **overfitting**.","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"dee7ef","input":"### The image above\n\nThis image shows $h$ vs $f$ for made up data.\n\nThe boundary with positive slope is $f$.\n\nThe yellow and purple regions are where $h$ and $f$ disagree.\n\nThe probability of a dot landing here is $E_{out}(h) = \\mathbb{P}_{\\bar{x}\\in \\mathcal{X}}[h(\\bar{x})\\neq f(\\bar{x})]$\n\nThe proportion of dots actually in the purple or yellow region is $E_{in}(h)$\n\nWhen $h$ is picked **before** the data it is likely that these values are close.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"e8710c","input":"### $E_{in} \\approx E_{out}$?\n\nSo, to summarize we know $E_{in}(h)$ but we want to know $E_{out}(h)$.  \n\nBut measuring $E_{out}(h)$ is generally impossible. \n\nBut because of probability theory $E_{in}(h)$ and $E_{out}(h)$ must be pretty close.\n\nWhy is this true?","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"e957b8","input":"### Hoeffding?\n\nIf we pick the best $g \\in \\mathcal{H}$ with respect to $E_{in}$ then the Hoeffding bound does not apply.  \n\nTo show this you could check the percentage of time that the best of 1000 coin flippers (in 100 flips) has a sample average different from 0.5 by more than $\\epsilon = 0.1$.\n\nWe compute the Hoeffding bound below. \n\nAfter that we perform an experiment to see how often epsilon is actually exceeded by the best flipper.","pos":37,"type":"cell"}
{"cell_type":"markdown","id":"eb95d4","input":"### Too much!\n\nBut that's too much to ask for **when $\\mathcal{H}$ can be anything**.  \n\nBecause if $\\mathcal{H}$ is big enough then any function is in there.\n\nThere may be the function $h$ that behaves just like $f$ on $\\mathcal{D}$ but just the opposite of $f$ for examples not in $\\mathcal{D}$.\n\nThen $E_{in}(h) = 0$ but $E_{out}(h)=1$!\n\nIn other words $E_{in}$ and $E_{out}$ are as far apart as possible.\n\nThis tells us that we must consider only a special kind of \"nice\" $\\mathcal{H}$ (described in Ch 2).\n\nFiniteness is a special kind of \"niceness\"\n\nLet's consider what happens when $\\mathcal{H}$ is finite.","pos":29,"type":"cell"}
{"id":0,"time":1582919182220,"type":"user"}
{"last_load":1582919182938,"type":"file"}