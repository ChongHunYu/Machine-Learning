{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":80105472},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"type":"settings"}
{"cell_type":"code","exec_count":13,"id":"bdcb1b","input":"num_experiments = 10**4\nepsilon=0.1\nepsilon_exceeded = 0\nfor i in range(num_experiments):\n    flips = np.random.randint(0,2,100*1000).reshape(100,1000)\n    mu = np.mean(flips,axis=0)\n    best_person = np.argmax(mu)\n    best_record = np.max(mu)\n    diff = np.abs(best_record-0.5)\n    if diff > epsilon:\n        epsilon_exceeded +=1\nanswer = epsilon_exceeded/num_experiments                \nprint(\"The difference exceeded epsilon {:0.2f}% of the time\".format(100*answer))        \n","output":{"0":{"name":"stdout","output_type":"stream","text":"The difference exceeded epsilon 100.00% of the time\n"}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"53d555","input":"answer=2*np.exp(-2*0.1**2*100)\nprint(\"If Hoeffding holds for the best flipper,\\n then the difference between E_in and E_out \\n will exceed epsilon at most {:0.2f}% of the time\".format(100*answer))","output":{"0":{"name":"stdout","output_type":"stream","text":"If Hoeffding holds for the best flipper,\n then the difference between E_in and E_out \n will exceed epsilon at most 27.07% of the time\n"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"1cffd6","input":"import numpy as np\nN = 100\nE_out = 0.1\nred, green = 1,0\n\nresults = []\nnumtries = 10000\nfor tries in range(numtries):\n    D = np.random.choice([red,green],N,p=[E_out,1-E_out])\n    E_in = np.sum(D)/N\n    results.append(E_in)\n\nE_ins = np.array(results)\nepsilon = 0.09\nexceeded = np.abs(E_ins-E_out)>epsilon\nprint(\"Difference exceeds epsilon {}% of time\".format(100*np.mean(exceeded)))\nprint(\"Hoeffding bound: at most {0:.2f}% of time\".format(200*np.exp(-2*epsilon**2*N)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Difference exceeds epsilon 0.19% of time\nHoeffding bound: at most 39.58% of time\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"61e9a8","input":"import numpy as np\n## |D|=10, |H|=100\nflips = np.random.randint(0,2,100*10).reshape(10,100)\n\nmu = np.mean(flips,axis=0)\nmu","output":{"0":{"data":{"text/plain":"array([0.5, 0.6, 0.5, 0.2, 0.8, 0.6, 0.7, 0.7, 0.7, 0.5, 0.8, 0.3, 0.5,\n       0.6, 0.7, 0.6, 0.7, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.6, 0.6,\n       0.5, 0.4, 0.8, 0.4, 0.6, 0.7, 0.5, 0.3, 0.6, 0.5, 0.2, 0.2, 0.4,\n       0.7, 0.6, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0. , 0.4, 0.4, 0.5, 0.2,\n       0.6, 0.7, 0.6, 0.7, 0.5, 0.4, 0.6, 0.6, 0.5, 0.6, 0.2, 0.3, 0.7,\n       0.6, 0.7, 0.7, 0.4, 0.4, 0.5, 0.8, 0.6, 0.5, 0.4, 0.5, 0.6, 0.4,\n       0.4, 0.6, 0.1, 0.7, 0.8, 0.6, 0.7, 0.5, 0.7, 0.5, 0.4, 0.4, 0.7,\n       0.7, 0.3, 0.6, 0.4, 0.5, 0.3, 0.5, 0.4, 0.4])"},"exec_count":3,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"767895","input":"best_person = np.argmax(mu)\nbest_person, mu[best_person]","output":{"0":{"data":{"text/plain":"(4, 0.8)"},"exec_count":4,"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"8bedbc","input":"import numpy as np\nN=100000\nE_out = 0.1\nred, green = 1,0\nD = np.random.choice([red,green],N,p=[E_out,1-E_out])\nE_in = np.sum(D)/N\nprint(\"E_in = {}\".format(E_in))\nprint(\"E_out = {}\".format(E_out))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"E_in = 0.10075\nE_out = 0.1\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"6afd0d","input":"flips = np.random.randint(0,2,100*10).reshape(10,100)\nmu = np.mean(flips,axis=0)\nmu[best_person]","output":{"0":{"data":{"text/plain":"0.6"},"exec_count":5,"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":66,"id":"8f1a38","input":"for epsilon in [0.1,0.05,0.025,0.01,0.001]:\n    N = np.logspace(4,12,base=2)\n    delta = 2*np.exp(-2*epsilon**2*N)\n    plt.plot(N,delta,label=r\"$\\epsilon= {}$\".format(epsilon))\nplt.ylabel(r\"$\\delta$\")\nplt.xlabel(r\"$N$\")\nplt.legend()\nplt.title(r\"Confidence as a function of $N$ \")\nplt.show()","output":{"0":{"data":{"image/png":"3104a65d94303f1b0e40cf31e630541dbeff4081","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":69,"id":"9fc96c","input":"import matplotlib.pyplot as plt\n\nfor N in [10,100,1000,10000]:\n\n    epsilon = np.logspace(0,-5)\n    delta = 2*np.exp(-2*epsilon**2*N)\n    plt.plot(epsilon,delta,label=\"N={}\".format(N))\nplt.xlabel(r\"$\\epsilon$\")\nplt.ylabel(r\"$\\delta$\")\nplt.legend()\nx1,x2,y1,y2 = plt.axis()\n\nplt.axis((x1,x2,y1,1))\nplt.title(r\"Confidence as a function of $\\epsilon$\")\nplt.show()","output":{"0":{"data":{"image/png":"22a34ae61039a2ceafe0e606f40d2538a1788e33","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":14,"type":"cell"}
{"cell_type":"code","id":"de4266","input":"","pos":33,"type":"cell"}
{"cell_type":"markdown","id":"09271f","input":"### Hoeffding is \"distribution free\"\n\nThe amazing thing about the Hoeffding inequality is that the right hand side does not depend on $P$.\n\n$$P[|E_{in}(h)-E_{out}(h)| > \\epsilon] \\leq 2e^{-2\\epsilon^2N}$$\n\nIt is true for *all* $P$, even the worst possible $P$.\n\nFor that reason, the bound is usally *very* conservative.","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"098245","input":"### Probably approximately equal\n\nThe Hoeffding bound is a mathematical formula that rigorously defines \"probably approximately equal\".\n\nIt says that, for any $\\epsilon > 0$,\n\n$$P[|E_{in}(h)-E_{out}(h)| > \\epsilon] \\leq 2e^{-2\\epsilon^2N}$$\n\nIn words:  The probability that $E_{in}(h)$ and $E_{out}(h)$ differ by more than a tiny amount $\\epsilon$ decreases exponentially as $N \\rightarrow \\infty$.\n\nBut for small $\\epsilon$, the term $\\epsilon^2$ will be **really** small and slow down the convergence.","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"1e9098","input":"### Hoeffding?\n\nIf we pick the best $g \\in \\mathcal{H}$ with respect to $E_{in}$ then the Hoeffding bound does not apply.  \n\nTo show this you could check the percentage of time that the best of 1000 coin flippers (in 100 flips) has a sample average different from 0.5 by more than $\\epsilon = 0.1$.\n\nWe compute the Hoeffding bound below. \n\nAfter that we perform an experiment to see how often epsilon is actually exceeded by the best flipper.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"1ebb57","input":"### The distribution on $\\mathcal{X}$\n\nIn our cat classification exercise we imagined that $\\mathcal{X}$ is all 256x256 pixel images.\n\nWhich of those are you likely to actually see on the internet, or as a result of taking a photograph?\n\n\n$\\mathcal{D}$ gives us a sample showing which $\\bar{x}\\in \\mathcal{X}$ are likely to be encountered.  \n\nThis is what $P$ is.\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"2ee908","input":"### But are we happy with that?\n\nWe are not very happy with the bound \n\n$$P[|E_{in}(g)-E_{out}(g)|>\\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n\nbecause $\\mathcal{H}$ is often very large, or even infinite.\n\nThe perceptron, for example, has an infinite hypothesis class.\n\nThis bound was improved in the 70's by Vapnik and Chervonenkis to apply\n\nto some infinite $\\mathcal{H}$.\n\nThe ones with finite VC dimension (see Ch 2).\n","pos":32,"type":"cell"}
{"cell_type":"markdown","id":"33baaa","input":"### $E_{in} \\approx E_{out}$?\n\nSo, to summarize we know $E_{in}(h)$ but we want to know $E_{out}(h)$.  \n\nBut measuring $E_{out}(h)$ is generally impossible. \n\nBut because of probability theory $E_{in}(h)$ and $E_{out}(h)$ must be pretty close.\n\nWhy is this true?","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"3be596","input":"### Uh oh, must have been the pressure?\n\nObviously no one is really any better than anyone else at flipping coins.\n\nThe best person's performance was all luck and no skill.\n\nWhen we tested the person on fresh data the luck vanished and only the skill remained.\n\nWith only skill and no luck to rely on, performance decreased.\n\nThis is called **regression to the mean**.\n\nWe have to be very careful about this in ML when evaluating model performance.","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"402c6c","input":"### $\\mathcal{H} = \\{h_1,h_2,\\ldots,h_m\\}$\n\nConsider a hypothesis class $\\mathcal{H}$ with $m$ hypotheses.\n\nEach $h_i$ has an out of sample error rate $E_{out}(h_i)$.\n\nThis is like an urn of red and green marbles with a certain percentage of reds.\n\nEach $h_i$ also has an in sample error rate $E_{in}(h_i)$.\n\nThis is like a sample of 10 marbles from its urn, which gives an estimate of the true percentage of reds.\n\nIf $\\mathcal{H}$ is big compared to $\\mathcal{D}$, then through luck one of the $h_i$ may have $E_{in}(h_i) \\not \\approx E_{out}(h_i)$.","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"4abfdf","input":"### So what have we learned?\n\nBut if $E_{in}(h)$ is not small then there's really not much we can say.\n\nAnd if $\\mathcal{H}$ only has one hypothesis in it, we would really be pretty lucky if it were any good.\n\nWhat we need is some guarantee that \n\nif $\\mathcal{H}$ is big\n\nand we take the $h \\in \\mathcal{H}$ for which $E_{in}(h)$ is least \n\nthen $E_{in}(h)$ is *still* close to $E_{out}(h)$","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"62ae30","input":"### You have to urn it\n\nSuppose that every $\\bar{x} \\in \\mathcal{X}$ is a ball in an urn.\n\nWe color the ball green if $h(\\bar{x}) = f(\\bar{x})$ and red if $h(\\bar{x}) \\neq f(\\bar{x})$\n\n$E_{out}(h)$ is the probability of pulling a red ball out of the urn. \n\n$E_{in}(h)$ is the proportion of red balls you see if you actually pull $N=|\\mathcal{D}|$ of them out and check what you get. \n\nArguably the **meaning** of $E_{out}$ is just the limit of $E_{in}$ as $N \\rightarrow \\infty$.\n\nSo for big $N$ they should be close together.\n\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"6c00dd","input":"### A crude bound\n\nWe **can** come up with a Hoeffding like bound assuming that $\\mathcal{H}$ contains $M$ hypotheses. \n\nBecause the probability that the **best** $g$ has $E_{in}$ within $\\epsilon$ of $E_{out}$\n\nis at most the probability that **some** $h$ has $E_{in}$ within $\\epsilon$ of $E_{out}$.\n\n$$P[|E_{in}(g)-E_{out}(g)| > \\epsilon] \\leq \\sum_{h \\in \\mathcal{H}} P[|E_{in}(h)-E_{out}(h)| > \\epsilon]$$\n\nBut Hoeffding **does** apply to each summand in RHS because each $h$ was picked before $\\mathcal{D}$.  Thus\n\n$$P[|E_{in}(g)-E_{out}(g)|>\\epsilon] \\leq 2Me^{-2\\epsilon^2N}$$\n","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"7cf04c","input":"### How good is a hypothesis $h$?\n\nThe measure of how well $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$ approximates $f$ is:\n\n$$E_{out}(h) = P[h(\\bar{x}) \\neq f(\\bar{x})]$$\n\nwhere $P$ is the probability distribution over $\\mathcal{X}$ that generates examples (i.e. $\\mathcal{D}$).\n\nIn other words if we pick some $\\bar{x} \\in \\mathcal{X}$ at random (according to the distribution $P$) how likely is it that $f$ and $h$ agree on this instance?\n\nThis is **out of sample error** and it is the ultimate measure of the quality of a hypothesis. ","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"83f57e","input":"### Too much!\n\nBut that's too much to ask for.  \n\nBecause if $\\mathcal{H}$ is big enough then any function is in there.\n\nSo consider the function $h$ that behaves just like $f$ on $\\mathcal{D}$ but just the opposite of $f$ for examples not in $\\mathcal{D}$.\n\nThen $E_{in}(h) = 0$ but $E_{out}(h)=1$!\n\nIn other words they are as far apart as possible.\n\nThis tells us that we must consider only a special kind of $\\mathcal{H}$ (described in Ch 2).\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"84efc2","input":"### So what have we learned?\n\nWe have learned that in the special case $\\mathcal{H} = \\{h\\}$\n\nif $N = \\mathcal{D}$ is large\n\nand\n\nif $E_{in}(h)$ is small\n\nThen $E_{out}(h)$ is *probably* also small, \n\nand therefore when we see a new instance $\\bar{x} \\in \\mathcal{X}$ it is likely that\n\n$h(\\bar{x}) = f(\\bar{x})$.\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"850b1f","input":"### In sample error\n\n\nWe use $\\mathcal{D}$ to compute the **in sample error**:\n\n$$E_{in}(h) = \\frac{1}{N}\\sum_{n=1}^N [\\![ h(\\bar{x}_n) \\neq f(\\bar{x}_n)]\\!]$$\n\nThis is just the proportion of $\\mathcal{D}$ for which $h$ says the same thing as $f$.\n\nFor example if $\\mathcal{D} = \\{(a,0),(b,1),(c,0)\\}$ and $h(a)=h(b)=h(c)=0$, then\n\n$$E_{in}(h) = \\frac{1}{3}(0+1+0) = \\frac{1}{3}$$","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"86194e","input":"### Feasibility of learning\n\nLet  $\\mathcal{X}=\\mathbb{Z}$ and $\\mathcal{Y} = \\{0,1\\}$.\n\nSuppose $\\mathcal{D} = \\{(1,0),(2,0),(3,0)\\}$.\n\nThat means $f(1)=f(2)=f(3)=0$.\n\nConsider $f(4)$. \n\nWhat should we guess that it is?  \n\nWhy?  How do we know that $f$ is not a very complicated rule?\n\nIf $f$ can really be anything then there is no reason to have any belief about the values of $f$ outside $\\mathcal{D}$.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"8880d1","input":"### The distribution on $\\mathcal{X}$\n\nSuppose that $\\mathcal{X}$ are the features of frogs: length, weight, tongue length, color\n\nAll of these features could take on any value in principle.\n\nBut which ones are you likely to actually see?\n\nThose are the ones in $\\mathcal{D}$. ","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"a16a66","input":"![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkT3sktb6U0E%2Fhqdefault.jpg&f=1&nofb=1)","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"ad1898","input":"### Shoulda woulda coulda\n\nWe said that $E_{in}(h)$ and $E_{out}(h)$ \"should\" be close in value, and we can see from our little program (above) that they actually are.\n\nThey **might** differ by a lot if you are super unluckly.\n\nBut it is very unlikely that you will be extremely unlucky.\n\n$E_{in}(h)$ and $E_{out}(h)$ are **probably approximately equal**.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"beb40c","input":"### Feasibility of learning\n\nConsider the basic problem in supervised learning:\n\nThe learner gets to see a finite sample $\\mathcal{D} = \\{(\\bar{x}_1,y_1,\\ldots,\\bar{x}_n,y_n)\\}$\n\nfrom an unknown target function $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$.\n\nYou want to predict what $f$ would do on a new instance $\\bar{x} \\in \\mathcal{X}$ which is not in the sample.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"c17ae7","input":"### How good a hypothesis is $h$?\n\nUnfortunately, it is generally impossible to know $E_{out}(h)$, because we don't know $P$ or $f$.\n\nHowever $\\mathcal{D}$ gives us data about both $f$ and $P$.\n\nWe assume that the $\\bar{x}$ in $\\mathcal{D}$ were drawn independently according to $P$.\n\nAnd of course in supervised learning $\\mathcal{D}$ also tell us $y = f(\\bar{x})$.\n\nThe bigger $\\mathcal{D}$ is, the more we know about $f$ and $P$.\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"d9a665","input":"### Luck vs skill  $(E_{in} vs E_{out}?)$\n\nEntities that are exceptional are exceptional for two reasons: luck and skill.\n\nSkill persists but luck does not.  \n\nConsider 100 people, each of whom flip a fair coin ten times.\n\nWe simulate this below.  Each column is a \"person\".\n\nThe row corresponding to a person is their flipping record.\n","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"e049e2","input":"### Wow, what a performance!\n\nSomeone has a real tendency to flip heads.\n\nOr do they?\n\nDo you think their luck will hold up in ten more flips?\n","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"e0cc8a","input":"### A different question\n\nRather than tackling this tough problem head on we will try to answer an easier one.\n\n* Given a function $h:\\mathcal{X} \\rightarrow \\mathcal{Y}$, how similar is $h$ to $f$?\n\nTo make *this* question make sense, we assume that there is some probability distribution $P$ over $\\mathcal{X}$ from which the examples in $\\mathcal{D}$ are drawn. \n\nThis corresponds to real life:  There seems to be some real probability distribution that determines the features of the people you meet, the cars you see, the trees in the forest, etc.\n","pos":2,"type":"cell"}
{"last_load":1581612647159,"type":"file"}