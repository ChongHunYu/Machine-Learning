{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss (or Cost) and Noise\n",
    "\n",
    "### Loss (or Cost) \n",
    "Not all mistakes are equally bad. \n",
    "\n",
    "In binary classification, any hypothesis can be wrong in two ways:\n",
    "\n",
    "1. False negative: $h(\\bar{x}) = -1$ but $f(\\bar{x}) = 1$\n",
    "2. False positive: $h(\\bar{x}) = 1$ but $f(\\bar{x}) = -1$\n",
    "\n",
    "In medical diagnoses, for example, false negatives are tragic while false positives are inconvenient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Notation\n",
    "\n",
    "$$Error = E(h,f)$$\n",
    "\n",
    "is usually based on a pointwise error measure $e(h(\\bar{x},f(\\bar{x}))$.\n",
    "\n",
    "Then $E(h,f)$ is the average of $e(h(\\bar{x}),f(\\bar{x}))$ over all $\\bar{x} \\in \\mathcal{X}$.\n",
    "\n",
    "$$E(h,f) = \\sum_{\\bar{x}\\in \\mathcal{X}}e(h(\\bar{x}),f(\\bar{x}))\\cdot P(\\bar{x}) = \\mathbb{E}[e(h(\\bar{x}),f(\\bar{x}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Simple classification error\n",
    "\n",
    "If $e(h(\\bar{x}),f(\\bar{x})) = [\\![h(\\bar{x}) \\neq f(\\bar{x})]\\!]$\n",
    "\n",
    "then\n",
    "\n",
    "$$E(h,f) =\\sum_{\\bar{x}\\in\\mathcal{X}} [\\![h(\\bar{x}) \\neq f(\\bar{x})]\\!]\\cdot P(\\bar{x}) =\\sum_{\\bar{x}\\in\\mathcal{X}\\,\\, h(\\bar{x}) \\neq f(\\bar{x})}P(\\bar{x}) = P[h(\\bar{x}) \\neq f(\\bar{x})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Simple regression error\n",
    "\n",
    "A common error measure in regression is squared difference:\n",
    "\n",
    "$$e(h(\\bar{x}),f(\\bar{x})) = (h(\\bar{x})-f(\\bar{x}))^2$$\n",
    "\n",
    "[least squares](https://en.wikipedia.org/wiki/Linear_regression#/media/File:Linear_least_squares_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Applying $E(h,f)$\n",
    "\n",
    "With a general error measure $E(h,f)$, the \"best\" hypothesis is\n",
    "\n",
    "$$g = argmin_{h \\in \\mathcal{H}} E(h,f)$$\n",
    "\n",
    "Remember that we actually have to compute this $g$, for instance by finding the best weights for the perceptron.  \n",
    "\n",
    "Different choices of $e(h(\\bar{x}),f(\\bar{x}))$ may make this optimization problem harder or easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Noisy targets\n",
    "\n",
    "So far we have spoken of $f$ as if it is a deterministic function of $\\bar{x}$.\n",
    "\n",
    "But in real life two instances might be virtually indiscernible but their outcomes different.\n",
    "\n",
    "Consider two loan applicants who look the same \"on paper\" but one defaults and the other repays the loan. \n",
    "\n",
    "This might be because of random problems that one borrower was unlucky to encounter, or random advantages that one borrower was lucky to acquire.\n",
    "\n",
    "We call this lack of determinism in $f$ \"noise\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A more general target\n",
    "\n",
    "Rather than thinking of $f$ as a deterministic function, we might think of it as a conditional probability distribution.\n",
    "\n",
    "$$P(y | \\bar{x})$$\n",
    "\n",
    "For example is $\\bar{x}$ is some vector of features describing an email, there is a 30% chance that the user will consider it spam, and a 70% chance that the user will consider it ham.\n",
    "\n",
    "Then rather than having $f(\\bar{x})=1$ or $f(\\bar{x}) = -1$, we have\n",
    "\n",
    "$$P(y=1|\\bar{x}) = 0.7,$$ and $$P(y=-1|\\bar{x}) = 0.3$$\n",
    " \n",
    "In this case our learning algorithm will give us a probability rather than a hard classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Noisy regression\n",
    "\n",
    "Noise occurs in regression problems too.  \n",
    "\n",
    "Consider trying to predict the adult height of a child given the parents' DNA.\n",
    "\n",
    "There is a distribution of possible outcomes (probably normal). \n",
    "\n",
    "Thus $P(y | \\bar{x})$ would be a normal distribution depending on $\\bar{x}$ (the parent genomes).\n",
    "\n",
    "You want to learn this conditional distribution from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Is f a conditional distribution?  Yes!\n",
    "\n",
    "We lose nothing by this new noisy view of the target.  \n",
    "\n",
    "Even a deterministic $f$ can be considered a conditional distribution:\n",
    "\n",
    "$$P(y=1|\\bar{x}) = \\begin{cases} 1\\text{ if }f(\\bar{x})=1\\\\\n",
    "                                 0\\text{ if }f(\\bar{x})\\neq 1\n",
    "                   \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### $P(y | \\bar{x})$ vs $P(\\bar{x})$\n",
    "\n",
    "$P(y | \\bar{x})$ is what we want to learn when we use discriminative models like classifiers, \n",
    "\n",
    "while $P(\\bar{x})$ is just the relative importance of $\\bar{x}$ in guaging how well we have learned.\n",
    "\n",
    "We imagine that labeled instances are generated by the joint distribution $P(\\bar{x},y)$,\n",
    "\n",
    "which determines both $P(\\bar{x}) = \\sum_{y} P(\\bar{x},y)$ and $P(y | \\bar{x}) = \\frac{P(\\bar{x},y)}{P(\\bar{x})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generative models\n",
    "\n",
    "Some ML approaches try to learn $P(\\bar{x},y)$ rather than \n",
    "\n",
    "the relatively easier $P(y | \\bar{x})$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Generative_model\n",
    "\n",
    "A *discriminative model* learns $P(y | \\bar{x})$ while\n",
    "\n",
    "a *generative model* learns $P(\\bar{x},y)$.\n",
    "\n",
    "A discriminative model will be able to identify spam, but\n",
    "\n",
    "a generative model will be able to *make its own* synthetic spam.\n",
    "\n",
    "\n",
    "A discriminative facial recognition model can classify faces, but\n",
    "\n",
    "a generative facial recognition model can generate synthetic faces.  (Deepfakes, pictures of no-one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Noisy Hoeffding\n",
    "\n",
    "The Hoeffding bound applies to noisy target functions as well as deterministic ones,\n",
    "\n",
    "intuitively because the Hoeffding bound applies to all random realizations of $P(y | \\bar{x})$.\n",
    "\n",
    "We can still have $E_{in}$ close to $E_{out}$.\n",
    "\n",
    "But for noisy targets $E_{in}$ is probably bigger than for noiseless targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}