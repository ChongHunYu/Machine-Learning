{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83415040},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"1746dd","input":"","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"0d1dc9","input":"### Simple regression error\n\nA common error measure in regression is squared difference:\n\n$$e(h(\\bar{x}),f(\\bar{x})) = (h(\\bar{x})-f(\\bar{x}))^2$$\n\n[least squares](https://en.wikipedia.org/wiki/Linear_regression#/media/File:Linear_least_squares_example2.png)","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"1f1953","input":"### A more general target\n\nRather than thinking of $f$ as a deterministic function, we might think of it as a conditional probability distribution.\n\n$$P(y | \\bar{x})$$\n\nFor example is $\\bar{x}$ is some vector of features describing an email, there is a 30% chance that the user will consider it spam, and a 70% chance that the user will consider it ham.\n\nThen rather than having $f(\\bar{x})=1$ or $f(\\bar{x}) = -1$, we have\n\n$$P(y=1|\\bar{x}) = 0.7,$$ and $$P(y=-1|\\bar{x}) = 0.3$$\n \nIn this case our learning algorithm will give us a probability rather than a hard classification.\n\n\n","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"26c4ca","input":"### Noisy regression\n\nNoise occurs in regression problems too.  \n\nConsider trying to predict the adult height of a child given the parents' DNA.\n\nThere is a distribution of possible outcomes (probably normal). \n\nThus $P(y | \\bar{x})$ would be a normal distribution depending on $\\bar{x}$ (the parent genomes).\n\nYou want to learn this conditional distribution from the data.","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"39cc7b","input":"### Is f a conditional distribution?  Yes!\n\nWe lose nothing by this new noisy view of the target.  \n\nEven a deterministic $f$ can be considered a conditional distribution:\n\n$$P(y=1|\\bar{x}) = \\begin{cases} 1\\text{ if }f(\\bar{x})=1\\\\\n                                 0\\text{ if }f(\\bar{x})\\neq 1\n                   \\end{cases}$$","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"3fbcc8","input":"### Noisy targets\n\nSo far we have spoken of $f$ as if it is a deterministic function of $\\bar{x}$.\n\nBut in real life two instances might be virtually indiscernible but their outcomes different.\n\nConsider two loan applicants who look the same \"on paper\" but one defaults and the other repays the loan. \n\nThis might be because of random problems that one borrower was unlucky to encounter, or random advantages that one borrower was lucky to acquire.\n\nWe call this lack of determinism in $f$ \"noise\".","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"5d7c0b","input":"### Notation\n\n$$Error = E(h,f)$$\n\nis usually based on a pointwise error measure $e(h(\\bar{x},f(\\bar{x}))$.\n\nThen $E(h,f)$ is the average of $e(h(\\bar{x}),f(\\bar{x}))$ over all $\\bar{x} \\in \\mathcal{X}$.\n\n$$E(h,f) = \\sum_{\\bar{x}\\in \\mathcal{X}}e(h(\\bar{x}),f(\\bar{x}))\\cdot P(\\bar{x}) = \\mathbb{E}[e(h(\\bar{x}),f(\\bar{x}))]$$","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"9ae08d","input":"### Applying $E(h,f)$\n\nWith a general error measure $E(h,f)$, the \"best\" hypothesis is\n\n$$g = argmin_{h \\in \\mathcal{H}} E(h,f)$$\n\nRemember that we actually have to compute this $g$, for instance by finding the best weights for the perceptron.  \n\nDifferent choices of $e(h(\\bar{x}),f(\\bar{x}))$ may make this optimization problem harder or easier.","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"a048a2","input":"### $P(y | \\bar{x})$ vs $P(\\bar{x})$\n\n$P(y | \\bar{x})$ is what we want to learn when we use discriminative models like classifiers, \n\nwhile $P(\\bar{x})$ is just the relative importance of $\\bar{x}$ in guaging how well we have learned.\n\nWe imagine that labeled instances are generated by the joint distribution $P(\\bar{x},y)$,\n\nwhich determines both $P(\\bar{x}) = \\sum_{y} P(\\bar{x},y)$ and $P(y | \\bar{x}) = \\frac{P(\\bar{x},y)}{P(\\bar{x})}$","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"b687fc","input":"## Loss (or Cost) and Noise\n\n### Loss (or Cost) \nNot all mistakes are equally bad. \n\nIn binary classification, any hypothesis can be wrong in two ways:\n\n1. False negative: $h(\\bar{x}) = -1$ but $f(\\bar{x}) = 1$\n2. False positive: $h(\\bar{x}) = 1$ but $f(\\bar{x}) = -1$\n\nIn medical diagnoses, for example, false negatives are tragic while false positives are inconvenient. \n\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"c95641","input":"### Generative models\n\nSome ML approaches try to learn $P(\\bar{x},y)$ rather than \n\nthe relatively easier $P(y | \\bar{x})$\n\nhttps://en.wikipedia.org/wiki/Generative_model\n\nA *discriminative model* learns $P(y | \\bar{x})$ while\n\na *generative model* learns $P(\\bar{x},y)$.\n\nA discriminative model will be able to identify spam, but\n\na generative model will be able to *make its own* synthetic spam.\n\n\nA discriminative facial recognition model can classify faces, but\n\na generative facial recognition model can generate synthetic faces.  (Deepfakes, pictures of no-one)","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"f66548","input":"### Simple classification error\n\nIf $e(h(\\bar{x}),f(\\bar{x})) = [\\![h(\\bar{x}) \\neq f(\\bar{x})]\\!]$\n\nthen\n\n$$E(h,f) =\\sum_{\\bar{x}\\in\\mathcal{X}} [\\![h(\\bar{x}) \\neq f(\\bar{x})]\\!]\\cdot P(\\bar{x}) =\\sum_{\\bar{x}\\in\\mathcal{X}\\,\\, h(\\bar{x}) \\neq f(\\bar{x})}P(\\bar{x}) = P[h(\\bar{x}) \\neq f(\\bar{x})]$$","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"fd83bd","input":"### Noisy Hoeffding\n\nThe Hoeffding bound applies to noisy target functions as well as deterministic ones,\n\nintuitively because the Hoeffding bound applies to all random realizations of $P(y | \\bar{x})$.\n\nWe can still have $E_{in}$ close to $E_{out}$.\n\nBut for noisy targets $E_{in}$ is probably bigger than for noiseless targets.","pos":11,"type":"cell"}
{"id":0,"time":1589853054210,"type":"user"}
{"last_load":1589853054255,"type":"file"}