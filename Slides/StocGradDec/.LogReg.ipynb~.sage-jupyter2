{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":83308544},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"45fda6","input":"wsgd = np.random.randn(X_train.shape[1])\nwsgd,path = sgd(wsgd,X_train,y_train,lr_pw_grad,eta=0.01,num_epochs=10)\nw,path = ms.grad_descent(w,X_train,y_train,ms.fast_grad_lr,eta=0.1,max_iter=100)\nprint(\"E_in = {}\".format(E_in(X_test,y_test,w)))\n","pos":27,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7c260b","input":"errors = [E_lr(ww,X_train,y_train) for ww in path]\nplt.plot(np.arange(len(errors)),errors)\nplt.title(\"Error as gradient descent progresses\")\nplt.show()","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8ec439","input":"","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"abed28","input":"print(\"E_in = {}\".format(E_in(X_test,y_test,wsgd)))\n","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"dc5a44","input":"wsgd = np.random.randn(X_train.shape[1])\nwsgd,path = sgd(wsgd,X_train,y_train,lr_pw_grad,eta=0.01,num_epochs=250)","pos":24,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"29f8d2","input":"mc = 2*(sigmoid(X.dot(w)) > 0.5)-1 !=y\nmc","output":{"0":{"data":{"text/plain":"array([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False])"},"exec_count":10,"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"29a37d","input":"from boundary import softboundary\nplt=softboundary()\nplt.show()","output":{"0":{"data":{"image/png":"d973914078e8fa64e5ee756744ef1847768b4526","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":11,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"6e7d2e","input":"from sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nprint(X.shape)\n### Visualize data\n\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(X, y))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: {}'.format(label))\nfor ax, (image, label) in zip(axes[1, :], images_and_labels[4:8]):\n    ax.set_axis_off()\n    ax.imshow(image.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: {}'.format(label))    ","output":{"0":{"name":"stdout","output_type":"stream","text":"(70000, 784)\n"},"1":{"data":{"image/png":"11204dafc0bef92d8c2fb9eff2ab9d0f761100d3","text/plain":"<Figure size 432x288 with 8 Axes>"},"exec_count":14,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"923529","input":"import matplotlib.pyplot as plt\nimport numpy as np\n\nxx = np.linspace(-7,7)\nyy = np.exp(xx)/(1+np.exp(xx))\n\nplt.plot(xx,yy)\nplt.title(\"The logistic, or sigmoid function\")\nplt.show()","output":{"0":{"data":{"image/png":"3f34d1e3633236666f92b80dd1e335d1594cc9fd","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":2,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":1,"type":"cell"}
{"cell_type":"code","exec_count":2,"id":"a4d395","input":"def sigmoid(array):\n    \"\"\"Applies the sigmoid or logistic function to a numpy array\"\"\"\n    return 1/(1+np.exp(-array))\n\ndef E_lr(w,X,y):\n    \"\"\"The logistic regression error function\"\"\"\n    return np.mean(np.log(1+np.exp(-y*X.dot(w))))\n\n\ndef fast_grad_lr(w,X,y):\n    \"\"\"params: X,w,y\n       A fast version of the logistic regression\n       (No python loops -- all ops vectorized numpy ops)\n    \"\"\"\n    ### You complete me\n    \ndef slowgrad(w,X,y):\n    \"\"\"A 'natural' version of the logistic regression gradient\n    with Python repeat loops allowed.\"\"\"\n    ### You complete me   ","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"aed8e0","input":"sevens = (y==\"7\")\ntwos = (y==\"2\")\nsevenORtwo = sevens+twos\nXt = X[sevenORtwo]\nyt = y[sevenORtwo]\nyt = (yt==\"7\")*2-1\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\n#scaler = MinMaxScaler()\n\n\nXt = np.hstack((np.ones((Xt.shape[0],1)),Xt))  ## bias column\n\nX_train,X_test,y_train,y_test = train_test_split(Xt,yt)\n\n\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":3,"id":"8a37a4","input":"import mystuff as ms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\nw = np.array([4,-3])\nfig = plt.figure(figsize=(9,8))\n\nplt.scatter(X[:ynum,1],np.zeros(ynum),c='b',alpha=0.3,s=60,label=\"positive example\")\nplt.scatter(X[ynum:,1],np.zeros(nnum),c='r',alpha=0.3,s=60,label=\"negative example\")\n\nplt.plot(X[:,1],X.dot(w))\nplt.title(\"Some 1D data and a decision boundary\")\nplt.legend()\nplt.show()","output":{"0":{"data":{"image/png":"6578cb21e2e2655b7b04ac3f6b34c06d8da39bd6","text/plain":"<Figure size 648x576 with 1 Axes>"},"exec_count":3,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"16aa05","input":"ms.fast_grad_lr(w,X,y)","output":{"0":{"data":{"text/plain":"array([-0.02068757, -0.01424479])"},"exec_count":4,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":40,"id":"389e28","input":"def E_in(X,y,w):\n    mc = (np.sign(X.dot(w)) != y)\n    return np.sum(mc)/len(mc)\n\ndef Pocket(X,y,w,max_iter=1000):\n    E_in_best = 1\n    E_in_argbest = w\n    for i in range(max_iter):\n        mc = (np.sign(X.dot(w)) != y)\n        if not mc.any():\n            break\n        if E_in(X,y,w) < E_in_best:\n            E_in_best = E_in(X,y,w)\n            E_in_argbest = w\n            \n        badx = X[mc][0]\n        bady = y[mc][0]\n        w = w+bady*badx\n    return E_in_argbest,i\n\nw = np.zeros(X_train.shape[1])\n\nw,i=Pocket(X_train,y_train,w)\n\nprint(\"Finished in {} iterations\".format(i))\nprint(\"E_in = {}\".format(E_in(X_test,y_test,w)))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nE_in = 0.02072248669840381\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":41,"id":"4a7b05","input":"## Warm up?\n\nw = np.linalg.pinv(X_train).dot(y_train)\nw,i=Pocket(X_train,y_train,w)\n\nprint(\"Finished in {} iterations\".format(i))\nprint(\"E_in = {}\".format(E_in(X_test,y_test,w)))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"Finished in 999 iterations\nE_in = 0.019602352282273874\n"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"87e855","input":"w = np.zeros(X_train.shape[1])\n\nw,path = ms.grad_descent(w,X_train,y_train,ms.fast_grad_lr,eta=0.1,max_iter=3000)\nerrors = [E_lr(ww,X_train,y_train) for ww in path]\nplt.plot(np.arange(len(errors)),errors)\nplt.title(\"Error as gradient descent progresses\")\nplt.show()","output":{"0":{"data":{"image/png":"bf84d2299613d354cb9c33dbd4587de3558d97f4","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":43,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":44,"id":"7ecd32","input":"print(\"E_in = {}\".format(E_in(X_test,y_test,w)))\n","output":{"0":{"name":"stdout","output_type":"stream","text":"E_in = 0.014001680201624195\n"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"d8148e","input":"%%time\nslowgrad(w,X,y)","output":{"0":{"name":"stdout","output_type":"stream","text":"CPU times: user 2 µs, sys: 1 µs, total: 3 µs\nWall time: 8.11 µs\n"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":51,"id":"c7606c","input":"def lr_pw_grad(x,w,yy):\n    return -yy*x/(1+np.exp(yy*w.dot(x)))\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    \n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        for i in range(X.shape[0]):\n            ### Redacted!\n            history.append(w)\n    return w,np.array(history)","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"3a4e89","input":"%%time\nms.fast_grad_lr(w,X,y)","output":{"0":{"name":"stdout","output_type":"stream","text":"CPU times: user 126 µs, sys: 68 µs, total: 194 µs\nWall time: 168 µs\n"},"1":{"data":{"text/plain":"array([-0.02068757, -0.01424479])"},"exec_count":6,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"e3567b","input":"w = np.zeros(2)\nw,path = ms.grad_descent(w,X,y,ms.fast_grad_lr,eta=0.1,max_iter=3000)\nerrors = [E_lr(ww,X,y) for ww in path]\nplt.plot(np.arange(len(errors)),errors)\nplt.title(\"Error as gradient descent progresses\")\nplt.show()","output":{"0":{"data":{"image/png":"117987ddde95d50b02db080b82851aad6dad719d","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":7,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"b3291c","input":"def plot_sigmoid(w,dom):\n    F = np.c_[np.ones(dom.shape[0]),dom]\n    return 1/(1+np.exp(-F.dot(w)))\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\n#w = np.linalg.pinv(X).dot(y)\n\ndom = np.linspace(-7,5)\n\nfig = plt.figure(figsize=(9,8))\nplt.scatter(X[:ynum,1],np.ones(ynum),c='b',alpha=0.3,s=60)\nplt.scatter(X[ynum:,1],np.zeros(nnum),c='r',alpha=0.3,s=60)\nplt.plot(dom,plot_sigmoid(w,dom),label=r\"regression line $\\bar{w}=X^\\dagger \\bar{y}$\")\nplt.plot(np.linspace(-7,5),np.zeros(50)+0.5,label=\"threshold\")\nmc = 2*(sigmoid(X.dot(w)) > 0.5)-1 !=y\nplt.scatter(X[mc,1],np.ones(np.sum(mc)),c='yellow',alpha=1,s=60,label=\"misclassified\")\nplt.legend()\nplt.title(\"Some 1D data, logistic regression\")\nplt.show()\nprint(\"{} points misclassified\".format(np.sum(mc)))\nprint(\"w =\",w)","output":{"0":{"data":{"image/png":"8ce4c0887acfb7327d50a08917bbe71fa76041c4","text/plain":"<Figure size 648x576 with 1 Axes>"},"exec_count":8,"metadata":{"needs_background":"light"},"output_type":"execute_result"},"1":{"name":"stdout","output_type":"stream","text":"0 points misclassified\nw = [ 4.93747616 -3.32230352]\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"21cb27","input":"xx = np.linspace(-5,8,100)\nyy = np.linspace(-8,2,100)\n\nXX,YY = np.meshgrid(xx,yy)\nW = np.c_[XX.ravel(),YY.ravel()]\n#errs = ((X.dot(W.T)).T - y)**2\nerrs = np.log(1+np.exp(-X.dot(W.T).T*y))\nE = np.mean(errs,axis=1)\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (logistic regression)\")\nplt.xlabel(r\"$w_0$\")\nP = path[:1000]\nplt.plot(P[:,0],P[:,1],c='r',label=\"SGD path\")\nplt.ylabel(r\"$w_1$\")\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"8e1069e585a22647719056f7e7a63fa1ddbf3fa4","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":9,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"11512e","input":"### Logistic Regression\n\nThe basic idea of logistic regression to use $\\theta(\\bar{w}^T\\bar{x})$ as a model for $P(y=+1|\\bar{x})$.\n\nThat is, we assume that\n\n$$h_\\bar{w}(\\bar{x}) = \\theta(\\bar{w}^T\\bar{x}) \\approx P(y=+1 | \\bar{x}).$$\n\nWe want to find the $\\bar{w}$ that fits this assumption the best.\n\nThere is a somewhat crazy strategy for doing this that requires a second of thought.\n\nThis approach is called the method of [*maximum likelihood*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n\nStatement:  Assume that $P(\\bar{x})$ and $\\theta(\\bar{w}^T\\bar{x})$ produced the data $(\\bar{x}_1,y_1),(\\bar{x}_2,y_2),\\ldots$, but we don't know for sure what $\\bar{w}$ actually was.  \n\n**Which $\\bar{w}$ maximizes the chance that we would see the data that we actually do see?**\n\n---\n\nWe now construct a measure of how the data is for a given $\\bar{w}$.\n\nThis will basically be our error function for $\\bar{w}$.\n\nThe $\\bar{w}$ that maximizes the likelihood of seeing the data will be the final hypothesis.\n\n---\n\nUnder the assumption that $\\theta(\\bar{w}^T\\bar{x}) \\approx P(y=+1 | \\bar{x})$,\n\n$$P(y | \\bar{x}) = \\begin{cases} \\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=+1\\\\ 1-\\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=-1 \\end{cases}$$\n\nA nice property of the logistic function helps simplify this expression:\n\n$$1-\\theta(s) = 1-\\frac{e^s}{1+e^{s}} = \\frac{1+e^s-e^{-s}}{1+e^{s}} = \\frac{1}{1+e^s}$$\n\nTherefore\n\n$$1-\\theta(s) = \\theta(-s)$$\n\nand\n\n\\begin{align}\nP(y | \\bar{x}) &= \\begin{cases} h_\\bar{w}(\\bar{x}) \\text{ for } y=+1\\\\ 1-h_\\bar{w}(\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\ &= \\begin{cases} \\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=+1\\\\ 1-\\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\ &= \\begin{cases} \\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=+1\\\\ \\theta(-\\bar{w}^T\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\\n&= \\theta(y\\bar{w}^T\\bar{x})\n\\end{align}\n\nThe dataset is supposed to be independently generated:\n\n$(\\bar{x}_1,y_1),(\\bar{x}_2,y_2),\\cdots,(\\bar{x}_N,y_N)$.\n\nThe probability that we see all the $y$'s that we actually do see given the $\\bar{x}$'s is therefore just the product:\n\n$$\\prod_{n=1}^N P(y_n|\\bar{x}_n) \\approx \\prod_{n=1}^N \\theta(y\\bar{w}^T\\bar{x})$$\n\nWe want to maximize the right hand side by selecting the right $\\bar{w}$.\n\nBut we can equivalently minimize the negative logarithm.  \n\nThis works better both numerically and analytically.\n\n\\begin{align}\n-\\frac{1}{N}\\ln(\\prod_{n=1}^N P(y_n|\\bar{x}_n)) &= -\\frac{1}{N}\\sum_{n=1}^N \\ln(P(y_n|\\bar{x}_n))\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(\\frac{1}{P(y_n|\\bar{x}_n)})\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(\\frac{1}{\\theta(y\\bar{w}^T\\bar{x})})\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})\n\\end{align}\n\nNotice that this is just the average of the pointwise error function\n\n$${\\bf e}(\\bar{w}) = \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})$$\n\nWe can see that ${\\bf e}(\\bar{w})$ is big when $\\bar{x}_n$ is misclassified and small when $\\bar{x}$ is classified correctly.\n\nWhen $\\bar{x}_n$ is miscalssified $y_n$ and $\\bar{w}^T\\bar{x}$ are different signs.\n\nBut when the classification is correct, the signs are the same. \n\nTo summarize, the in-sample error is\n\n$$E_{in}(\\bar{w}) = \\frac{1}{N}\\sum_{n=1}^N \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})$$\n\nand the pointwise error is\n\n$${\\bf e}(\\bar{w}) = \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})$$\n\nand $E_{in}$ is the average of ${\\bf e}(\\bar{w})$ over all the training data.\n\n\n### What's the gradient?\n\nAs we said earlier, training in machine learning usually just means finding the solution to the equation\n\n$$\\nabla E_{in}(\\bar{w}) = \\bar{0}.$$\n\nAs the book explains, for the logistic regression error function,\n\n$$\\nabla E_{in}(\\bar{w}) = \\frac{1}{N}\\sum_{n=1}^N \\nabla {\\bf e}$$\n\nand\n\n$$\\nabla {\\bf e} = -\\frac{y_n\\bar{x}_n}{1+e^{y_n\\bar{w}^T\\bar{x}}}$$\n\nThe zeros of this expression can't be found analytically the way we did for the linear regression gradient.\n\nInstead we use gradient descent with $\\nabla E_{in}$ or stochastic gradient descent with $\\nabla {\\bf e}$.\n\n---\n\n\nIn HW3 you are asked to implement this gradient descent.\n\nIn a subsequent assignment you will also implement logistic regression.\n\nFor that reason we leave some functions blank below.\n\nWe first implement the \"slow\" version of the logistic regression gradient which is the straightforward translation of the mathematical formula into python.\n\nWe then try a \"fast\" version that uses only numpy operations (no python loops).\n\nThe numpy version is about 20 times faster.  \n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"18ac6d","input":"### Real data\n\nLet's try logistic regression out on the MNIST dataset.\n\n","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"561638","input":"### Logistic regression\n\nEarlier we thresholded linear regression to make a classifier.\n\nFirst we learned $\\bar{w} = X^\\dagger\\bar{y}$, where $X^\\dagger$ is the pseudoinverse.\n\nThen, with the threshold $a=0$, we defined this hypothesis:\n\n$$h_\\bar{w}(\\bar{x}) = \\begin{cases} +1 \\,\\text{if}\\, \\bar{w}^T\\bar{x} > a \\\\ \n                                    -1 \\,\\text{if}\\, \\bar{w}^T\\bar{x} \\leq a\\end{cases}$$\n\nThis is the same as\n\n$$\\hat{y} = sign(\\bar{w}^T\\bar{x}).$$\n\nThis sort of abrupt cutoff is sometimes called a **hard threshold**.\n\nWe didn't minimize classification error directly because it is too blocky a function.\n\nThere are a lot of flat abrupt plateaus and no slopes, and gradient descent can't operate.\n\nBut what regression tries to optimize (linear fit) is not exactly the same as what classification seeks to optimize (best decision boundary).\n\nTherefore we didn't find the optimal classification boundary in the previous slides.\n\n\n---\n\nWe now examine a sort of compromise that keeps the best properties of each:  \n\nThe error function will be continuous, allowing optimization using calculus (gradient descent).\n\nBut the error function also is closer to ordinary classification error.\n\n---\n\nTo accomplish this we will use a **soft threshold**.\n\n$$h_\\bar{w}(\\bar{x}) = \\theta(\\bar{w}^T\\bar{x})$$\n\nwhere $\\theta(s) = \\frac{e^s}{1+e^s} = \\frac{1}{1+e^{-s}}$.\n\nRather than abruptly changing from -1 to +1, this hypothesis gently changes from 0 to 1.\n\nBy introducing continuous change, it becomes possible to use gradient descent to find the best $\\bar{w}$.\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"703c44","input":"### Other possibilities\n\nSometimes you might **want** to predict a probability rather than a definite value.\n\nActually thresholding $\\hat{p}$ destroys information.\n\nHere is a plot of some data with a \"soft\" decision boundary.\n\nIn the yellow region we are confident to predict +1.\n\nIn the blue region we are confident predicting -1.\n\nIn the green region we know we might be wrong.\n\n","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"a39b01","input":"### Other thresholds\n\nIt's natural to threshold $\\hat{p}$ at $\\frac{1}{2}$ in the absence of other information.\n\nBut suppose that $\\hat{p}$ is the probability that someone does **not** have cancer given a certain test.\n\nThen you might want to threshold at $\\frac{95}{100}$ rather than $\\frac{1}{2}$ because one type of error is much worse than the other.\n\nIn general adjusting the threshold gives a tradeoff between [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n\nIn a later lesson we might study this tradeoff in more detail.\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"ed32d5","input":"### Hard thresholding\n\nLogistic regression is called 'regression' because it estimates the continous valued function $P(y=+1 |\\bar{x})$.\n\nBut it is really a *classification* technique.  \n\nSuppose you give logistic regression a new instance $\\bar{x}$ and it gives you back $\\hat{p} \\approx P(y=+1| \\bar{x})$.  \n\nYou can then *threshold* this $\\hat{p}$ to get a definite decision:\n\nIf $\\hat{p} \\geq \\frac{1}{2}$ then predict $y=+1$.\n\nIf $\\hat{p} <\\frac{1}{2}$ then predict $y=-1$.\n\nThis is what we did for the images above.\n\nWe obtained a perfect classification score on our toy data.","pos":11,"type":"cell"}
{"id":0,"time":1587061042408,"type":"user"}
{"last_load":1587061045688,"type":"file"}