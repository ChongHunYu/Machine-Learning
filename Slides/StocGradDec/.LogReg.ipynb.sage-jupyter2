{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":90861568},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1583427448068,"exec_count":1,"id":"69742b","input":"import matplotlib.pyplot as plt\nimport numpy as np\n\nxx = np.linspace(-7,7)\nyy = np.exp(xx)/(1+np.exp(xx))\n\nplt.plot(xx,yy)\nplt.title(\"The logistic, or sigmoid function\")\nplt.show()","kernel":"python3","output":{"0":{"data":{"image/png":"5c48f49e2f5db3a3f503bdbf66984559a4c7d504","text/plain":"<Figure size 864x504 with 1 Axes>"},"metadata":{"image/png":{"height":426,"width":706},"needs_background":"light"}}},"pos":1,"start":1583427447248,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":10,"id":"05ee9d","input":"%%time\nslowgrad(w,X,y)","output":{"0":{"name":"stdout","output_type":"stream","text":"CPU times: user 1.35 ms, sys: 134 µs, total: 1.49 ms\nWall time: 1.4 ms\n"},"1":{"data":{"text/plain":"array([-0.02306013,  0.01168821])"},"exec_count":10,"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"877347","input":"%%time\nms.fast_grad_lr(w,X,y)","output":{"0":{"name":"stdout","output_type":"stream","text":"CPU times: user 195 µs, sys: 0 ns, total: 195 µs\nWall time: 180 µs\n"},"1":{"data":{"text/plain":"array([-0.02306013,  0.01168821])"},"exec_count":11,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"8eb079","input":"w = np.zeros(2)\nw,path = ms.grad_descent(w,X,y,ms.fast_grad_lr,eta=0.1,max_iter=3000)\nerrors = [E_lr(ww,X,y) for ww in path]\nplt.plot(np.arange(len(errors)),errors)\nplt.title(\"Error as gradient descent progresses\")\nplt.show()","output":{"0":{"data":{"image/png":"117987ddde95d50b02db080b82851aad6dad719d","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"ccc3a5","input":"def sigmoid(array):\n    \"\"\"Applies the sigmoid or logistic function to a numpy array\"\"\"\n    return 1/(1+np.exp(-array))\n\ndef E_lr(w,X,y):\n    \"\"\"The logistic regression error function\"\"\"\n    return np.mean(np.log(1+np.exp(-y*X.dot(w))))\n\n\ndef fast_grad_lr(w,X,y):\n    \"\"\"params: X,w,y\n       A fast version of the logistic regression\n       (No python loops -- all ops vectorized numpy ops)\n    \"\"\"\n    ### You complete me\n    \ndef slowgrad(w,X,y):\n    \"\"\"A 'natural' version of the logistic regression gradient\n    with Python repeat loops allowed.\"\"\"\n    ### You complete me   ","pos":3,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"1a33e3","input":"def plot_sigmoid(w,dom):\n    F = np.c_[np.ones(dom.shape[0]),dom]\n    return 1/(1+np.exp(-F.dot(w)))\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\n#w = np.linalg.pinv(X).dot(y)\n\ndom = np.linspace(-7,5)\n\nfig = plt.figure(figsize=(9,8))\nplt.scatter(X[:ynum,1],np.ones(ynum),c='b',alpha=0.3,s=60)\nplt.scatter(X[ynum:,1],-np.ones(nnum),c='r',alpha=0.3,s=60)\nplt.plot(dom,plot_sigmoid(w,dom),label=r\"regression line $\\bar{w}=X^\\dagger \\bar{y}$\")\nplt.plot(np.linspace(-7,5),np.zeros(50)+0.5,label=\"threshold\")\nmc = 2*(sigmoid(X.dot(w)) > 0.5)-1 !=y\nplt.scatter(X[mc,1],np.ones(np.sum(mc)),c='yellow',alpha=1,s=60,label=\"misclassified\")\nplt.legend()\nplt.title(\"Some 1D data, regression for classification\")\nplt.show()\nprint(\"{} points misclassified\".format(np.sum(mc)))\nprint(\"w =\",w)","output":{"0":{"data":{"image/png":"4e4f2262f4024a46a10b43770619dc75b025f258","text/plain":"<Figure size 648x576 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"0 points misclassified\nw = [ 4.93747616 -3.32230352]\n"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":28,"id":"2305b1","input":"xx = np.linspace(-5,5,100)\nyy = np.linspace(-5,2,100)\n\nXX,YY = np.meshgrid(xx,yy)\nW = np.c_[XX.ravel(),YY.ravel()]\n#errs = ((X.dot(W.T)).T - y)**2\nerrs = np.log(1+np.exp(-X.dot(W.T).T*y))\nE = np.mean(errs,axis=1)\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (logistic regression)\")\nplt.xlabel(r\"$w_0$\")\nP = path[:1000]\nplt.plot(P[:,0],P[:,1],c='r',label=\"SGD path\")\nplt.ylabel(r\"$w_1$\")\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"b575f9a6a48dac876beee47926aeeaf8e706ea43","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"cad495","input":"import mystuff as ms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\nw = np.array([4,-3])\nfig = plt.figure(figsize=(9,8))\n\nplt.scatter(X[:ynum,1],np.zeros(ynum),c='b',alpha=0.3,s=60,label=\"positive example\")\nplt.scatter(X[ynum:,1],np.zeros(nnum),c='r',alpha=0.3,s=60,label=\"negative example\")\n\nplt.plot(X[:,1],X.dot(w))\nplt.title(\"Some 1D data and a decision boundary\")\nplt.legend()\nplt.show()","output":{"0":{"data":{"image/png":"6578cb21e2e2655b7b04ac3f6b34c06d8da39bd6","text/plain":"<Figure size 648x576 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"8cc527","input":"ms.fast_grad_lr(w,X,y)","output":{"0":{"data":{"text/plain":"array([-0.02068757, -0.01424479])"},"exec_count":5,"output_type":"execute_result"}},"pos":5,"type":"cell"}
{"cell_type":"code","id":"9fd188","input":"","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"62d2b1","input":"### Logistic Regression\n\nThe basic idea of logistic regression is that\n\n$$h_\\bar{w}(\\bar{x}) = \\theta(\\bar{w}^T\\bar{x}) \\approx P(y=+1 | \\bar{x}).$$\n\nUnder the assumption that this is true, we find the $\\bar{w}$ that is most likely given the data $X$.\n\nThis approach is called the method of *maximum likelihood*.\n\nWe now construct a measure of how likely any $\\bar{w}$ is given the data.\n\nThis will basically be our error function.\n\n---\n\nUnder the assumption that $\\theta(\\bar{w}^T\\bar{x}) \\approx P(y=+1 | \\bar{x})$,\n\n$$P(y | \\bar{x}) = \\begin{cases} h_\\bar{w}(\\bar{x}) \\text{ for } y=+1\\\\ 1-h_\\bar{w}(\\bar{x}) \\text{ for } y=-1 \\end{cases}$$\n\nA nice property of the logistic function helps simplify this expression:\n\n$$1-\\theta(s) = 1-\\frac{e^s}{1+e^{s}} = \\frac{1+e^s-e^{-s}}{1+e^{s}} = \\frac{1}{1+e^s}$$\n\nTherefore\n\n$$1-\\theta(s) = \\theta(-s)$$\n\nand\n\n\\begin{align}\nP(y | \\bar{x}) &= \\begin{cases} h_\\bar{w}(\\bar{x}) \\text{ for } y=+1\\\\ 1-h_\\bar{w}(\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\ &= \\begin{cases} \\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=+1\\\\ 1-\\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\ &= \\begin{cases} \\theta(\\bar{w}^T\\bar{x}) \\text{ for } y=+1\\\\ \\theta(-\\bar{w}^T\\bar{x}) \\text{ for } y=-1 \\end{cases}\\\\\n&= \\theta(y\\bar{w}^T\\bar{x})\n\\end{align}\n\nThe dataset is supposed to be independently generated:\n\n$(\\bar{x}_1,y_1),(\\bar{x}_2,y_2),\\cdots,(\\bar{x}_N,y_N)$.\n\nThe probability that we see all the $y$'s that we actually do see given the $\\bar{x}$'s is therefore just the product:\n\n$$\\prod_{n=1}^N P(y_n|\\bar{x}_n) \\approx \\prod_{n=1}^N \\theta(y\\bar{w}^T\\bar{x})$$\n\nThis is what we want to maximize by selecting the right $\\bar{w}$.\n\nBut we can equivalently minimize the negative logarithm.  \n\nThis works better both numerically and analytically.\n\n\\begin{align}\n-\\frac{1}{N}\\ln(\\prod_{n=1}^N P(y_n|\\bar{x}_n)) &= -\\frac{1}{N}\\sum_{n=1}^N \\ln(P(y_n|\\bar{x}_n))\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(\\frac{1}{P(y_n|\\bar{x}_n)})\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(\\frac{1}{\\theta(y\\bar{w}^T\\bar{x})})\\\\\n&= \\frac{1}{N}\\sum_{n=1}^N \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})\n\\end{align}\n\nNotice that this is just the average of the pointwise error function\n\n$${\\bf e}(\\bar{w}) = \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})$$\n\nWe can see that ${\\bf e}(\\bar{w})$ is big when $\\bar{x}_n$ is misclassified and small when $\\bar{x}$ is classified correctly.\n\nWhen $\\bar{x}_n$ is miscalssified $y_n$ and $\\bar{w}^T\\bar{x}$ are different signs.\n\nBut when the classification is correct, the signs are the same. \n\nTo summarize:\n\n$$E_{in}(\\bar{w}) = \\frac{1}{N}\\sum_{n=1}^N \\ln(1+e^{-y_n\\bar{w}^T\\bar{x}_n})$$\n\n### What's the gradient?\n\nAs we said earlier, training in machine learning usually just means finding the solution to the equation\n\n$$\\nabla E_{in}(\\bar{w}) = \\bar{0}.$$\n\nAs the book explains, for the logistic regression error function,\n\n$$\\nabla E_{in}(\\bar{w}) = -\\frac{1}{N}\\sum_{n=1}^N \\frac{y_n\\bar{x}_n}{1+e^{y_n\\bar{w}^T\\bar{x}}}$$\n\nThis expression can't be solved analytically the way we did for the linear regression gradient.\n\nInstead we use gradient descent.\n\n---\n\n\nBelow you are requested to implement this gradient descent with logistic regression.\n\nFirst implement the \"slow\" version which is the straightforward translation of the mathematical formula into python.\n\nThen try a \"fast\" version that uses only numpy operations (no python loops).\n\nThe numpy version is about 20 times faster.  \n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"b7c82b","input":"### Logistic regression\n\nEarlier we thresholded linear regression to make a classifier.\n\nFirst we learned $\\bar{w} = X^\\dagger\\bar{y}$, where $X^\\dagger$ is the pseudoinverse.\n\nThen, with the threshold $a=0$, we defined this hypothesis:\n\n$$h_\\bar{w}(\\bar{x}) = \\begin{cases} +1 \\,\\text{if}\\, \\bar{w}^T\\bar{x} > a \\\\ \n                                    -1 \\,\\text{if}\\, \\bar{w}^T\\bar{x} \\leq a\\end{cases}$$\n\nThis is the same as\n\n$$\\hat{y} = sign(\\bar{w}^T\\bar{x}).$$\n\nThis sort of abrupt cutoff is sometimes called a **hard threshold**.\n\nWe didn't minimize classification error directly because it is too blocky a function.\n\nThere are a lot of flat abrupt plateaus and no slopes, and gradient descent can't operate.\n\nBut what regression tries to optimize (linear fit) is not exactly the same as what classification seeks to optimize (best decision boundary).\n\nTherefore we didn't find the optimal classification boundary in the previous slides.\n\n\n---\n\nWe now examine a sort of compromise that keeps the best properties of each:  \n\nThe error function will be continuous, allowing optimization using calculus (gradient descent).\n\nBut the error function also is closer to ordinary classification error.\n\n---\n\nTo accomplish this we will use a **soft threshold**.\n\n$$h_\\bar{w}(\\bar{x}) = \\theta(\\bar{w}^T\\bar{x})$$\n\nwhere $\\theta(s) = \\frac{e^s}{1+e^s} = \\frac{1}{1+e^{-s}}$.\n\nRather than abruptly changing from -1 to +1, this hypothesis gently changes from 0 to 1.\n\nBy introducing continuous change, it becomes possible to use gradient descent to find the best $\\bar{w}$.\n","pos":0,"type":"cell"}
{"id":0,"time":1583426761000,"type":"user"}
{"last_load":1583426759889,"type":"file"}