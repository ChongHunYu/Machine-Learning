{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":66637824},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"type":"settings"}
{"cell_type":"code","exec_count":12,"id":"f1950b","input":"import mystuff as ms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\nw = np.array([4,-3])\nfig = plt.figure(figsize=(9,8))\n\nplt.scatter(X[:ynum,1],np.zeros(ynum),c='b',alpha=0.3,s=60,label=\"positive example\")\nplt.scatter(X[ynum:,1],np.zeros(nnum),c='r',alpha=0.3,s=60,label=\"negative example\")\n\nplt.plot(X[:,1],X.dot(w))\nplt.title(\"Some 1D data and a decision boundary\")\nplt.legend()\nplt.show()","output":{"0":{"data":{"image/png":"6578cb21e2e2655b7b04ac3f6b34c06d8da39bd6","text/plain":"<Figure size 648x576 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":1,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"afa71c","input":"def E_in(X,w,y):\n    mc = (np.sign(X.dot(w)) != y)\n    return np.mean(mc)\nE_in(X,w,y)","output":{"0":{"data":{"text/plain":"0.02857142857142857"},"exec_count":13,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"85786b","input":"\nxx = np.linspace(-10,10,100)\nyy = np.linspace(-10,10,100)\n\nXX,YY = np.meshgrid(xx,yy)\nW = np.c_[XX.ravel(),YY.ravel()]\nerrs = np.sign(X.dot(W.T)).T != y\nE = np.mean(errs,axis=1)\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (classification)\")\nplt.xlabel(r\"$w_0$\")\nplt.ylabel(r\"$w_1$\")\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"055666e953f08416101a687b78420d751d912f60","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"66bdf2","input":"import mystuff as ms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\n\nynum=20  ## Number of positive examples\nnnum=15  ## Number of negative examples\n\nyes = np.random.randn(ynum)*3-2  ## positive examples\nno = np.random.randn(nnum)+3     ## negative examples\n\nyyes = np.ones(ynum)\nyno = np.ones(nnum)*-1\n\npreX = np.hstack((yes,no))  ## X but no bias yet\ny = np.hstack((yyes,yno))   ## targets y\n\nX = np.c_[np.ones(ynum+nnum),preX]  ## adding bias\n\nw = np.linalg.pinv(X).dot(y)\nfig = plt.figure(figsize=(9,8))\nplt.scatter(X[:ynum,1],np.ones(ynum),c='b',alpha=0.3,s=60)\nplt.scatter(X[ynum:,1],-np.ones(nnum),c='r',alpha=0.3,s=60)\nplt.plot(X[:,1],X.dot(w),label=r\"regression line $\\bar{w}=X^\\dagger \\bar{y}$\")\nplt.plot(np.linspace(-7,5),np.zeros(50),label=\"threshold\")\nmc = np.sign(X.dot(w))!=y\nplt.scatter(X[mc,1],np.ones(np.sum(mc)),c='yellow',alpha=1,s=60,label=\"misclassified\")\nplt.legend()\nplt.title(\"Some 1D data, regression for classification\")\nplt.show()\nprint(\"{} points misclassified\".format(np.sum(mc)))\nprint(\"w =\",w)","output":{"0":{"data":{"image/png":"52323df09486cd46da857528bc426aa625821aa7","text/plain":"<Figure size 648x576 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"3 points misclassified\nw = [ 0.21074322 -0.25333933]\n"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"3de0bc","input":"## Classification error and regression error\n\nimport mystuff as ms\nE_in(X,w,y), ms.R2(X.dot(w),y)","output":{"0":{"data":{"text/plain":"(0.08571428571428572, 0.515414831367409)"},"exec_count":16,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"7c1fa3","input":"xx = np.linspace(-10,10,100)\nyy = np.linspace(-10,10,100)\n\nXX,YY = np.meshgrid(xx,yy)\nW = np.c_[XX.ravel(),YY.ravel()]\nerrs = ((X.dot(W.T)).T - y)**2\nE = np.mean(errs,axis=1)\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (regression)\")\nplt.xlabel(r\"$w_0$\")\nplt.ylabel(r\"$w_1$\")\nplt.colorbar()\nplt.show()","output":{"0":{"data":{"image/png":"845377ec3cb316feb191debb575c8acc6e3cc92f","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"f7a82d","input":"def pw_linreg_grad(x,w,yy):\n    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n       return:  new w\n       This is the gradient for OLS error.\n       The mathematical formula can be found above.\n    \"\"\"\n    ### You complete me\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,max_iter=1000):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\n       return: the best w and the history (path)\n    \n    \"\"\"\n    history = []  ## Every time you compute a new w, do history.append(w).\n    ### You complete me\n    ## return w, np.array(history)\n    \nimport mystuff as ms  ### This has a compiled version of my solution.\n\n\nprint(\"Solution from normal equations: \\n\",w)\nw,path = ms.sgd(np.array([-2,5]),X,y,ms.pw_linreg_grad,eta=0.005)\nprint(\"Solution from SGD\")\nprint(\"w = {}, E_in(w) = {}\".format(w,E_in(X,w,y)))","output":{"0":{"name":"stdout","output_type":"stream","text":"Solution from normal equations: \n [ 0.21074322 -0.25333933]\nSolution from SGD\nw = [ 0.21850223 -0.21375578], E_in(w) = 0.05714285714285714\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"9675c6","input":"\n\nmc = np.sign(X.dot(w))!=y\nX[mc]","output":{"0":{"data":{"text/plain":"array([[1.        , 1.45910741],\n       [1.        , 1.08919806]])"},"exec_count":19,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"339916","input":"fig = plt.figure(figsize=(9,8))\nplt.scatter(X[:ynum,1],np.ones(ynum),c='b',alpha=0.3,s=60)\nplt.scatter(X[ynum:,1],-np.ones(nnum),c='r',alpha=0.3,s=60)\nplt.plot(X[:,1],X.dot(w),label=r\"regression line $\\bar{w}=SGD$\")\nplt.plot(np.linspace(-7,5),np.zeros(50),label=\"threshold\")\nmc = np.sign(X.dot(w))!=y\nplt.scatter(X[mc,1],np.ones(np.sum(mc)),c='yellow',alpha=1,s=60,label=\"misclassified\")\nplt.legend()\nplt.title(\"Some 1D data, regression for classification\")\nplt.show()\nprint(\"{} points misclassified\".format(np.sum(mc)))\nprint(\"w =\",w)","output":{"0":{"data":{"image/png":"161b5ce47a17e65837db56ff0cbc72ae64fa08b3","text/plain":"<Figure size 648x576 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"},"1":{"name":"stdout","output_type":"stream","text":"2 points misclassified\nw = [ 0.21850223 -0.21375578]\n"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"5ba467","input":"fig = plt.figure(figsize=(9,10))\n\nplt.contourf(XX,YY,E.reshape(100,100),levels=300)\nP = path[:1000]\nplt.plot(P[:,0],P[:,1],c='r',label=\"SGD path\")\nplt.title(r\"Contour plot of $z=E_{in}(\\bar{w})$ (regression)\")\nplt.xlabel(r\"$w_0$\")\nplt.ylabel(r\"$w_1$\")\nplt.colorbar()\nplt.legend()\nplt.show()","output":{"0":{"data":{"image/png":"8730e5024080841a2f6da0da0e9b7f813c8c8fab","text/plain":"<Figure size 648x720 with 2 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"cc2353","input":"Xgood = X[:ynum,1]\nXbad = X[ynum:,1]\na = np.array([[w[1]]])\nb = np.array([[-w[0]]])\ns=np.linalg.solve(a,b)\nprint(\"rightmost blue point = \",np.max(Xgood))\nprint(\"leftmost red point = \",np.min(Xbad))\nprint(\"Actual zero of line corresponding to w = \",s[0][0])\n","output":{"0":{"name":"stdout","output_type":"stream","text":"rightmost blue point =  1.459107407690932\nleftmost red point =  1.5567830047746631\nActual zero of line corresponding to w =  1.0222049998739224\n"}},"pos":19,"type":"cell"}
{"cell_type":"code","id":"fd4f8c","input":"","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"42071c","input":"### Similar solutions\n\nIn the output above you can see that SGD and the normal equations find solutions that are very similar.\n","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"6b584a","input":"### Classification error\n\nAbove we compute the usual in-sample error for a classification problem.\n\n$$E_{in} = \\frac{1}{N}\\sum_{n=1}^N [\\![sign(\\bar{w}^T\\bar{x}_n) \\neq y_n ]\\!]$$\n\nBelow we visualize the plot of $E_{in}$ using a contour plot.\n\nNotice that the derivative of $sign(s)$ is always either zero or undefined.\n\nThe plot of this $E_{in}$ is discontinuous.\n\nIt is like a pie where each pie piece is flat, but also each piece of the pie has a different height. \n\nThis makes it hard to minimize $E_{in}$ using gradient descent (or any other method).\n\nThe ball will not roll.\n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"753832","input":"### Applying SGD\n\nWhen we did regression above, we used the normal equations to find $\\bar{w}$.\n\n$$w = np.linalg.pinv(X).dot(y)$$\n\nInstead of doing that, let's use SGD to find a $\\bar{w}$ minimizing the OLS error.\n\nFirst we make a plot of the OLS error function, below.\n","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"a478d4","input":"### Visualizing the $\\bar{w}$ found by SGD\n\nBelow we plot the best regression line found by SGD.\n\nWe also show it's success rate as a thresholded classifier.\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"ce980e","input":"### Regression for classification\n\nHere is a trick we can use to get a $E_{in}$ that is easy to optimize.\n\nWe consider $\\mathcal{Y} = \\{\\pm 1\\}$ not as classes, but as outputs of the target function.\n\nWe will use regression to find a line fitting the data. \n\nThe error that we minimize to find this $\\bar{w}$ is the OLS error from regression (see first slide).\n\n**But** having found this $\\bar{w}$ we then use it for classification using the rule\n\n$$\\hat{y} = sign(\\bar{w}^T\\bar{x})$$\n\nPut another way, we **threshold** $\\bar{w}^T\\bar{x}$ with the threshold $\\theta=0$.\n\n$$h_\\bar{w}(\\bar{x}) = \\begin{cases} +1 \\,if\\, \\bar{w}^T\\bar{x} > \\theta \\\\ \n                                    -1 \\,if\\, \\bar{w}^T\\bar{x} \\leq \\theta\\end{cases}$$\n                                    \nWe plot this situation below.\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"d02320","input":"### Stochastic Gradient Descent (SGD)\n\nConsider a typical error function, such as ordinary least squares:\n\n$$E_{in}(\\bar{w}) = \\frac{1}{N}\\sum_{n=1}^N (\\bar{w}^T\\bar{x}_n - y_n)^2$$\n\nWe want to notice two things.  \n\n**One thing** is that this is just the average of the pointwise error function\n\n$${\\bf{e}}(\\bar{x}_n,y_n) = (\\bar{w}^T\\bar{x}_n - y_n)^2$$\n\n**The second thing** is that the gradient of $E_{in}$ depends on every point in the dataset.\n\nFor instance, the derivative of $E_{in}$ with respect to $w_k$ is\n\n$$\\frac{\\partial}{\\partial w_k} E_{in} = \\frac{2}{N}\\sum_{n=1}^N (\\bar{w}^T\\bar{x}_n - y_n)x_k$$\n\nTo compute this **for a single $\\bar{w}$** we need to know $\\sum_{n=1}^N (\\bar{w}^T\\bar{x}_n - y_n)x_k$, which depends on every data point.\n\nIf $N=10^6$ or so this will take awhile, especially if $d$ is also large.\n\nAnd then for gradient descent we might need to do this for thousands of $\\bar{w}$.\n\nThis is easily getting into the billions of computational steps.\n\nThe takeaway:  **Gradient descent can be too slow**\n\n### Stochastic approach\n\nStochastic is a fancy word for \"random\".\n\nThe idea in SGD is to use $\\nabla {\\bf e}$ in place of $\\nabla E_{in}$.\n\nNote that when \n\n$${\\bf{e}}(\\bar{x}_n,y_n) = (\\bar{w}^T\\bar{x}_n - y_n)^2$$\n\nthen\n\n$$\\nabla_{\\bar{w}}{\\bf{e}}(\\bar{x}_n,y_n) = 2(\\bar{w}^T\\bar{x}_n - y_n)\\bar{x}_n$$\n\ndepends only on $\\bar{x}_n$, a single datapoint.\n\nObserve that if $\\bar{x}_n$ is selected from $X$ uniformly at random, then the **expected value** of $\\nabla {\\bf e}$ is $\\nabla E_{in}$.\n\n$$\\mathbb{E}[\\nabla_{\\bar{w}}{\\bf{e}}(\\bar{x}_n,y_n)] = \\frac{1}{N}\\sum_{n=1}^N 2(\\bar{w}^T\\bar{x}_n - y_n)\\bar{x}_n.$$\n\nBut the right hand side is exactly $\\nabla E_{in}$.\n\n#### SGD\n\nThis lets us make the following change to gradient descent.\n\nRather than the update rule\n\n$$\\bar{w} = \\bar{w} - \\eta \\nabla E_{in}{\\bar{w}}$$\n\nwe do\n\n$$\\bar{w} = \\bar{w} - \\eta \\nabla {\\bf e}(\\bar{x}_n,y_n){\\bar{w}}$$\n\nThis is faster by a factor of $N$. \n\nAnd if $\\bar{x}_n$ is randomly chosen then this *on average* will go in the same direction as regular gradient descent.\n\n#### Pseudocode for SGD\n\nThe pseudocode for SGD is as follows\n\n1. Randomly initialize $\\bar{w}$\n2. Shuffle the rows of $X$\n3. For each row $\\bar{x}$ in $X$ perform the update \n$$\\bar{w} = \\bar{w} - \\eta \\nabla_{\\bar{w}} {\\bf e}(\\bar{x}_n,y_n)$$\n\n4. Perform more iterations through $X$ if necessary\n\n#### Behavior\n\nThe effect of SGD is for the \"ball\" to \"roll downhill\" just as in batch gradient descent.\n\nBut now the ball only goes downhill \"on average\".\n\nUsually the extra bouncing requires us to make $\\eta$ an order of magnitude smaller to achieve convergence.\n\n\n### Example\n\nBelow we will look at some examples of applying SGD to regression and classification problems.\n\nBecause we want to visualize a two dimensional weight space, we need one dimensional data.","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"d86daa","input":"### Could we do better?\n\nThis way of using regression for classification is kind of lazy.\n\nBecause the error measures do not quite agree on what \"best\" means\n\nthe \"best\" regression $\\bar{w}$ is not quite the best classification $\\bar{w}$.\n\nIf we look at the dataset we can see that there exists a $\\bar{w}$ with zero classification error.\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"e16482","input":"### SGD \n\nNow we perform the actual SGD.\n\nYou will implement these functions, so they are blank for the moment.\n\nBut we can still examine the output.\n","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"f7c869","input":"### Visualizing the path\n\nBelow we show the path taken by \"the ball\" as SDG minimized OLS error.\n\nNotice that it is a little jittery.\n\nIf the learning rate were higher it would be even more jittery.\n\nSGD always produces a jittery path.\n","pos":16,"type":"cell"}
{"error":"Error parsing the ipynb file 'ML_Spr_20/Slides/StocGradDec/sgd.ipynb': SyntaxError: Unexpected end of JSON input.  You must fix the ipynb file somehow before continuing.","type":"fatal"}
{"id":0,"time":1584664752432,"type":"user"}
{"last_load":1583254522108,"type":"file"}