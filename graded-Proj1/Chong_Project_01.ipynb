{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PART 1 (step 1 - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 593\r\n",
      "-rw-r--r-- 1 user user   22707 May 19 02:00 Chong_Project_01.ipynb\r\n",
      "-rw-r--r-- 1 user user    5607 May 17 14:41 Readme.txt\r\n",
      "drwxr-xr-x 2 user user       3 May 17 14:49 __pycache__\r\n",
      "-rw-r--r-- 1 user user   45029 May 17 14:41 data_banknote_authentication.txt\r\n",
      "-rw-r--r-- 1 user user   57569 May 17 14:41 day.csv\r\n",
      "-rw-r--r-- 1 user user    1838 May 17 14:41 description.md\r\n",
      "-rw-r--r-- 1 user user 1156736 May 17 14:41 hour.csv\r\n",
      "-rw-r--r-- 1 user user    1858 May 17 14:41 mystuff.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l\n",
    "#!cat mystuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -1\n",
       "1      -1\n",
       "2      -1\n",
       "3      -1\n",
       "4      -1\n",
       "       ..\n",
       "1367    1\n",
       "1368    1\n",
       "1369    1\n",
       "1370    1\n",
       "1371    1\n",
       "Name: class (integer), Length: 1372, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n",
    "######   (Part 1) I will be using data_banknote_authentication dataset for my Classification (Part 1)   #####)\n",
    "\n",
    "\n",
    "#Step 2A: Load the data using pandas.\n",
    "\n",
    "import pandas as pd\n",
    "bank_df = pd.read_csv(\"data_banknote_authentication.txt\",header=None)\n",
    "\n",
    "col_names = [\"variance of Wavelet Transformed image (continuous)\",\n",
    "             \"skewness of Wavelet Transformed image (continuous)\",\n",
    "             \"curtosis of Wavelet Transformed image (continuous)\",\n",
    "             \"entropy of image (continuous)\",\n",
    "             \"class (integer)\"]\n",
    "\n",
    "bank_df.columns = col_names\n",
    "\n",
    "\n",
    "#Step 2B: Convert any nominal columns to numerical columns.\n",
    "#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n",
    "\n",
    "\n",
    "#print(bank_df.head())\n",
    "def convenient_convert_target_values(target):\n",
    "    if target == 0:\n",
    "        target = -1\n",
    "    return target\n",
    "\n",
    "bank_df[\"class (integer)\"] = bank_df[\"class (integer)\"].apply(convenient_convert_target_values)     #Converting 0 to -1 , and 1 to 1\n",
    "\n",
    "\n",
    "#print(bank_df[\"class (integer)\"])\n",
    "#print(bank_df.head())\n",
    "\n",
    "\n",
    "#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.ones((bank_df.shape[0],1))     #inserting 1's of bias column\n",
    "X = np.hstack((X,bank_df[:]))         #combining bias column with everything else\n",
    "X = X[:,: -1]                         #taking out the last column for y \n",
    "\n",
    "#print(X)\n",
    "\n",
    "y = bank_df[\"class (integer)\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in = 0.09620991253644315\n"
     ]
    }
   ],
   "source": [
    "#print(y)\n",
    "\n",
    "#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#Step 5: Scale the data. You can use your own code or sklearn for this. \n",
    "#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\n",
    "X_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n",
    "\n",
    "#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n",
    "\n",
    "def gradient_of_the_pointwise_error_function(x, w, yy):\n",
    "    return (-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n",
    "          #(-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n",
    "\n",
    "def sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n",
    "    \"\"\"parameters: w (initial weight vector)\n",
    "                   X (data matrix)\n",
    "                   y (target vector)\n",
    "                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n",
    "    history = [] ## Every time you compute a new w, do history.append(w).\n",
    "    for j in range(num_epochs):\n",
    "        shuff = np.random.permutation(X.shape[0])\n",
    "        Xs = X[shuff]\n",
    "        Ys = y[shuff]\n",
    "        for i in range(X.shape[0]):\n",
    "            xs = Xs[i]\n",
    "            ys = Ys[i]\n",
    "            w = w - eta*pw_gradient(xs,w,ys)\n",
    "            history.append(w)\n",
    "    return w,np.array(history)\n",
    "\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "w, path = sgd(w, X_train, y_train, gradient_of_the_pointwise_error_function,eta=0.001,num_epochs=200 )\n",
    "\n",
    "#Step 7: Apply your model to the test data to get predictions y hat\n",
    "yhat = np.sign(X_test.dot(w))\n",
    "\n",
    "#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\n",
    "def E_in(X,y,w):\n",
    "    mc = (np.sign(X.dot(w)) != y)\n",
    "    return np.sum(mc)/len(mc)\n",
    "\n",
    "print(\"E_in = {}\".format(E_in(X_test, y_test, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967930029154519"
      ]
     },
     "execution_count": 30,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PART 1 (step 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in = 0.014577259475218658\n"
     ]
    }
   ],
   "source": [
    "#Step 9: (Optional) Try doing some transformations of features to improve accuracy. After you add or modify features (on the whole dataset) start again at step 4.\n",
    "\n",
    "\n",
    "#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#Step 5: Scale the data. You can use your own code or sklearn for this. \n",
    "#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\n",
    "X_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n",
    "\n",
    "#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n",
    "\n",
    "def gradient_of_the_pointwise_error_function(x, w, yy):\n",
    "    return (-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n",
    "\n",
    "\n",
    "def sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n",
    "    \"\"\"parameters: w (initial weight vector)\n",
    "                   X (data matrix)\n",
    "                   y (target vector)\n",
    "                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n",
    "    history = [] ## Every time you compute a new w, do history.append(w).\n",
    "    for j in range(num_epochs):\n",
    "        shuff = np.random.permutation(X.shape[0])\n",
    "        Xs = X[shuff]\n",
    "        Ys = y[shuff]\n",
    "        for i in range(X.shape[0]):\n",
    "            xs = Xs[i]\n",
    "            ys = Ys[i]\n",
    "            w = w - eta*pw_gradient(xs,w,ys)\n",
    "            history.append(w)\n",
    "    return w,np.array(history)\n",
    "\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "w, path = sgd(w, X_train, y_train, gradient_of_the_pointwise_error_function)\n",
    "\n",
    "#Step 7: Apply your model to the test data to get predictions y hat\n",
    "yhat = np.sign(X_test.dot(w))\n",
    "\n",
    "#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\n",
    "def E_in(X,y,w):\n",
    "    mc = (np.sign(X.dot(w)) != y)\n",
    "    return np.sum(mc)/len(mc)\n",
    "\n",
    "print(\"E_in = {}\".format(E_in(X_test, y_test, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2 (step 1 - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution from SGD\n",
      "w = [ 1.74614438e+03 -4.52906507e-01 -1.18069179e+03  1.25364782e+03\n",
      "  2.12766321e+03  1.88835464e+02 -2.83905076e+02  6.38602991e+02\n",
      "  3.70121221e+02 -7.42593035e+02  2.25344926e+03  1.51749088e+03\n",
      " -1.00832447e+03 -9.59336589e+02],\n",
      "R2_test(w) (sgd) = 0.5773020639881477\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n",
    "######   (Part 2) I will be using data_banknote_authentication dataset for my linear regression (Part 2)   #####)\n",
    "#Step 2A: Load the data using pandas.\n",
    "import pandas as pd\n",
    "day_df = pd.read_csv(\"day.csv\")\n",
    "\n",
    "#Step 2B: Convert any nominal columns to numerical columns.\n",
    "#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n",
    "\n",
    "#because it's inconvenient to work with and also kind of redundant.\n",
    "#You should probably pick one of either casual, registered or cnt to be the target, and drop the other two.\n",
    "day_df = day_df.drop(columns=['dteday','casual','registered'])\n",
    "\n",
    "#print(day_df.head())\n",
    "\n",
    "#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\n",
    "import numpy as np\n",
    "\n",
    "X = np.ones((day_df.shape[0],1))     #inserting 1's of bias column\n",
    "X = np.hstack((X,day_df[:]))         #combining bias column with everything else\n",
    "X = X[:,: -1]\n",
    "y = day_df['cnt']\n",
    "\n",
    "#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#Step 5: Scale the data. You can use your own code or sklearn for this. \n",
    "#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\n",
    "X_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n",
    "\n",
    "#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n",
    "\n",
    "def pw_linreg_grad(x,w,yy):\n",
    "    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n",
    "       return:  new w\n",
    "       This is the gradient for MSE error.\n",
    "       The mathematical formula can be found above.\n",
    "       w = (np.linalg.pinv(X_train).dot(y_train))\n",
    "       w=w-eta*gradient(w,X,y)\n",
    "    \"\"\"\n",
    "    return 2*(w.T.dot(x)-yy)*x\n",
    "\n",
    "def sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n",
    "    \"\"\"parameters: w (initial weight vector)\n",
    "                   X (data matrix)\n",
    "                   y (target vector)\n",
    "                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n",
    "    history = [] ## Every time you compute a new w, do history.append(w).\n",
    "    for j in range(num_epochs):\n",
    "        shuff = np.random.permutation(X.shape[0])\n",
    "        Xs = X[shuff]\n",
    "        Ys = y[shuff]\n",
    "        for i in range(X.shape[0]):\n",
    "            xs = Xs[i]\n",
    "            ys = Ys[i]\n",
    "            w = w - eta*pw_gradient(xs,w,ys)\n",
    "            history.append(w)\n",
    "    return w,np.array(history)\n",
    "\n",
    "def R2(y,yhat):\n",
    "    \"\"\"parameters: y, yhat\"\"\"\n",
    "    ## You complete me\n",
    "    ybar = np.mean(y)\n",
    "    \"\"\"  SStot = np.sum((y-yhat)**2)\n",
    "    SSres = np.sum((y-ybar)**2)\"\"\"\n",
    "    SSres = np.sum((y-yhat)**2)\n",
    "    SStot = np.sum((y-ybar)**2)\n",
    "    R2= 1-(SSres/SStot)\n",
    "    return R2\n",
    "    #ybar = np.mean(y)\n",
    "    #r2 = sum((y-yhat)**2)/sum((y-ybar)**2)\n",
    "    #r2 = 1-r2\n",
    "    #return r2\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "w, path = sgd(w, X_train, y_train, pw_linreg_grad)\n",
    "\n",
    "#Step 7: Apply your model to the test data to get predictions y hat\n",
    "yhat_test = X_test.dot(w)\n",
    "\n",
    "#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\n",
    "print(\"Solution from SGD\")\n",
    "print(\"w = {},\\nR2_test(w) (sgd) = {}\".format(w,R2(yhat_test ,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2  (step 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution from SGD\n",
      "w = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan],\n",
      "R2_test(w) (sgd) = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:51: RuntimeWarning: overflow encountered in multiply\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:51: RuntimeWarning: invalid value encountered in multiply\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:66: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n",
    "######   (Part 2) I will be using day.csv dataset for my linear (Part 2)   #####)\n",
    "#Step 2A: Load the data using pandas.\n",
    "import pandas as pd\n",
    "day_df = pd.read_csv(\"day.csv\")\n",
    "\n",
    "#Step 2B: Convert any nominal columns to numerical columns.\n",
    "#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n",
    "\n",
    "#because it's inconvenient to work with and also kind of redundant.\n",
    "#You should probably pick one of either casual, registered or cnt to be the target, and drop the other two.\n",
    "day_df = day_df.drop(columns=['dteday','casual','registered'])\n",
    "\n",
    "#print(day_df.head())\n",
    "\n",
    "#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\n",
    "import numpy as np\n",
    "\n",
    "X = np.ones((day_df.shape[0],1))     #inserting 1's of bias column\n",
    "X = np.hstack((X,day_df[:]))         #combining bias column with everything else\n",
    "X = X[:,: -1]\n",
    "y = day_df['cnt']\n",
    "\n",
    "\n",
    "#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "#Step 5: Scale the data. You can use your own code or sklearn for this. \n",
    "#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\n",
    "X_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n",
    "\n",
    "#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n",
    "def pw_linreg_grad(x,w,yy):\n",
    "    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n",
    "       return:  new w\n",
    "       This is the gradient for MSE error.\n",
    "       The mathematical formula can be found above.\n",
    "       w = (np.linalg.pinv(X_train).dot(y_train))\n",
    "       w=w-eta*gradient(w,X,y)\n",
    "    \"\"\"\n",
    "    return 2*(w.T.dot(x)-yy)*x\n",
    "\n",
    "def sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n",
    "    \"\"\"parameters: w (initial weight vector)\n",
    "                   X (data matrix)\n",
    "                   y (target vector)\n",
    "                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n",
    "    history = [] ## Every time you compute a new w, do history.append(w).\n",
    "    for j in range(num_epochs):\n",
    "        shuff = np.random.permutation(X.shape[0])\n",
    "        Xs = X[shuff]\n",
    "        Ys = y[shuff]\n",
    "        for i in range(X.shape[0]):\n",
    "            xs = Xs[i]\n",
    "            ys = Ys[i]\n",
    "            w = w - eta*pw_gradient(xs,w,ys)\n",
    "            history.append(w)\n",
    "    return w,np.array(history)\n",
    "\n",
    "def R2(y,yhat):\n",
    "    \"\"\"parameters: y, yhat\"\"\"\n",
    "    ## You complete me\n",
    "    ybar = np.mean(y)\n",
    "    \"\"\"  SStot = np.sum((y-yhat)**2)\n",
    "    SSres = np.sum((y-ybar)**2)\"\"\"\n",
    "    SSres = np.sum((y-yhat)**2)\n",
    "    SStot = np.sum((y-ybar)**2)\n",
    "    R2= 1-(SSres/SStot)\n",
    "    return R2\n",
    "    #ybar = np.mean(y)\n",
    "    #r2 = sum((y-yhat)**2)/sum((y-ybar)**2)\n",
    "    #r2 = 1-r2\n",
    "    #return r2\n",
    "\n",
    "w = np.random.randn(X_train.shape[1])\n",
    "w, path = sgd(w, X_train, y_train, pw_linreg_grad)\n",
    "\n",
    "#Step 7: Apply your model to the test data to get predictions y hat\n",
    "yhat_test = X_test.dot(w)\n",
    "\n",
    "#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\n",
    "print(\"Solution from SGD\")\n",
    "print(\"w = {},\\nR2_test(w) (sgd) = {}\".format(w,R2(yhat_test ,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}