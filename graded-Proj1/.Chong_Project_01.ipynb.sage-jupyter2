{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":168685568},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1589853641150,"exec_count":27,"id":"430553","input":"!ls -l\n#!cat mystuff.py","kernel":"python3","output":{"0":{"name":"stdout","text":"total 593\r\n-rw-r--r-- 1 user user   22707 May 19 02:00 Chong_Project_01.ipynb\r\n-rw-r--r-- 1 user user    5607 May 17 14:41 Readme.txt\r\ndrwxr-xr-x 2 user user       3 May 17 14:49 __pycache__\r\n-rw-r--r-- 1 user user   45029 May 17 14:41 data_banknote_authentication.txt\r\n-rw-r--r-- 1 user user   57569 May 17 14:41 day.csv\r\n-rw-r--r-- 1 user user    1838 May 17 14:41 description.md\r\n-rw-r--r-- 1 user user 1156736 May 17 14:41 hour.csv\r\n-rw-r--r-- 1 user user    1858 May 17 14:41 mystuff.py\r\n"}},"pos":1,"start":1589853641016,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853641179,"exec_count":28,"id":"e4b0a1","input":"#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n######   (Part 1) I will be using data_banknote_authentication dataset for my Classification (Part 1)   #####)\n\n\n#Step 2A: Load the data using pandas.\n\nimport pandas as pd\nbank_df = pd.read_csv(\"data_banknote_authentication.txt\",header=None)\n\ncol_names = [\"variance of Wavelet Transformed image (continuous)\",\n             \"skewness of Wavelet Transformed image (continuous)\",\n             \"curtosis of Wavelet Transformed image (continuous)\",\n             \"entropy of image (continuous)\",\n             \"class (integer)\"]\n\nbank_df.columns = col_names\n\n\n#Step 2B: Convert any nominal columns to numerical columns.\n#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n\n\n#print(bank_df.head())\ndef convenient_convert_target_values(target):\n    if target == 0:\n        target = -1\n    return target\n\nbank_df[\"class (integer)\"] = bank_df[\"class (integer)\"].apply(convenient_convert_target_values)     #Converting 0 to -1 , and 1 to 1\n\n\n#print(bank_df[\"class (integer)\"])\n#print(bank_df.head())\n\n\n#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\n\nimport numpy as np\n\nX = np.ones((bank_df.shape[0],1))     #inserting 1's of bias column\nX = np.hstack((X,bank_df[:]))         #combining bias column with everything else\nX = X[:,: -1]                         #taking out the last column for y \n\n#print(X)\n\ny = bank_df[\"class (integer)\"]\ny","kernel":"python3","output":{"0":{"data":{"text/plain":"0      -1\n1      -1\n2      -1\n3      -1\n4      -1\n       ..\n1367    1\n1368    1\n1369    1\n1370    1\n1371    1\nName: class (integer), Length: 1372, dtype: int64"},"exec_count":28}},"pos":2,"start":1589853641163,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853644311,"exec_count":29,"id":"541e08","input":"#print(y)\n\n#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\n\nfrom sklearn.model_selection import train_test_split\n\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\ny_train = np.asarray(y_train)\ny_test = np.asarray(y_test)\n\n#Step 5: Scale the data. You can use your own code or sklearn for this. \n#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n\n#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n\ndef gradient_of_the_pointwise_error_function(x, w, yy):\n    return (-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n          #(-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        Xs = X[shuff]\n        Ys = y[shuff]\n        for i in range(X.shape[0]):\n            xs = Xs[i]\n            ys = Ys[i]\n            w = w - eta*pw_gradient(xs,w,ys)\n            history.append(w)\n    return w,np.array(history)\n\nw = np.random.randn(X_train.shape[1])\nw, path = sgd(w, X_train, y_train, gradient_of_the_pointwise_error_function,eta=0.001,num_epochs=200 )\n\n#Step 7: Apply your model to the test data to get predictions y hat\nyhat = np.sign(X_test.dot(w))\n\n#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\ndef E_in(X,y,w):\n    mc = (np.sign(X.dot(w)) != y)\n    return np.sum(mc)/len(mc)\n\nprint(\"E_in = {}\".format(E_in(X_test, y_test, w)))","kernel":"python3","output":{"0":{"name":"stdout","text":"E_in = 0.09620991253644315\n"}},"pos":3,"start":1589853641194,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853644342,"exec_count":30,"id":"5652c7","input":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)","kernel":"python3","output":{"0":{"data":{"text/plain":"0.967930029154519"},"exec_count":30}},"pos":4,"start":1589853644325,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853645304,"exec_count":31,"id":"57a0dc","input":"#Step 9: (Optional) Try doing some transformations of features to improve accuracy. After you add or modify features (on the whole dataset) start again at step 4.\n\n\n#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\ny_train = np.asarray(y_train)\ny_test = np.asarray(y_test)\n\n#Step 5: Scale the data. You can use your own code or sklearn for this. \n#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\n\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n\n#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n\ndef gradient_of_the_pointwise_error_function(x, w, yy):\n    return (-yy * x)/(1+np.exp(yy*w.T.dot(x)))\n\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        Xs = X[shuff]\n        Ys = y[shuff]\n        for i in range(X.shape[0]):\n            xs = Xs[i]\n            ys = Ys[i]\n            w = w - eta*pw_gradient(xs,w,ys)\n            history.append(w)\n    return w,np.array(history)\n\nw = np.random.randn(X_train.shape[1])\nw, path = sgd(w, X_train, y_train, gradient_of_the_pointwise_error_function)\n\n#Step 7: Apply your model to the test data to get predictions y hat\nyhat = np.sign(X_test.dot(w))\n\n#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\ndef E_in(X,y,w):\n    mc = (np.sign(X.dot(w)) != y)\n    return np.sum(mc)/len(mc)\n\nprint(\"E_in = {}\".format(E_in(X_test, y_test, w)))","kernel":"python3","output":{"0":{"name":"stdout","text":"E_in = 0.014577259475218658\n"}},"pos":6,"start":1589853644357,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853645545,"exec_count":32,"id":"f114fc","input":"#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n######   (Part 2) I will be using data_banknote_authentication dataset for my linear regression (Part 2)   #####)\n#Step 2A: Load the data using pandas.\nimport pandas as pd\nday_df = pd.read_csv(\"day.csv\")\n\n#Step 2B: Convert any nominal columns to numerical columns.\n#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n\n#because it's inconvenient to work with and also kind of redundant.\n#You should probably pick one of either casual, registered or cnt to be the target, and drop the other two.\nday_df = day_df.drop(columns=['dteday','casual','registered'])\n\n#print(day_df.head())\n\n#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\nimport numpy as np\n\nX = np.ones((day_df.shape[0],1))     #inserting 1's of bias column\nX = np.hstack((X,day_df[:]))         #combining bias column with everything else\nX = X[:,: -1]\ny = day_df['cnt']\n\n#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\ny_train = np.asarray(y_train)\ny_test = np.asarray(y_test)\n\n#Step 5: Scale the data. You can use your own code or sklearn for this. \n#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n\n#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\n\ndef pw_linreg_grad(x,w,yy):\n    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n       return:  new w\n       This is the gradient for MSE error.\n       The mathematical formula can be found above.\n       w = (np.linalg.pinv(X_train).dot(y_train))\n       w=w-eta*gradient(w,X,y)\n    \"\"\"\n    return 2*(w.T.dot(x)-yy)*x\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        Xs = X[shuff]\n        Ys = y[shuff]\n        for i in range(X.shape[0]):\n            xs = Xs[i]\n            ys = Ys[i]\n            w = w - eta*pw_gradient(xs,w,ys)\n            history.append(w)\n    return w,np.array(history)\n\ndef R2(y,yhat):\n    \"\"\"parameters: y, yhat\"\"\"\n    ## You complete me\n    ybar = np.mean(y)\n    \"\"\"  SStot = np.sum((y-yhat)**2)\n    SSres = np.sum((y-ybar)**2)\"\"\"\n    SSres = np.sum((y-yhat)**2)\n    SStot = np.sum((y-ybar)**2)\n    R2= 1-(SSres/SStot)\n    return R2\n    #ybar = np.mean(y)\n    #r2 = sum((y-yhat)**2)/sum((y-ybar)**2)\n    #r2 = 1-r2\n    #return r2\nw = np.random.randn(X_train.shape[1])\nw, path = sgd(w, X_train, y_train, pw_linreg_grad)\n\n#Step 7: Apply your model to the test data to get predictions y hat\nyhat_test = X_test.dot(w)\n\n#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\nprint(\"Solution from SGD\")\nprint(\"w = {},\\nR2_test(w) (sgd) = {}\".format(w,R2(yhat_test ,y_test)))","kernel":"python3","output":{"0":{"name":"stdout","text":"Solution from SGD\nw = [ 1.74614438e+03 -4.52906507e-01 -1.18069179e+03  1.25364782e+03\n  2.12766321e+03  1.88835464e+02 -2.83905076e+02  6.38602991e+02\n  3.70121221e+02 -7.42593035e+02  2.25344926e+03  1.51749088e+03\n -1.00832447e+03 -9.59336589e+02],\nR2_test(w) (sgd) = 0.5773020639881477\n"}},"pos":8,"start":1589853645324,"state":"done","type":"cell"}
{"cell_type":"code","end":1589853645829,"exec_count":33,"id":"c4ba7b","input":"#Step 1: Find a binary classification dataset. Good places to look: Kaggle, OpenML, UCI Machine Learning Repository.\n######   (Part 2) I will be using day.csv dataset for my linear (Part 2)   #####)\n#Step 2A: Load the data using pandas.\nimport pandas as pd\nday_df = pd.read_csv(\"day.csv\")\n\n#Step 2B: Convert any nominal columns to numerical columns.\n#Step 2C: This is also a convenient place to convert target values to 1 or -1.\n\n#because it's inconvenient to work with and also kind of redundant.\n#You should probably pick one of either casual, registered or cnt to be the target, and drop the other two.\nday_df = day_df.drop(columns=['dteday','casual','registered'])\n\n#print(day_df.head())\n\n#Step 3: Convert the pandas dataframe to a numpy X and y. Make sure that X has a bias column and that y contains only values of \\pm 1±1.\nimport numpy as np\n\nX = np.ones((day_df.shape[0],1))     #inserting 1's of bias column\nX = np.hstack((X,day_df[:]))         #combining bias column with everything else\nX = X[:,: -1]\ny = day_df['cnt']\n\n\n#Step 4: Perform a train-test split on the data. You can use your own code or sklearn for this.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\ny_train = np.asarray(y_train)\ny_test = np.asarray(y_test)\n\n#Step 5: Scale the data. You can use your own code or sklearn for this. \n#Be sure you scale correctly: First fit the training data to get the scaling parameters, and then apply the scaling to both the training and testing data.\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = np.c_[np.ones(X_train.shape[0]),X_train]         #re-adding the bias column, because it disappeard after scaler\nX_test = np.c_[np.ones(X_test.shape[0]),X_test]            #re-adding the bias column, because it disappeard after scaler\n\n#Step 6:  Train a linear classifier such as logistic regression on the training data. This must be your own code.\ndef pw_linreg_grad(x,w,yy):\n    \"\"\"parameters: x,w,yy  (instance, weight vector, truth value = +-1)\n       return:  new w\n       This is the gradient for MSE error.\n       The mathematical formula can be found above.\n       w = (np.linalg.pinv(X_train).dot(y_train))\n       w=w-eta*gradient(w,X,y)\n    \"\"\"\n    return 2*(w.T.dot(x)-yy)*x\n\ndef sgd(w,X,y,pw_gradient,eta=0.05,num_epochs=50):\n    \"\"\"parameters: w (initial weight vector)\n                   X (data matrix)\n                   y (target vector)\n                   pw_gradient (pointwise gradient function taking params x,w,yy)\"\"\"\n    history = [] ## Every time you compute a new w, do history.append(w).\n    for j in range(num_epochs):\n        shuff = np.random.permutation(X.shape[0])\n        Xs = X[shuff]\n        Ys = y[shuff]\n        for i in range(X.shape[0]):\n            xs = Xs[i]\n            ys = Ys[i]\n            w = w - eta*pw_gradient(xs,w,ys)\n            history.append(w)\n    return w,np.array(history)\n\ndef R2(y,yhat):\n    \"\"\"parameters: y, yhat\"\"\"\n    ## You complete me\n    ybar = np.mean(y)\n    \"\"\"  SStot = np.sum((y-yhat)**2)\n    SSres = np.sum((y-ybar)**2)\"\"\"\n    SSres = np.sum((y-yhat)**2)\n    SStot = np.sum((y-ybar)**2)\n    R2= 1-(SSres/SStot)\n    return R2\n    #ybar = np.mean(y)\n    #r2 = sum((y-yhat)**2)/sum((y-ybar)**2)\n    #r2 = 1-r2\n    #return r2\n\nw = np.random.randn(X_train.shape[1])\nw, path = sgd(w, X_train, y_train, pw_linreg_grad)\n\n#Step 7: Apply your model to the test data to get predictions y hat\nyhat_test = X_test.dot(w)\n\n#Step 8: Evaluate the accuracy of your model. You can use simply the proportion of correct predictions.\nprint(\"Solution from SGD\")\nprint(\"w = {},\\nR2_test(w) (sgd) = {}\".format(w,R2(yhat_test ,y_test)))","kernel":"python3","output":{"0":{"name":"stdout","text":"Solution from SGD\nw = [nan nan nan nan nan nan nan nan nan nan nan nan nan nan],\nR2_test(w) (sgd) = nan\n"},"1":{"name":"stderr","text":"/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:51: RuntimeWarning: overflow encountered in multiply\n/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:51: RuntimeWarning: invalid value encountered in multiply\n/usr/local/lib/python3.6/dist-packages/ipykernel/__main__.py:66: RuntimeWarning: invalid value encountered in subtract\n"}},"pos":10,"start":1589853645555,"state":"done","type":"cell"}
{"cell_type":"code","id":"327a85","input":"","pos":12,"type":"cell"}
{"cell_type":"code","id":"fe7f69","input":"","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"244931","input":"## Part 2  (step 9)","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"ad1a02","input":"## PART 1 (step 9)","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"b1ddac","input":"## Part 2 (step 1 - 8)","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"fde686","input":"## PART 1 (step 1 - 8)","pos":0,"type":"cell"}
{"id":0,"time":1589851300455,"type":"user"}
{"last_load":1589765581095,"type":"file"}